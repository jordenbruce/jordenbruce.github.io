<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JordenBruce</title>
  
  <subtitle>A thousand miles begins with a single step.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jordenbruce.com/"/>
  <updated>2019-10-13T12:28:48.920Z</updated>
  <id>https://jordenbruce.com/</id>
  
  <author>
    <name>JordenBruce</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MapReduce执行过程详解</title>
    <link href="https://jordenbruce.com/2019/10/13/hadoop-mapreduce/"/>
    <id>https://jordenbruce.com/2019/10/13/hadoop-mapreduce/</id>
    <published>2019-10-13T14:57:51.000Z</published>
    <updated>2019-10-13T12:28:48.920Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。<br><a id="more"></a></p><h2 id="0x00-分析MapReduce执行过程"><a href="#0x00-分析MapReduce执行过程" class="headerlink" title="0x00 分析MapReduce执行过程"></a>0x00 分析MapReduce执行过程</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。整个流程如图：</p><p><img src="https://i.loli.net/2019/10/12/jotATrBqyIzV5cY.png" alt="MapReduce执行过程"></p><h2 id="0x01-Mapper任务的执行过程详解"><a href="#0x01-Mapper任务的执行过程详解" class="headerlink" title="0x01 Mapper任务的执行过程详解"></a>0x01 Mapper任务的执行过程详解</h2><p><strong>每个Mapper任务是一个java进程</strong>，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下几个阶段，如图所示：</p><p><img src="https://i.loli.net/2019/10/12/6jbSxLJQNqls4FP.png" alt="Mapper执行过程"></p><p>在上图中，把Mapper任务的运行过程分为六个阶段。</p><ol><li><p>第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值64MB，输入文件有两个，一个是32MB，一个是72MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p></li><li><p>第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一行的起始位置(单位是字节)，“值”是本行的文本内容。</p></li><li><p>第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p></li><li><p>第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer任务运行的数量。默认只有一个Reducer任务。</p></li><li><p>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入第六阶段；如果没有，直接输出到本地的linux文件中。</p></li><li><p>第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。</p></li></ol><h2 id="0x02-Reducer任务的执行过程详解"><a href="#0x02-Reducer任务的执行过程详解" class="headerlink" title="0x02 Reducer任务的执行过程详解"></a>0x02 Reducer任务的执行过程详解</h2><p><strong>每个Reducer任务是一个java进程</strong>。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为如下图所示的几个阶段。</p><p><img src="https://i.loli.net/2019/10/12/HXhC2wvUQLKdV4e.png" alt="Reducer执行过程"></p><ol><li><p>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。</p></li><li><p>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。</p></li><li><p>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS文件中。</p></li></ol><p>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p><h2 id="0x03-键值对的编号"><a href="#0x03-键值对的编号" class="headerlink" title="0x03 键值对的编号"></a>0x03 键值对的编号</h2><p>在对Mapper任务、Reducer任务的分析过程中，会看到很多阶段都出现了键值对，读者容易混淆，所以这里对键值对进行编号，方便大家理解键值对的变化情况，如下图所示。</p><p><img src="https://i.loli.net/2019/10/12/q6Y9Sl5ofQCW7LJ.png" alt="键值对"></p><p>在上图中，对于Mapper任务输入的键值对，定义为key1和value1。在map方法中处理后，输出的键值对，定义为key2和value2。reduce方法接收key2和value2，处理后，输出key3和value3。在下文讨论键值对时，可能把key1和value1简写为&lt;k1,v1&gt;，key2和value2简写为&lt;k2,v2&gt;，key3和value3简写为&lt;k3,v3&gt;。</p><h2 id="转载说明"><a href="#转载说明" class="headerlink" title="转载说明"></a>转载说明</h2><p><a href="https://my.oschina.net/itblog/blog/275294" target="_blank" rel="noopener">Hadoop MapReduce执行过程详解（带hadoop例子）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="mapreduce" scheme="https://jordenbruce.com/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库的初级手册</title>
    <link href="https://jordenbruce.com/2019/10/12/junior-manual/"/>
    <id>https://jordenbruce.com/2019/10/12/junior-manual/</id>
    <published>2019-10-12T12:54:02.000Z</published>
    <updated>2019-10-12T13:51:19.048Z</updated>
    
    <content type="html"><![CDATA[<p>这套初级手册是基于 Hadoop+Hive+Sqoop+Airflow 编写的，主要目的是对数据仓库有个宏观的认识，包括数据采集、ETL流程、任务调度和数据模型等。当中的每一个环节都有着丰富的内容，会在后期的中级手册与高级手册进行讨论，欢迎大家来围观。<br><a id="more"></a></p><h2 id="0x00-环境搭建"><a href="#0x00-环境搭建" class="headerlink" title="0x00 环境搭建"></a>0x00 环境搭建</h2><ul><li><a href="https://jordenbruce.com/2019/09/15/hadoop-release/">Hadoop发行版的选取</a></li><li><a href="https://jordenbruce.com/2019/09/15/hadoop-build/">编译Hadoop源码包</a></li><li><a href="https://jordenbruce.com/2019/09/15/hadoop-install/">手动搭建Hadoop分布式运行环境</a></li><li><a href="https://jordenbruce.com/2019/09/15/hive-install/">手动搭建Hive开发环境</a></li><li><a href="https://jordenbruce.com/2019/09/15/sqoop-install/">手动搭建Sqoop开发环境</a></li></ul><h2 id="0x01-工具使用"><a href="#0x01-工具使用" class="headerlink" title="0x01 工具使用"></a>0x01 工具使用</h2><ul><li><a href="https://jordenbruce.com/2019/09/20/hadoop-cli/">hadoop命令行的常用操作</a></li><li><a href="https://jordenbruce.com/2019/09/22/hdfs-cli/">hdfs命令行的常用操作</a></li><li><a href="https://jordenbruce.com/2019/09/22/yarn-cli/">yarn命令行的常用操作</a></li><li><a href="https://jordenbruce.com/2019/09/23/hql-table/">HiveQL的Table常用操作</a></li><li><a href="https://jordenbruce.com/2019/09/24/hql-dml/">HiveQL的导入与导出</a></li><li><a href="https://jordenbruce.com/2019/09/26/hql-select/">HiveQL的Select语句</a></li><li><a href="https://www.cnblogs.com/liupengpengg/p/7908274.html" target="_blank" rel="noopener">HiveQL的Join语句</a></li><li><a href="https://jordenbruce.com/2019/09/30/hql-function/">HiveQL的函数概览</a></li><li><a href="https://jordenbruce.com/2019/10/05/hql-function-math/">HiveQL的数学函数</a></li><li><a href="https://jordenbruce.com/2019/10/05/hql-function-date/">HiveQL的日期函数</a></li><li><a href="https://jordenbruce.com/2019/10/05/hql-function-string/">HiveQL的字符串函数</a></li><li><a href="https://jordenbruce.com/2019/10/08/sqoop-cli/">Sqoop命令行的导入与导出</a></li></ul><h2 id="0x02-数据模型"><a href="#0x02-数据模型" class="headerlink" title="0x02 数据模型"></a>0x02 数据模型</h2><ul><li><a href="https://jordenbruce.com/2019/10/09/growth-model/">简化版的增长模型</a></li></ul><h2 id="0x03-任务流调度"><a href="#0x03-任务流调度" class="headerlink" title="0x03 任务流调度"></a>0x03 任务流调度</h2><ul><li><a href="https://jordenbruce.com/2019/10/10/workflow/">常见的任务调度系统</a></li></ul><h2 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h2><p>除了以上提到的主题之外，还有数据管理（元数据，计算管理，存储和成本管理，数据质量），数据应用（接口服务，报表服务，应用服务），数据挖掘等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这套初级手册是基于 Hadoop+Hive+Sqoop+Airflow 编写的，主要目的是对数据仓库有个宏观的认识，包括数据采集、ETL流程、任务调度和数据模型等。当中的每一个环节都有着丰富的内容，会在后期的中级手册与高级手册进行讨论，欢迎大家来围观。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="modeling" scheme="https://jordenbruce.com/tags/modeling/"/>
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
      <category term="workflow" scheme="https://jordenbruce.com/tags/workflow/"/>
    
      <category term="sqoop" scheme="https://jordenbruce.com/tags/sqoop/"/>
    
  </entry>
  
  <entry>
    <title>常见的任务调度系统</title>
    <link href="https://jordenbruce.com/2019/10/10/workflow/"/>
    <id>https://jordenbruce.com/2019/10/10/workflow/</id>
    <published>2019-10-10T13:32:00.000Z</published>
    <updated>2019-10-12T13:24:28.607Z</updated>
    
    <content type="html"><![CDATA[<p>离线数据仓库的最大特点是离线，也就是说数据会经过ETL处理之后才显示，最常见的形式是T+1，比如隔天出数。而提前设计的ETL任务一般需要在凌晨去运行，数据工程师不可能每天凌晨起来执行任务，这时候就需要有任务调度系统了。它不仅要能够定时地启动任务，还要能够管理任务之间复杂的依赖关系，使得成千上万的任务有条不紊地运行起来，保证数据能够在预定时间（比如上班前）能够出来，方便其他同事进行查阅。<br><a id="more"></a></p><p>由于任务调度系统太重要了，很多有开发能力的公司都选择自主研发。尽管需要一些成本，但是为了稳定性和个性化需求也是值得的。当然，对于一些初创型的互联网公司，市场上也有很多开源的任务调度系统，它们经过反复迭代，稳定性有了很大提升，甚至达到了企业级标准。接下来将介绍几款开源的任务调度系统，它们都分别在各大互联网公司有着成功案例，请放心使用。</p><h2 id="0x00-Luigi"><a href="#0x00-Luigi" class="headerlink" title="0x00 Luigi"></a>0x00 Luigi</h2><p>Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.</p><p>github: <a href="https://github.com/spotify/luigi" target="_blank" rel="noopener">https://github.com/spotify/luigi</a><br>documentation: <a href="https://luigi.readthedocs.io/en/stable/" target="_blank" rel="noopener">https://luigi.readthedocs.io/en/stable/</a></p><h2 id="0x01-Azkaban"><a href="#0x01-Azkaban" class="headerlink" title="0x01 Azkaban"></a>0x01 Azkaban</h2><p>Azkaban is a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.</p><p>github: <a href="https://github.com/azkaban/azkaban" target="_blank" rel="noopener">https://github.com/azkaban/azkaban</a><br>documentation: <a href="https://azkaban.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://azkaban.readthedocs.io/en/latest/</a></p><h2 id="0x02-Apache-Airflow"><a href="#0x02-Apache-Airflow" class="headerlink" title="0x02 Apache Airflow"></a>0x02 Apache Airflow</h2><p>Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.</p><p>When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.</p><p>Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.</p><p>github: <a href="https://github.com/apache/airflow" target="_blank" rel="noopener">https://github.com/apache/airflow</a><br>documentation: <a href="https://airflow.apache.org/" target="_blank" rel="noopener">https://airflow.apache.org/</a></p><h2 id="0x03-Dolphin-Scheduler"><a href="#0x03-Dolphin-Scheduler" class="headerlink" title="0x03 Dolphin Scheduler"></a>0x03 Dolphin Scheduler</h2><p>Dolphin Scheduler is a distributed and easy-to-expand visual DAG workflow scheduling system, dedicated to solving the complex dependencies in data processing, making the scheduling system out of the box for data processing. </p><p>github: <a href="https://github.com/apache/incubator-dolphinscheduler" target="_blank" rel="noopener">https://github.com/apache/incubator-dolphinscheduler</a><br>documentation: <a href="https://dolphinscheduler.apache.org/en-us/docs/user_doc/quick-start.html" target="_blank" rel="noopener">https://dolphinscheduler.apache.org/en-us/docs/user_doc/quick-start.html</a></p><h2 id="0x04-延伸"><a href="#0x04-延伸" class="headerlink" title="0x04 延伸"></a>0x04 延伸</h2><p>上述提到的开源任务调度系统，尽管功能很完善，但是易用性有待提高，更不能满足企业的个性化需求，建议进行二次开发。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/common-workflow-language/common-workflow-language/wiki/Existing-Workflow-systems" target="_blank" rel="noopener">Existing Workflow systems</a><br><a href="https://www.oschina.net/project/tag/327/task-schedule" target="_blank" rel="noopener">Job/Task Schedule</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;离线数据仓库的最大特点是离线，也就是说数据会经过ETL处理之后才显示，最常见的形式是T+1，比如隔天出数。而提前设计的ETL任务一般需要在凌晨去运行，数据工程师不可能每天凌晨起来执行任务，这时候就需要有任务调度系统了。它不仅要能够定时地启动任务，还要能够管理任务之间复杂的依赖关系，使得成千上万的任务有条不紊地运行起来，保证数据能够在预定时间（比如上班前）能够出来，方便其他同事进行查阅。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="workflow" scheme="https://jordenbruce.com/tags/workflow/"/>
    
  </entry>
  
  <entry>
    <title>简化版的增长模型</title>
    <link href="https://jordenbruce.com/2019/10/09/growth-model/"/>
    <id>https://jordenbruce.com/2019/10/09/growth-model/</id>
    <published>2019-10-09T13:00:04.000Z</published>
    <updated>2019-10-12T12:51:34.671Z</updated>
    
    <content type="html"><![CDATA[<p>在互联网环境下，存量市场的竞争越来越激烈，许多公司都将用户提到了战略高度。借助用户生命周期管理与用户画像等，实现对用户的精细化运营。而增长模型是支撑这些应用的基础数据模型，包括日活、新增与留存。<br><a id="more"></a></p><h2 id="0x00-表模型概览"><a href="#0x00-表模型概览" class="headerlink" title="0x00 表模型概览"></a>0x00 表模型概览</h2><p>这套增长模型是基于公司客户端日志来设计的，ETL处理流程如下：</p><p><img src="https://i.loli.net/2019/10/09/iyDBbVJae69HNsY.png" alt="增长模型ETL流程"></p><p>涉及的表有5张，请参考表说明：</p><table><thead><tr><th>表名</th><th>说明</th></tr></thead><tbody><tr><td>ods.com_client_log_di</td><td>Kafka上报的客户端日志</td></tr><tr><td>cdm.dwd_bhv_app_startup_di</td><td>APP启动信息表</td></tr><tr><td>cdm.dws_bhv_device_actuser_di</td><td>用户活跃信息表</td></tr><tr><td>cdm.dim_bhv_device_ds</td><td>用户全量信息表 (新增)</td></tr><tr><td>ads.dm_bhv_device_remain_di</td><td>用户留存信息表</td></tr></tbody></table><p>接下来会逐个介绍每个表的ETL逻辑。</p><h2 id="0x01-客户端日志"><a href="#0x01-客户端日志" class="headerlink" title="0x01 客户端日志"></a>0x01 客户端日志</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists ods.com_client_log_di (</span><br><span class="line">     log_id         bigint  comment &apos;全局唯一日志ID&apos;</span><br><span class="line">    ,log_timestamp  bigint  comment &apos;上报日志时间戳&apos;</span><br><span class="line">    ,device         string  comment &apos;唯一设备ID&apos;</span><br><span class="line">    ,os             string  comment &apos;操作系统&apos;</span><br><span class="line">    ,brand          string  comment &apos;品牌&apos;</span><br><span class="line">    ,model          string  comment &apos;机型&apos;</span><br><span class="line">    ,manufacturer   string  comment &apos;制造商&apos;</span><br><span class="line">    ,ip             string  comment &apos;客户端IP&apos;</span><br><span class="line">    ,network        string  comment &apos;网络类型&apos;</span><br><span class="line">    ,app_version    string  comment &apos;应用版本&apos;</span><br><span class="line">    ,package_name   string  comment &apos;包名&apos;</span><br><span class="line">    ,sdk_version    string  comment &apos;SDK版本&apos;</span><br><span class="line">    ,event          string  comment &apos;事件&apos;</span><br><span class="line">    ,page           string  comment &apos;页面&apos;</span><br><span class="line">    ,extend         map&lt;string,string&gt;  comment &apos;扩展属性集&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;客户端日志&apos;</span><br><span class="line">partitioned by (</span><br><span class="line">    dt string comment &apos;日期分区&apos;</span><br><span class="line">)</span><br><span class="line">stored as parquet</span><br><span class="line">tblproperties(&apos;parquet.compression&apos;=&apos;SNAPPY&apos;)</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>一般客户端日志的上报量非常大，采用 Kafka 入库 Hive 的方式，消费时延可以做到小时级别。</p><h2 id="0x02-APP启动信息表"><a href="#0x02-APP启动信息表" class="headerlink" title="0x02 APP启动信息表"></a>0x02 APP启动信息表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table cdm.dwd_bhv_app_startup_di partition (dt = &apos;$&#123;YESTERDAY&#125;&apos;)</span><br><span class="line">select </span><br><span class="line">     cast(log_id as string) as log_id</span><br><span class="line">    ,from_unixtime(log_timestamp div 1000, &apos;yyyy-MM-dd HH:mm:ss&apos;) as log_time </span><br><span class="line">    ,device</span><br><span class="line">    ,os</span><br><span class="line">    ,brand</span><br><span class="line">    ,model</span><br><span class="line">    ,manufacturer</span><br><span class="line">    ,ip</span><br><span class="line">    ,network</span><br><span class="line">    ,app_version</span><br><span class="line">    ,package_name</span><br><span class="line">    ,sdk_version</span><br><span class="line">    ,from_unixtime(unix_timestamp(), &apos;yyyy-MM-dd HH:mm:ss&apos;) as etl_time </span><br><span class="line">from ods.com_client_log_di</span><br><span class="line">where dt = &apos;$&#123;YESTERDAY&#125;&apos;</span><br><span class="line">    and event = &apos;app_startup&apos;</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x03-用户活跃信息表"><a href="#0x03-用户活跃信息表" class="headerlink" title="0x03 用户活跃信息表"></a>0x03 用户活跃信息表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table cdm.dws_bhv_device_actuser_di partition (dt = &apos;$&#123;YESTERDAY&#125;&apos;)</span><br><span class="line">select </span><br><span class="line">     device</span><br><span class="line">    ,os</span><br><span class="line">    ,ip</span><br><span class="line">    ,from_unixtime(unix_timestamp(), &apos;yyyy-MM-dd HH:mm:ss&apos;) as etl_time </span><br><span class="line">from (</span><br><span class="line">    select</span><br><span class="line">         device</span><br><span class="line">        ,os</span><br><span class="line">        ,ip</span><br><span class="line">        ,row_number() over(partition by device order by log_id asc) as rank</span><br><span class="line">    from cdm.dwd_bhv_app_startup_di</span><br><span class="line">    where dt = &apos;$&#123;YESTERDAY&#125;&apos;</span><br><span class="line">        and device is not null</span><br><span class="line">        and device != &apos;&apos;</span><br><span class="line">) t</span><br><span class="line">where rank = 1</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x04-用户全量信息表"><a href="#0x04-用户全量信息表" class="headerlink" title="0x04 用户全量信息表"></a>0x04 用户全量信息表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table cdm.dim_bhv_device_ds partition(dt = &apos;$&#123;YESTERDAY&#125;&apos;)</span><br><span class="line">select </span><br><span class="line">     coalesce(a.device,b.device) as device</span><br><span class="line">    ,coalesce(b.activate_date,a.dt) as activate_date</span><br><span class="line">    ,if(b.device is null,1,0) as is_new</span><br><span class="line">    ,case when b.device is not null and a.device is not null</span><br><span class="line">              then concat(b.actlist,&apos;,1&apos;)</span><br><span class="line">          when b.device is null and a.device is not null</span><br><span class="line">              then concat(repeat(&apos;0,&apos;,datediff(&apos;$&#123;YESTERDAY&#125;&apos;,&apos;2019-01-01&apos;)),&apos;1&apos;)</span><br><span class="line">          when b.device is not null and a.device is null</span><br><span class="line">              then concat(b.actlist,&apos;,0&apos;)</span><br><span class="line">          else &apos;other&apos; </span><br><span class="line">     end as actlist</span><br><span class="line">    ,from_unixtime(unix_timestamp(), &apos;yyyy-MM-dd HH:mm:ss&apos;) as etl_time </span><br><span class="line">from (</span><br><span class="line">    select dt, device</span><br><span class="line">    from cdm.dws_bhv_device_actuser_di</span><br><span class="line">    where dt = &apos;$&#123;YESTERDAY&#125;&apos;</span><br><span class="line">) a</span><br><span class="line">full join (</span><br><span class="line">    select dt, device, activate_date, actlist</span><br><span class="line">    from cdm.dim_bhv_device_ds</span><br><span class="line">    where dt = date_sub(&apos;$&#123;YESTERDAY&#125;&apos;,1)</span><br><span class="line">) b</span><br><span class="line">on a.device = b.device </span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x05-用户留存信息表"><a href="#0x05-用户留存信息表" class="headerlink" title="0x05 用户留存信息表"></a>0x05 用户留存信息表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table ads.dm_bhv_device_remain_di partition(dt) </span><br><span class="line">select </span><br><span class="line">     a.device</span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 1)  as remain_s1d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 2)  as remain_s2d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 3)  as remain_s3d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 4)  as remain_s4d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 5)  as remain_s5d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 6)  as remain_s6d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 7)  as remain_s7d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 14) as remain_s14d</span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 30) as remain_s30d</span><br><span class="line">    ,from_unixtime(unix_timestamp(), &apos;yyyy-MM-dd HH:mm:ss&apos;) as etl_time </span><br><span class="line">    ,a.dt</span><br><span class="line">from (</span><br><span class="line">    select device, dt</span><br><span class="line">    from cdm.dws_bhv_device_actuser_di</span><br><span class="line">    where dt in (date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-1),date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-2)</span><br><span class="line">                ,date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-3),date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-4)</span><br><span class="line">                ,date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-5),date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-6)</span><br><span class="line">                ,date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-7),date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-14)</span><br><span class="line">                ,date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-30))</span><br><span class="line">) a</span><br><span class="line">left join (</span><br><span class="line">    select dt, device, actlist </span><br><span class="line">    from cdm.dim_bhv_device_ds</span><br><span class="line">    where dt = &apos;$&#123;YESTERDAY&#125;&apos; </span><br><span class="line">) b</span><br><span class="line">on a.device = b.device </span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x06-延伸"><a href="#0x06-延伸" class="headerlink" title="0x06 延伸"></a>0x06 延伸</h2><p>正如开篇所说的，这套增长模式是简化版的，还可以衍生出很多的数据模型，比如新增留存信息表等。另外，每家公司对于日活、新增和留存的业务逻辑定义不一样，ETL处理流程会比本文复杂得多，比如剔除作弊用户等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在互联网环境下，存量市场的竞争越来越激烈，许多公司都将用户提到了战略高度。借助用户生命周期管理与用户画像等，实现对用户的精细化运营。而增长模型是支撑这些应用的基础数据模型，包括日活、新增与留存。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="modeling" scheme="https://jordenbruce.com/tags/modeling/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop命令行的导入与导出</title>
    <link href="https://jordenbruce.com/2019/10/08/sqoop-cli/"/>
    <id>https://jordenbruce.com/2019/10/08/sqoop-cli/</id>
    <published>2019-10-08T13:18:01.000Z</published>
    <updated>2019-10-08T15:33:38.689Z</updated>
    
    <content type="html"><![CDATA[<p>Sqoop是一种被设计为在Hadoop与关系数据库之间传输数据的工具。您可以使用Sqoop将数据从MySQL或Oracle等关系数据库管理系统（RDBMS）导入Hadoop分布式文件系统（HDFS），在Hadoop MapReduce中转换数据，然后将数据导出回RDBMS 。Sqoop使用MapReduce导入和导出数据，还提供了并行操作以及容错能力。<br><a id="more"></a></p><h2 id="0x00-sqoop命令行的语法"><a href="#0x00-sqoop命令行的语法" class="headerlink" title="0x00 sqoop命令行的语法"></a>0x00 sqoop命令行的语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">usage: sqoop COMMAND [ARGS]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br><span class="line"></span><br><span class="line">See &apos;sqoop help COMMAND&apos; for information on a specific command.</span><br></pre></td></tr></table></figure><p>每个命令的具体含义如下：</p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>list-databases</td><td>列出所有数据库名</td></tr><tr><td>list-tables</td><td>列出某个数据库下所有表</td></tr><tr><td>create-hive-table</td><td>生成与关系数据库表结构对应的hive表结构</td></tr><tr><td>eval</td><td>执行一个SQL语句并显示结果</td></tr><tr><td>import</td><td>将关系型数据库中的数据导入到HDFS（包括Hive，HBase）中</td></tr><tr><td>import-all-tables</td><td>导入某个数据库下所有表到HDFS中</td></tr><tr><td>import-mainframe</td><td>从一台主机中导入数据集至HDFS</td></tr><tr><td>export</td><td>从HDFS（包括Hive和HBase）中将数据导出到关系型数据库中</td></tr><tr><td>job</td><td>用来生成一个sqoop任务，生成后不会立即执行，需要手动执行</td></tr><tr><td>merge</td><td>将HDFS中不同目录下面的数据合并在一起并放入指定目录中</td></tr><tr><td>codegen</td><td>将关系型数据库中的表映射为一个Java类，在该类中有各列对应的各个字段</td></tr><tr><td>metastore</td><td>启动元数据服务</td></tr><tr><td>version</td><td>查看版本</td></tr><tr><td>help</td><td>查看帮助</td></tr></tbody></table><p>常用命令有：list-tables, import, export 。</p><h2 id="0x01-sqoop-import"><a href="#0x01-sqoop-import" class="headerlink" title="0x01 sqoop import"></a>0x01 sqoop import</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">-D mapred.job.queue.name=root.data \</span><br><span class="line">--connect jdbc:mysql://mdw01:3306/common \</span><br><span class="line">--username sqoop -P \</span><br><span class="line">--table employee \</span><br><span class="line">--mapreduce-job-name sqoop_import_table_full \</span><br><span class="line">--create-hive-table --hive-import \</span><br><span class="line">--hive-database default --hive-table ods_employee_ds</span><br></pre></td></tr></table></figure><p>上述命令是全量导入，sqoop 还支持增量导入。</p><h2 id="0x02-sqoop-export"><a href="#0x02-sqoop-export" class="headerlink" title="0x02 sqoop export"></a>0x02 sqoop export</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">-D mapred.job.queue.name=root.data \</span><br><span class="line">--connect jdbc:mysql://mdw01:3306/common \</span><br><span class="line">--username sqoop -P \</span><br><span class="line">--table employee \</span><br><span class="line">--mapreduce-job-name sqoop_export_table \</span><br><span class="line">--export-dir /user/hive/warehouse/ods_employee_ds</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html" target="_blank" rel="noopener">Sqoop User Guide</a><br><a href="https://www.jianshu.com/p/b1fa9d853c89" target="_blank" rel="noopener">Sqoop 命令与参数详解</a><br><a href="https://www.cnblogs.com/xiaodf/p/6030102.html" target="_blank" rel="noopener">Sqoop 使用手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sqoop是一种被设计为在Hadoop与关系数据库之间传输数据的工具。您可以使用Sqoop将数据从MySQL或Oracle等关系数据库管理系统（RDBMS）导入Hadoop分布式文件系统（HDFS），在Hadoop MapReduce中转换数据，然后将数据导出回RDBMS 。Sqoop使用MapReduce导入和导出数据，还提供了并行操作以及容错能力。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="sqoop" scheme="https://jordenbruce.com/tags/sqoop/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的字符串函数</title>
    <link href="https://jordenbruce.com/2019/10/05/hql-function-string/"/>
    <id>https://jordenbruce.com/2019/10/05/hql-function-string/</id>
    <published>2019-10-05T04:39:39.000Z</published>
    <updated>2019-10-05T06:25:24.081Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 进行数据清洗时，使用最多的函数应该属字符串函数了。主要有：字符编码，字符拼接，字符查找，字符格式化，字符处理，字符截取，字符构造，字符切分，字符替换，编辑距离等。<br><a id="more"></a></p><h2 id="0x00-字符编码"><a href="#0x00-字符编码" class="headerlink" title="0x00 字符编码"></a>0x00 字符编码</h2><ul><li>ascii - 第一个字符的ASCII码</li><li>base64 - 从二进制转换为基本64字符串</li><li>unbase64 - 从基本64字符串转换为二进制</li><li>encode - 从字符串加密为指定字符集的二进制</li><li>decode - 从二进制解码为指定字符集的字符串</li></ul><h2 id="0x01-字符构造"><a href="#0x01-字符构造" class="headerlink" title="0x01 字符构造"></a>0x01 字符构造</h2><ul><li>repeat - 字符组重复n次的字符串</li><li>space - n个空格的字符串</li></ul><h2 id="0x02-字符拼接"><a href="#0x02-字符拼接" class="headerlink" title="0x02 字符拼接"></a>0x02 字符拼接</h2><ul><li>concat - 按顺序串联字符组所得到的字符串</li><li>concat_ws - 以指定分隔符按顺序串联字符组所得到的字符串</li></ul><h2 id="0x03-字符格式化"><a href="#0x03-字符格式化" class="headerlink" title="0x03 字符格式化"></a>0x03 字符格式化</h2><ul><li>lower - 将所有字符都转换为小写形式的字符串</li><li>upper - 将所有字符都转换为大写形式的字符串</li><li>initcap - 首字母大写而其他字母小写的字符串</li><li>format_number</li><li>printf</li></ul><h2 id="0x04-字符查找"><a href="#0x04-字符查找" class="headerlink" title="0x04 字符查找"></a>0x04 字符查找</h2><ul><li>elt - 取第几个字符串</li><li>field - 第一个匹配上字符串的位置</li><li>find_in_set - 查找以逗号分隔字符串的第一个匹配位置</li><li>in_file - 判断字符串是否在文件中占一行</li><li>instr - 查找字符串中第一个字符组匹配上的位置</li><li>locate - 查找字符串中(某个位置之后)第一个字符组匹配上的位置</li><li>ngrams</li><li>context_ngrams</li></ul><h2 id="0x05-字符截取"><a href="#0x05-字符截取" class="headerlink" title="0x05 字符截取"></a>0x05 字符截取</h2><ul><li>substring (substr) - 截取从某个位置开始指定长度的字符串</li><li>substring_index</li><li>regexp_extract - 使用正则表达式提取指定位置的字符串</li><li>get_json_object - 以指定json路径截取json字符串的元素</li><li>parse_url - 截取URL中指定部分</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select regexp_extract(&apos;foothebar&apos;, &apos;foo(.*?)(bar)&apos;, 2);</span><br><span class="line">select get_json_object(src_json.json, &apos;$.owner&apos;) from src_json;</span><br><span class="line">select parse_url(&apos;http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&apos;, &apos;HOST&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x06-字符切分"><a href="#0x06-字符切分" class="headerlink" title="0x06 字符切分"></a>0x06 字符切分</h2><ul><li>split - 拆分指定字符(正则表达式)前后的字符串</li><li>str_to_map - 按照指定格式拆分字符串为Map结构</li><li>sentences</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select split(&apos;abc,dsf,wes&apos;,&apos;,&apos;);</span><br><span class="line">select str_to_map(&apos;abc:abc,dsf:dsf&apos;,&apos;,&apos;,&apos;:&apos;);</span><br><span class="line">select sentences(&apos;Hello there! How are you?&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x07-字符替换"><a href="#0x07-字符替换" class="headerlink" title="0x07 字符替换"></a>0x07 字符替换</h2><ul><li>replace</li><li>regexp_replace - 将按模式(正则表达式)匹配上的所有旧字符串替换为新字符串并返回</li><li>translate</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select regexp_replace(&quot;foobar&quot;, &quot;oo|ar&quot;, &quot;&quot;);</span><br></pre></td></tr></table></figure><h2 id="0x08-字符处理"><a href="#0x08-字符处理" class="headerlink" title="0x08 字符处理"></a>0x08 字符处理</h2><ul><li>length - 获取字符串的长度</li><li>trim - 去掉字符串左右两边的空格</li><li>ltrim - 去掉字符串左边的空格</li><li>rtrim - 去掉字符串右边的空格</li><li>reverse - 将字符串的所有字符反转</li><li>lpad - 在字符串左侧添加n个指定字符组</li><li>rpad - 在字符串右侧添加n个指定字符组</li><li>soundex</li></ul><h2 id="0x09-编辑距离"><a href="#0x09-编辑距离" class="headerlink" title="0x09 编辑距离"></a>0x09 编辑距离</h2><ul><li>levenshtein - 由一个字符串转成另一个字符串所需的最少编辑操作次数</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-StringFunctions" target="_blank" rel="noopener">String Functions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 进行数据清洗时，使用最多的函数应该属字符串函数了。主要有：字符编码，字符拼接，字符查找，字符格式化，字符处理，字符截取，字符构造，字符切分，字符替换，编辑距离等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的日期函数</title>
    <link href="https://jordenbruce.com/2019/10/05/hql-function-date/"/>
    <id>https://jordenbruce.com/2019/10/05/hql-function-date/</id>
    <published>2019-10-05T01:03:57.000Z</published>
    <updated>2019-10-05T06:28:04.931Z</updated>
    
    <content type="html"><![CDATA[<p>不管是数仓模型的日期维度，还是日常任务的调度时间，都会用到日期函数。主要有：当前时间，时间分段，时间加减，时间转换等。<br><a id="more"></a></p><h2 id="0x00-当前时间"><a href="#0x00-当前时间" class="headerlink" title="0x00 当前时间"></a>0x00 当前时间</h2><ul><li>current_timestamp - 当前时间戳</li><li>current_date - 当前日期</li><li>unix_timestamp</li></ul><h2 id="0x01-时间分段"><a href="#0x01-时间分段" class="headerlink" title="0x01 时间分段"></a>0x01 时间分段</h2><ul><li>year - 年</li><li>quarter - 季度</li><li>month - 月</li><li>day - 天</li><li>hour - 时</li><li>minute - 分</li><li>second - 秒</li><li>extract</li><li>weekofyear - 所属年的第几周</li><li>last_day - 日期所属月份的最后一天</li><li>next_day - 晚于start_date的下一个day_of_week</li><li>trunc - 截断为格式指定单位的日期 (月，年)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select hour(&apos;2019-10-05 09:38:37&apos;);</span><br><span class="line">select weekofyear(&apos;2019-10-05&apos;);</span><br><span class="line">select trunc(&apos;2019-10-05&apos;,&apos;YYYY&apos;);</span><br><span class="line">select trunc(&apos;2019-10-05&apos;,&apos;MM&apos;);</span><br><span class="line">select last_day(&apos;2019-10-05&apos;);</span><br><span class="line">select next_day(&apos;2019-10-05&apos;,&apos;FRIDAY&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x02-时间加减"><a href="#0x02-时间加减" class="headerlink" title="0x02 时间加减"></a>0x02 时间加减</h2><ul><li>date_add - 添加开始日期的天数</li><li>date_sub - 减去开始日期的天数</li><li>add_months - 起始日期之后num_months的日期</li><li>datediff - 从开始日期到结束日期的天数</li><li>months_between - 两个日期之间的月份数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select date_add(&apos;2019-10-05&apos;,3);</span><br><span class="line">select date_sub(&apos;2019-10-05&apos;,4);</span><br><span class="line">select add_months(&apos;2019-10-05&apos;,1,&apos;yyyy-MM-dd&apos;);</span><br><span class="line">select datediff(&apos;2019-12-31&apos;,&apos;2019-10-05&apos;);</span><br><span class="line">select months_between(&apos;2019-12-31&apos;,&apos;2019-10-05&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x03-时间转换"><a href="#0x03-时间转换" class="headerlink" title="0x03 时间转换"></a>0x03 时间转换</h2><ul><li>from_unixtime - 从时间戳转换为标准日期格式</li><li>unix_timestamp - 从标准日期格式转换为时间戳</li><li>to_date - 取时间戳字符串的日期部分</li><li>from_utc_timestamp</li><li>to_utc_timestamp</li><li>date_format</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select from_unixtime(1570240800, &apos;yyyy-MM-dd&apos;);</span><br><span class="line">select unix_timestamp(&apos;2019-10-05&apos;,&apos;yyyy-MM-dd&apos;);</span><br><span class="line">select to_date(&apos;2019-10-05 09:38:37&apos;);</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-DateFunctions" target="_blank" rel="noopener">Date Functions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不管是数仓模型的日期维度，还是日常任务的调度时间，都会用到日期函数。主要有：当前时间，时间分段，时间加减，时间转换等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的数学函数</title>
    <link href="https://jordenbruce.com/2019/10/05/hql-function-math/"/>
    <id>https://jordenbruce.com/2019/10/05/hql-function-math/</id>
    <published>2019-10-04T23:26:30.000Z</published>
    <updated>2019-10-05T06:28:13.497Z</updated>
    
    <content type="html"><![CDATA[<p>使用Hive进行数据分析时，经常会用到数学函数和聚合函数。Hive 支持的内置数学函数有很多，主要有随机函数，取整函数，数学函数，三角函数，进制函数，符号函数，位函数，多列最值函数，分桶函数等。<br><a id="more"></a></p><h2 id="0x00-随机函数"><a href="#0x00-随机函数" class="headerlink" title="0x00 随机函数"></a>0x00 随机函数</h2><ul><li>rand - 随机数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select cast(rand()*1000 as int);</span><br></pre></td></tr></table></figure><h2 id="0x01-取整函数"><a href="#0x01-取整函数" class="headerlink" title="0x01 取整函数"></a>0x01 取整函数</h2><ul><li>round - 保留几位小数</li><li>bround</li><li>floor - 向下取整</li><li>ceil - 向上取整</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select round(rand()*100, 2);</span><br><span class="line">select floor(rand()*100);</span><br><span class="line">select ceil(rand()*100);</span><br></pre></td></tr></table></figure><h2 id="0x02-数学函数"><a href="#0x02-数学函数" class="headerlink" title="0x02 数学函数"></a>0x02 数学函数</h2><ul><li>log - 对数</li><li>log2 - 以2为底的对数</li><li>log10 - 以10为底的对数</li><li>ln - 以e为底的对数</li><li>pow - 指数</li><li>exp - 以e为底的指数</li><li>sqrt - 平方根</li><li>cbrt - 立方根</li><li>e - 自然常数e</li><li>pi - 自然常数π</li><li>factorial - 阶乘</li></ul><h2 id="0x03-三角函数"><a href="#0x03-三角函数" class="headerlink" title="0x03 三角函数"></a>0x03 三角函数</h2><ul><li>sin - 正弦</li><li>cos - 余弦</li><li>tan - 正切</li><li>asin - 反正弦</li><li>acos - 反余弦</li><li>atan - 反正切</li><li>degrees - 从弧度转换为度</li><li>radians - 从度转换为弧度</li></ul><h2 id="0x04-进制函数"><a href="#0x04-进制函数" class="headerlink" title="0x04 进制函数"></a>0x04 进制函数</h2><ul><li>bin - 二进制</li><li>hex - 十六进制</li><li>unhex</li><li>conv - 进制转换</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select conv(cast(rand()*1000 as bigint), 10, 2);</span><br></pre></td></tr></table></figure><h2 id="0x05-符号函数"><a href="#0x05-符号函数" class="headerlink" title="0x05 符号函数"></a>0x05 符号函数</h2><ul><li>positive - 正数</li><li>negative - 负数</li><li>sign - 正负判断</li><li>abs - 绝对值</li></ul><h2 id="0x06-位函数"><a href="#0x06-位函数" class="headerlink" title="0x06 位函数"></a>0x06 位函数</h2><ul><li>shiftleft - 按位左移</li><li>shiftright - 按位右移</li><li>shiftrightunsigned - 按位无符号右移</li></ul><h2 id="0x07-多列最值函数"><a href="#0x07-多列最值函数" class="headerlink" title="0x07 多列最值函数"></a>0x07 多列最值函数</h2><ul><li>greatest - 一行多列取最大值</li><li>least - 一行多列取最小值</li></ul><h2 id="0x07-分桶函数"><a href="#0x07-分桶函数" class="headerlink" title="0x07 分桶函数"></a>0x07 分桶函数</h2><ul><li>pmod - 取模的正数</li><li>width_bucket</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-MathematicalFunctions" target="_blank" rel="noopener">Mathematical Functions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Hive进行数据分析时，经常会用到数学函数和聚合函数。Hive 支持的内置数学函数有很多，主要有随机函数，取整函数，数学函数，三角函数，进制函数，符号函数，位函数，多列最值函数，分桶函数等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的函数概览</title>
    <link href="https://jordenbruce.com/2019/09/30/hql-function/"/>
    <id>https://jordenbruce.com/2019/09/30/hql-function/</id>
    <published>2019-09-30T11:24:49.000Z</published>
    <updated>2019-09-30T11:30:20.371Z</updated>
    
    <content type="html"><![CDATA[<p>Select语句主要有三部分：Select子句 (WITH,SELECT,FROM,WHERE,GROUP BY,HAVING,ORDER BY,LIMIT)，Join语句，Function函数。其中，Function函数 是最精彩也是最丰富的部分，不仅官方内置了大量函数，而且用户还可以自定义函数。本文以内置函数为主。<br><a id="more"></a></p><h2 id="0x00-函数语法"><a href="#0x00-函数语法" class="headerlink" title="0x00 函数语法"></a>0x00 函数语法</h2><p>在命令行环境下，使用以下命令查看当前Hive版本的所有函数及其语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SHOW FUNCTIONS;</span><br><span class="line">DESCRIBE FUNCTION &lt;function_name&gt;;</span><br><span class="line">DESCRIBE FUNCTION EXTENDED &lt;function_name&gt;;</span><br></pre></td></tr></table></figure><h2 id="0x01-运算符-Operators"><a href="#0x01-运算符-Operators" class="headerlink" title="0x01 运算符 (Operators)"></a>0x01 运算符 (Operators)</h2><p>内置的运算符有5大类：</p><ul><li>关系运算符 (Relational Operators)</li><li>算术运算符 (Arithmetic Operators)</li><li>逻辑运算符 (Logical Operators)</li><li>字符串运算符 (String Operators)</li><li>复杂类型 (Complex Types)</li></ul><p>大部分都是经常使用的，比如 =,is not null,+,and 等，以下几个运算符不常用却很重要的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select * from default.managed_user where user_id rlike &apos;^(A|B).*&apos;;</span><br><span class="line"></span><br><span class="line">select from_unixtime(log_timestamp div 1000, &apos;yyyy-MM-dd HH:mm:ss&apos;) as log_time;</span><br><span class="line"></span><br><span class="line">select array(&apos;hadoop&apos;, &apos;hive&apos;, &apos;hql&apos;);</span><br><span class="line">select named_struct(&apos;kid&apos;,3001, &apos;user_id&apos;,&apos;C001&apos;, &apos;user_name&apos;,&apos;sqoop&apos;);</span><br><span class="line">select map(&apos;kid&apos;,3001, &apos;user_id&apos;,&apos;C001&apos;, &apos;user_name&apos;,&apos;sqoop&apos;);</span><br></pre></td></tr></table></figure><p>对于 array,struct,map 三种复杂数据类型，都有对应的运算符取其内的数值。</p><h2 id="0x02-标准函数-UDF-Functions"><a href="#0x02-标准函数-UDF-Functions" class="headerlink" title="0x02 标准函数 (UDF, Functions)"></a>0x02 标准函数 (UDF, Functions)</h2><p>内置的标准函数有8大类：</p><ul><li>数学函数 (Mathematical Functions)</li><li>日期函数 (Date Functions)</li><li>字符串函数 (String Functions)</li><li>字符串掩码函数 (Data Masking Functions)</li><li>条件函数 (Conditional Functions)</li><li>类型转换函数 (Type Conversion Functions)</li><li>集合函数 (Collection Functions)</li><li>其他函数 (Misc. Functions)</li></ul><p>常用三大类函数是：数学函数，字符串函数，日期函数；后面会对每一大类函数单独写一篇手册，因为函数是在太丰富了。另外5类函数常用的有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">select mask_last_n(&apos;13912345678&apos;, 4);</span><br><span class="line"></span><br><span class="line">select if(2019&gt;2014, true, false);</span><br><span class="line">select coalesce(user_name, &apos;unknown&apos;);</span><br><span class="line">select case when user_name = &apos;hql&apos; then &apos;hive&apos; else &apos;other&apos; end;</span><br><span class="line"></span><br><span class="line">select cast(user_id as string);</span><br><span class="line"></span><br><span class="line">select size(map_column_name);</span><br><span class="line"></span><br><span class="line">select md5(&apos;HiveQL&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x03-聚合函数-UDAF-Aggregate-Functions"><a href="#0x03-聚合函数-UDAF-Aggregate-Functions" class="headerlink" title="0x03 聚合函数 (UDAF, Aggregate Functions)"></a>0x03 聚合函数 (UDAF, Aggregate Functions)</h2><p>内置的聚合函数主要有：</p><ul><li>常见聚合函数 (count,sum,avg,max,min)</li><li>统计聚合函数 (方差，标准差，协方差，相关系数，分位数，直方图)</li><li>字符串聚合函数 (collect_list,collect_set)</li></ul><h2 id="0x04-表生成函数-UDTF-Table-Generating-Functions"><a href="#0x04-表生成函数-UDTF-Table-Generating-Functions" class="headerlink" title="0x04 表生成函数 (UDTF, Table-Generating Functions)"></a>0x04 表生成函数 (UDTF, Table-Generating Functions)</h2><p>内置的表生成函数有：</p><ul><li>explode</li><li>posexplode</li><li>inline</li><li>stack</li><li>json_tuple</li><li>parse_url_tuple</li></ul><p>最常用的函数是 explode，使用方式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select explode(array(&apos;A&apos;,&apos;B&apos;,&apos;C&apos;)) as col;</span><br><span class="line">select tf.* from (select 0) t lateral view explode(array(&apos;A&apos;,&apos;B&apos;,&apos;C&apos;)) tf as col;</span><br><span class="line"></span><br><span class="line">select explode(map(&apos;A&apos;,10,&apos;B&apos;,20,&apos;C&apos;,30)) as (key,value);</span><br><span class="line">select tf.* from (select 0) t lateral view explode(map(&apos;A&apos;,10,&apos;B&apos;,20,&apos;C&apos;,30)) tf as key,value;</span><br></pre></td></tr></table></figure><h2 id="0x05-XPath特定函数-XPath-specific-Functions"><a href="#0x05-XPath特定函数-XPath-specific-Functions" class="headerlink" title="0x05 XPath特定函数 (XPath-specific Functions)"></a>0x05 XPath特定函数 (XPath-specific Functions)</h2><p>内置的XPath特定函数有：</p><ul><li>xpath</li><li>xpath_string</li><li>xpath_boolean</li><li>xpath_short</li><li>xpath_int</li><li>xpath_long</li><li>xpath_float</li><li>xpath_double</li><li>xpath_number</li></ul><h2 id="0x06-窗口和分析函数-Windowing-and-Analytics-Functions"><a href="#0x06-窗口和分析函数-Windowing-and-Analytics-Functions" class="headerlink" title="0x06 窗口和分析函数 (Windowing and Analytics Functions)"></a>0x06 窗口和分析函数 (Windowing and Analytics Functions)</h2><p>内置的窗口和分析函数主要有：</p><ul><li>窗口函数 (Windowing functions)</li><li>OVER子句 (The OVER clause)</li><li>分析函数 (Analytics functions)</li></ul><p>以下是两个常用的使用方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select a, sum(b) over (partition by c) from t;</span><br><span class="line"></span><br><span class="line">select a, row_number() over (partition by b order by d desc) from t;</span><br></pre></td></tr></table></figure><h2 id="0x07-参考"><a href="#0x07-参考" class="headerlink" title="0x07 参考"></a>0x07 参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">LanguageManual UDF</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+XPathUDF" target="_blank" rel="noopener">LanguageManual XPathUDF</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">LanguageManual WindowingAndAnalytics</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Select语句主要有三部分：Select子句 (WITH,SELECT,FROM,WHERE,GROUP BY,HAVING,ORDER BY,LIMIT)，Join语句，Function函数。其中，Function函数 是最精彩也是最丰富的部分，不仅官方内置了大量函数，而且用户还可以自定义函数。本文以内置函数为主。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的Select语句</title>
    <link href="https://jordenbruce.com/2019/09/26/hql-select/"/>
    <id>https://jordenbruce.com/2019/09/26/hql-select/</id>
    <published>2019-09-26T13:30:42.000Z</published>
    <updated>2019-09-26T15:22:43.596Z</updated>
    
    <content type="html"><![CDATA[<p>前面文章已经解决了数据存储的问题，这篇将介绍查询数据的Select语句。当表中的数据越来越多时，如何查询想要的数据，或者进行数据分析呢？<br><a id="more"></a></p><h2 id="0x00-语法"><a href="#0x00-语法" class="headerlink" title="0x00 语法"></a>0x00 语法</h2><p>官方的语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[WITH CommonTableExpression (, CommonTableExpression)*]</span><br><span class="line">SELECT [ALL | DISTINCT] select_expr, select_expr, ...</span><br><span class="line">  FROM table_reference</span><br><span class="line">  [WHERE where_condition]</span><br><span class="line">  [GROUP BY col_list]</span><br><span class="line">    [HAVING having_condition]</span><br><span class="line">  [ORDER BY col_list]</span><br><span class="line">  [CLUSTER BY col_list</span><br><span class="line">    | [DISTRIBUTE BY col_list] [SORT BY col_list]</span><br><span class="line">  ]</span><br><span class="line"> [LIMIT [offset,] rows]</span><br></pre></td></tr></table></figure><p>主要包括：WITH子句 (公共临时表)，SELECT子句 (查询结果的输出列)，FROM子句 (查询的数据源)，WHERE子句 (过滤条件)，GROUP BY子句 (分组列表)，HAVING子句 (分组的过滤条件)，ORDER BY子句 (排序列表)，LIMIT子句 (限制输出行数) 等。需要说明的是：</p><ul><li>select语句可以是union查询的一部分或者是另一个查询的子查询。</li><li>table_reference指示查询的输入。它可以是普通的表，视图，join构造或者是子查询。</li></ul><h2 id="0x01-单表查询"><a href="#0x01-单表查询" class="headerlink" title="0x01 单表查询"></a>0x01 单表查询</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select reg_date, count(kid) as cnt</span><br><span class="line">from default.ods_user</span><br><span class="line">where reg_date &gt;= &apos;2019-09-20&apos;</span><br><span class="line">group by reg_date</span><br><span class="line">  having count(kid) &gt; 1</span><br><span class="line">order by reg_date desc</span><br><span class="line">limit 3 </span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>请问，这条SQL语句表达的是什么业务场景？</p><h2 id="0x02-多表连接查询"><a href="#0x02-多表连接查询" class="headerlink" title="0x02 多表连接查询"></a>0x02 多表连接查询</h2><p>HiveQL支持的JOIN方式有：</p><ul><li>内连接 ([INNER] JOIN)</li><li>外连接 ({LEFT|RIGHT|FULL} [OUTER] JOIN)</li><li>左半连接 (LEFT SEMI JOIN)</li><li>笛卡尔积关联 (CROSS JOIN)</li></ul><p>每一种连接的使用说明，请参考 <a href="https://www.cnblogs.com/liupengpengg/p/7908274.html" target="_blank" rel="noopener">Hive中Join的类型和用法</a></p><h2 id="0x03-函数"><a href="#0x03-函数" class="headerlink" title="0x03 函数"></a>0x03 函数</h2><p>HiveQL内置函数主要有6大类：</p><ul><li>运算符 (Operators)</li><li>标准函数 (UDF, Functions)</li><li>聚合函数 (UDAF, Aggregate Functions)</li><li>表生成函数 (UDTF, Table-Generating Functions)</li><li>XPath特定函数 (XPath-specific Functions)</li><li>窗口和分析函数 (Windowing and Analytics Functions)</li></ul><p>有关常用函数的使用说明，下一篇再继续写。</p><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select" target="_blank" rel="noopener">LanguageManual Select</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins" target="_blank" rel="noopener">LanguageManual Joins</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">LanguageManual UDF</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面文章已经解决了数据存储的问题，这篇将介绍查询数据的Select语句。当表中的数据越来越多时，如何查询想要的数据，或者进行数据分析呢？&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的导入与导出</title>
    <link href="https://jordenbruce.com/2019/09/24/hql-dml/"/>
    <id>https://jordenbruce.com/2019/09/24/hql-dml/</id>
    <published>2019-09-24T13:21:29.000Z</published>
    <updated>2019-09-24T14:44:08.526Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇说完了Table常用操作，创建了几种表，可是表中还没有数据，这就需要数据导入；数据按照业务逻辑经过Hive各种处理之后，还需要数据导出，方便进一步的分析处理。<br><a id="more"></a></p><h2 id="0x00-综述"><a href="#0x00-综述" class="headerlink" title="0x00 综述"></a>0x00 综述</h2><p>常见的数据导入方式有：</p><ul><li>本地文件导入到Hive表</li><li>HDFS文件导入到Hive表</li><li>Hive表导入到一个Hive表</li><li>创建表的过程中从其他表导入</li><li>Hive表导入到多个Hive表</li></ul><p>常见的数据导出方式有：</p><ul><li>Hive表导出到本地文件系统</li><li>Hive表导出到HDFS</li><li>命令行导出到本地文件系统</li></ul><h2 id="0x01-导入"><a href="#0x01-导入" class="headerlink" title="0x01 导入"></a>0x01 导入</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/home/hadoop/user1.txt&apos; into table default.ods_user;</span><br><span class="line">load data inpath &apos;/ods/user2.txt&apos; into table default.ods_user;</span><br><span class="line"></span><br><span class="line">insert overwrite table default.managed_user </span><br><span class="line">select kid,user_id,user_name,reg_date from default.ods_user;</span><br><span class="line"></span><br><span class="line">insert overwrite table default.partitioned_user </span><br><span class="line">partition( dt = &apos;2019-09-20&apos;) </span><br><span class="line">select kid,user_id,user_name,reg_date </span><br><span class="line">from default.managed_user</span><br><span class="line">where reg_date = &apos;2019-09-20&apos;;</span><br><span class="line"></span><br><span class="line">create table default.user_20190921 as </span><br><span class="line">select kid,user_id,user_name,reg_date </span><br><span class="line">from default.ods_user </span><br><span class="line">where reg_date = &apos;2019-09-21&apos;;</span><br><span class="line"></span><br><span class="line">from default.managed_user </span><br><span class="line">insert overwrite table default.partitioned_user partition( dt = &apos;2019-09-21&apos;) </span><br><span class="line">select kid,user_id,user_name,reg_date where reg_date = &apos;2019-09-21&apos; </span><br><span class="line">insert overwrite table default.partitioned_user partition( dt = &apos;2019-09-22&apos;) </span><br><span class="line">select kid,user_id,user_name,reg_date where reg_date = &apos;2019-09-22&apos; ;</span><br></pre></td></tr></table></figure><h2 id="0x02-导出"><a href="#0x02-导出" class="headerlink" title="0x02 导出"></a>0x02 导出</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory &apos;/home/hadoop/user&apos; </span><br><span class="line">row format delimited fields terminated by &apos;|&apos; </span><br><span class="line">stored as textfile </span><br><span class="line">select kid,user_id,user_name,reg_date from default.managed_user;</span><br><span class="line"></span><br><span class="line">insert overwrite directory &apos;/external/user&apos; </span><br><span class="line">stored as parquet </span><br><span class="line">select kid,user_id,user_name,reg_date from default.managed_user;</span><br><span class="line"></span><br><span class="line">hive -e &quot;select * from default.managed_user&quot; &gt;&gt; /home/hadoop/user/source.txt</span><br></pre></td></tr></table></figure><h2 id="0x03-参考"><a href="#0x03-参考" class="headerlink" title="0x03 参考"></a>0x03 参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML" target="_blank" rel="noopener">LanguageManual DML</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇说完了Table常用操作，创建了几种表，可是表中还没有数据，这就需要数据导入；数据按照业务逻辑经过Hive各种处理之后，还需要数据导出，方便进一步的分析处理。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的Table常用操作</title>
    <link href="https://jordenbruce.com/2019/09/23/hql-table/"/>
    <id>https://jordenbruce.com/2019/09/23/hql-table/</id>
    <published>2019-09-23T00:28:31.000Z</published>
    <updated>2019-09-24T13:21:42.720Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive数据仓库软件有助于读取，写入和管理驻留在分布式存储中并使用SQL语法查询的大型数据集。而Table是Hive组织数据存储的主要数据单元，是一种结构化存储，用二维表结构来表示。<br><a id="more"></a></p><h2 id="0x00-create-table"><a href="#0x00-create-table" class="headerlink" title="0x00 create table"></a>0x00 create table</h2><p>经常使用的表有：内部表(managed table)，外部表(external table)，分区表(partitioned table)，临时表(temporary table)等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">--1. managed table</span><br><span class="line">create table if not exists default.managed_user (</span><br><span class="line">     kid       bigint  comment &apos;主键&apos;</span><br><span class="line">    ,user_id   string  comment &apos;用户编号&apos;</span><br><span class="line">    ,user_name string  comment &apos;用户名称&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;内部用户表&apos;</span><br><span class="line">stored as parquet</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">--2. external table</span><br><span class="line">create external table if not exists default.external_user (</span><br><span class="line">     kid       bigint  comment &apos;主键&apos;</span><br><span class="line">    ,user_id   string  comment &apos;用户编号&apos;</span><br><span class="line">    ,user_name string  comment &apos;用户名称&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;外部用户表&apos;</span><br><span class="line">stored as textfile</span><br><span class="line">location &apos;/external/user&apos;</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">--3. partitioned table</span><br><span class="line">create table if not exists default.partitioned_user (</span><br><span class="line">     kid       bigint  comment &apos;主键&apos;</span><br><span class="line">    ,user_id   string  comment &apos;用户编号&apos;</span><br><span class="line">    ,user_name string  comment &apos;用户名称&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;分区用户表&apos;</span><br><span class="line">partitioned by (</span><br><span class="line">    dt string comment &apos;日期分区&apos;</span><br><span class="line">)</span><br><span class="line">stored as parquet</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">--4. temporary table</span><br><span class="line">create temporary table if not exists default.temporary_user </span><br><span class="line">like default.managed_user</span><br><span class="line">;</span><br><span class="line">create temporary table if not exists default.temporary_user as </span><br><span class="line">select kid, user_id from default.partitioned_user where dt = &apos;2019-09-22&apos;</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x01-alter-table"><a href="#0x01-alter-table" class="headerlink" title="0x01 alter table"></a>0x01 alter table</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">alter table default.external_user rename to default.external_user_ds;</span><br><span class="line"></span><br><span class="line">alter table default.managed_user set tblproperties(&quot;skip.header.line.count&quot;=&quot;1&quot;);</span><br><span class="line"></span><br><span class="line">alter table default.managed_user add columns (reg_date string comment &apos;注册日期&apos;);</span><br><span class="line">alter table default.partitioned_user add columns (reg_date string comment &apos;注册日期&apos;) cascade;</span><br></pre></td></tr></table></figure><h2 id="0x02-describe-table-amp-show-table"><a href="#0x02-describe-table-amp-show-table" class="headerlink" title="0x02 describe table &amp; show table"></a>0x02 describe table &amp; show table</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">desc default.managed_user;</span><br><span class="line">desc formatted default.managed_user;</span><br><span class="line"></span><br><span class="line">show tables;</span><br><span class="line">show create table default.managed_user;</span><br><span class="line">show partitions default.partitioned_user;</span><br></pre></td></tr></table></figure><h2 id="0x03-truncate-table-amp-drop-table"><a href="#0x03-truncate-table-amp-drop-table" class="headerlink" title="0x03 truncate table &amp; drop table"></a>0x03 truncate table &amp; drop table</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">truncate table default.managed_user;</span><br><span class="line"></span><br><span class="line">drop table default.partitioned_user;</span><br><span class="line">alter table default.partitioned_user drop partition(dt = &apos;2019-09-22&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">LanguageManual DDL</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Hive数据仓库软件有助于读取，写入和管理驻留在分布式存储中并使用SQL语法查询的大型数据集。而Table是Hive组织数据存储的主要数据单元，是一种结构化存储，用二维表结构来表示。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>yarn命令行的常用操作</title>
    <link href="https://jordenbruce.com/2019/09/22/yarn-cli/"/>
    <id>https://jordenbruce.com/2019/09/22/yarn-cli/</id>
    <published>2019-09-22T09:17:06.000Z</published>
    <updated>2019-09-22T10:39:56.718Z</updated>
    
    <content type="html"><![CDATA[<p>hadoop-0.23中引入的新架构将JobTracker的两个主要功能划分为：资源管理和作业生命周期管理。新的ResourceManager管理应用程序的全局计算资源分配，每个应用程序的ApplicationMaster管理应用程序的调度和协调。应用程序可以是传统yarnuce作业中的单个作业，也可以是此类作业的DAG。在该计算机上管理用户进程的ResourceManager和每台计算机的NodeManager守护程序构成了计算结构。实际上，每个应用程序的ApplicationMaster是特定于框架的库，其任务是与来自ResourceManager的资源进行协商，并与NodeManager一起执行和监视任务。<br><a id="more"></a></p><h2 id="0x00-yarn命令行的语法"><a href="#0x00-yarn命令行的语法" class="headerlink" title="0x00 yarn命令行的语法"></a>0x00 yarn命令行的语法</h2><p>YARN命令由bin/yarn脚本调用。在不带任何参数的情况下运行yarn脚本会打印所有命令的描述。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Usage: yarn [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line">  CLASSNAME                             run the class named CLASSNAME</span><br><span class="line"> or</span><br><span class="line">  where COMMAND is one of:</span><br><span class="line">  resourcemanager -format-state-store   deletes the RMStateStore</span><br><span class="line">  resourcemanager                       run the ResourceManager</span><br><span class="line">  nodemanager                           run a nodemanager on each slave</span><br><span class="line">  timelineserver                        run the timeline server</span><br><span class="line">  rmadmin                               admin tools</span><br><span class="line">  sharedcachemanager                    run the SharedCacheManager daemon</span><br><span class="line">  scmadmin                              SharedCacheManager admin tools</span><br><span class="line">  version                               print the version</span><br><span class="line">  jar &lt;jar&gt;                             run a jar file</span><br><span class="line">  application                           prints application(s)</span><br><span class="line">                                        report/kill application</span><br><span class="line">  applicationattempt                    prints applicationattempt(s)</span><br><span class="line">                                        report</span><br><span class="line">  container                             prints container(s) report</span><br><span class="line">  node                                  prints node report(s)</span><br><span class="line">  queue                                 prints queue information</span><br><span class="line">  logs                                  dump container logs</span><br><span class="line">  classpath                             prints the class path needed to</span><br><span class="line">                                        get the Hadoop jar and the</span><br><span class="line">                                        required libraries</span><br><span class="line">  cluster                               prints cluster information</span><br><span class="line">  daemonlog                             get/set the log level for each</span><br><span class="line">                                        daemon</span><br><span class="line"></span><br><span class="line">Most commands print help when invoked w/o parameters.</span><br></pre></td></tr></table></figure><h2 id="0x01-yarn-queue"><a href="#0x01-yarn-queue" class="headerlink" title="0x01 yarn queue"></a>0x01 yarn queue</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn queue -status default</span><br></pre></td></tr></table></figure><h2 id="0x02-yarn-application"><a href="#0x02-yarn-application" class="headerlink" title="0x02 yarn application"></a>0x02 yarn application</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yarn application -list</span><br><span class="line">yarn application -status application_id</span><br><span class="line">yarn application -kill application_id</span><br></pre></td></tr></table></figure><h2 id="0x03-yarn-jar"><a href="#0x03-yarn-jar" class="headerlink" title="0x03 yarn jar"></a>0x03 yarn jar</h2><p>运行一个jar文件。用户可以将其YARN代码捆绑在jar文件中，并使用此命令执行它。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar wordcount /input/ /output/</span><br></pre></td></tr></table></figure><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-site/YarnCommands.html" target="_blank" rel="noopener">YARN Commands Guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;hadoop-0.23中引入的新架构将JobTracker的两个主要功能划分为：资源管理和作业生命周期管理。新的ResourceManager管理应用程序的全局计算资源分配，每个应用程序的ApplicationMaster管理应用程序的调度和协调。应用程序可以是传统yarnuce作业中的单个作业，也可以是此类作业的DAG。在该计算机上管理用户进程的ResourceManager和每台计算机的NodeManager守护程序构成了计算结构。实际上，每个应用程序的ApplicationMaster是特定于框架的库，其任务是与来自ResourceManager的资源进行协商，并与NodeManager一起执行和监视任务。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="yarn" scheme="https://jordenbruce.com/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>hdfs命令行的常用操作</title>
    <link href="https://jordenbruce.com/2019/09/22/hdfs-cli/"/>
    <id>https://jordenbruce.com/2019/09/22/hdfs-cli/</id>
    <published>2019-09-22T07:26:53.000Z</published>
    <updated>2019-09-22T08:57:26.965Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS是Hadoop应用程序使用的主要分布式存储。HDFS群集主要由管理文件系统元数据的NameNode和存储实际数据的DataNode组成。客户端与NameNode联系以获取文件元数据或文件修改，并直接与DataNode执行实际的文件I/O。<br><a id="more"></a></p><h2 id="0x00-hdfs命令行的语法"><a href="#0x00-hdfs命令行的语法" class="headerlink" title="0x00 hdfs命令行的语法"></a>0x00 hdfs命令行的语法</h2><p>所有HDFS命令均由bin/hdfs脚本调用。运行不带任何参数的hdfs脚本会打印所有命令的描述。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND</span><br><span class="line">       where COMMAND is one of:</span><br><span class="line">  dfs                  run a filesystem command on the file systems supported in Hadoop.</span><br><span class="line">  classpath            prints the classpath</span><br><span class="line">  namenode -format     format the DFS filesystem</span><br><span class="line">  secondarynamenode    run the DFS secondary namenode</span><br><span class="line">  namenode             run the DFS namenode</span><br><span class="line">  journalnode          run the DFS journalnode</span><br><span class="line">  zkfc                 run the ZK Failover Controller daemon</span><br><span class="line">  datanode             run a DFS datanode</span><br><span class="line">  dfsadmin             run a DFS admin client</span><br><span class="line">  haadmin              run a DFS HA admin client</span><br><span class="line">  fsck                 run a DFS filesystem checking utility</span><br><span class="line">  balancer             run a cluster balancing utility</span><br><span class="line">  jmxget               get JMX exported values from NameNode or DataNode.</span><br><span class="line">  mover                run a utility to move block replicas across</span><br><span class="line">                       storage types</span><br><span class="line">  oiv                  apply the offline fsimage viewer to an fsimage</span><br><span class="line">  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage</span><br><span class="line">  oev                  apply the offline edits viewer to an edits file</span><br><span class="line">  fetchdt              fetch a delegation token from the NameNode</span><br><span class="line">  getconf              get config values from configuration</span><br><span class="line">  groups               get the groups which users belong to</span><br><span class="line">  snapshotDiff         diff two snapshots of a directory or diff the</span><br><span class="line">                       current directory contents with a snapshot</span><br><span class="line">  lsSnapshottableDir   list all snapshottable dirs owned by the current user</span><br><span class="line">                                                Use -help to see options</span><br><span class="line">  portmap              run a portmap service</span><br><span class="line">  nfs3                 run an NFS version 3 gateway</span><br><span class="line">  cacheadmin           configure the HDFS cache</span><br><span class="line">  crypto               configure HDFS encryption zones</span><br><span class="line">  storagepolicies      list/get/set block storage policies</span><br><span class="line">  version              print the version</span><br><span class="line"></span><br><span class="line">Most commands print help when invoked w/o parameters.</span><br></pre></td></tr></table></figure><h2 id="0x01-hdfs-dfs"><a href="#0x01-hdfs-dfs" class="headerlink" title="0x01 hdfs dfs"></a>0x01 hdfs dfs</h2><p>在Hadoop支持的文件系统上运行文件系统命令。目前Hadoop兼容文件系统有：Amazon S3，Azure Blob Storage，OpenStack Swift 。常用操作命令与 hadoop fs 类似，也建议使用 hadoop fs 命令。</p><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-common/FileSystemShell.html" target="_blank" rel="noopener">File System Shell Guide</a></p><h2 id="0x02-hdfs-balancer"><a href="#0x02-hdfs-balancer" class="headerlink" title="0x02 hdfs balancer"></a>0x02 hdfs balancer</h2><p>HDFS数据不一定总是在整个DataNode上均匀地放置。一个常见的原因是向现有群集中添加了新的DataNode。HDFS为管理员提供了一个工具balancer，可以分析整个DataNode上的块放置和重新平衡数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs balancer -policy datanode -threshold 20 -include -f /tmp/hdfs-blancer.txt</span><br></pre></td></tr></table></figure><h2 id="0x03-hdfs-dfsadmin"><a href="#0x03-hdfs-dfsadmin" class="headerlink" title="0x03 hdfs dfsadmin"></a>0x03 hdfs dfsadmin</h2><p>dfsadmin 命令用于管理HDFS集群，这些命令常用于管理员。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -report -live</span><br><span class="line">hdfs dfsadmin -printTopology</span><br><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">hdfs dfsadmin -safemode get</span><br><span class="line">hdfs dfsadmin -setBalancerBandwidth 6250000</span><br></pre></td></tr></table></figure><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfsadmin" target="_blank" rel="noopener">HDFS Commands Guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HDFS是Hadoop应用程序使用的主要分布式存储。HDFS群集主要由管理文件系统元数据的NameNode和存储实际数据的DataNode组成。客户端与NameNode联系以获取文件元数据或文件修改，并直接与DataNode执行实际的文件I/O。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hdfs" scheme="https://jordenbruce.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>hadoop命令行的常用操作</title>
    <link href="https://jordenbruce.com/2019/09/20/hadoop-cli/"/>
    <id>https://jordenbruce.com/2019/09/20/hadoop-cli/</id>
    <published>2019-09-20T14:49:53.000Z</published>
    <updated>2019-09-22T08:57:48.186Z</updated>
    
    <content type="html"><![CDATA[<p>编译并安装Hadoop分布式运行环境之后，第一个要用到的命令行就是hadoop。需要注意的是：每个发行版的命令行语法有些不一样，可以通过<code>hadoop -help</code>进行查看。<br><a id="more"></a></p><h2 id="0x00-hadoop命令行的语法"><a href="#0x00-hadoop命令行的语法" class="headerlink" title="0x00 hadoop命令行的语法"></a>0x00 hadoop命令行的语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line">  CLASSNAME            run the class named CLASSNAME</span><br><span class="line"> or</span><br><span class="line">  where COMMAND is one of:</span><br><span class="line">  fs                   run a generic filesystem user client</span><br><span class="line">  version              print the version</span><br><span class="line">  jar &lt;jar&gt;            run a jar file</span><br><span class="line">                       note: please use &quot;yarn jar&quot; to launch</span><br><span class="line">                             YARN applications, not this command.</span><br><span class="line">  checknative [-a|-h]  check native hadoop and compression libraries availability</span><br><span class="line">  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively</span><br><span class="line">  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span><br><span class="line">  classpath            prints the class path needed to get the</span><br><span class="line">  credential           interact with credential providers</span><br><span class="line">                       Hadoop jar and the required libraries</span><br><span class="line">  daemonlog            get/set the log level for each daemon</span><br><span class="line">  trace                view and modify Hadoop tracing settings</span><br><span class="line"></span><br><span class="line">Most commands print help when invoked w/o parameters.</span><br></pre></td></tr></table></figure><p>每个命令的具体含义如下：</p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>version</td><td>打印hadoop版本</td></tr><tr><td>checknative</td><td>检测native库和压缩库的可用性</td></tr><tr><td>fs</td><td>hdfs命令行的客户端</td></tr><tr><td>jar</td><td>运行jar包里的mapreduce程序(推荐使用yarn jar)</td></tr><tr><td>distcp</td><td>用于大规模集群内部和集群之间拷贝的工具</td></tr><tr><td>archive</td><td>将小文件进行hadoop存档</td></tr><tr><td>classpath</td><td>打印类路径</td></tr><tr><td>credential</td><td>管理凭证供应商</td></tr><tr><td>daemonlog</td><td>获取/设置每个守护程序的日志级别</td></tr><tr><td>trace</td><td>查看和修改Hadoop跟踪设置</td></tr></tbody></table><p>其中，最常用的有 fs jar archive distcp 。</p><h2 id="0x01-hadoop-fs"><a href="#0x01-hadoop-fs" class="headerlink" title="0x01 hadoop fs"></a>0x01 hadoop fs</h2><p>调用文件系统(FS)Shell命令应使用 bin/hadoop fs <args>的形式。所有的的FS shell命令使用URI路径作为参数。URI格式是scheme://authority/path。对HDFS文件系统，scheme是hdfs，对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。一个HDFS文件或目录比如/parent/child可以表示成hdfs://namenode:namenodeport/parent/child，或者更简单的/parent/child（假设你配置文件中的默认值是namenode:namenodeport）。大多数FS Shell命令的行为和对应的Unix Shell命令类似。</args></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /</span><br><span class="line">hadoop fs -mkdir /external</span><br><span class="line">hadoop fs -touchz /external/readme</span><br><span class="line">hadoop fs -put $&#123;HADOOP_HOME&#125;README.txt /external</span><br><span class="line">hadoop fs -du -h /external</span><br><span class="line">hadoop fs -find /external readme</span><br><span class="line">hadoop fs -tail /external/README.txt</span><br><span class="line">hadoop fs -rm /external/readme</span><br></pre></td></tr></table></figure><p>还有很多命令，这里就不一一演示了。</p><h2 id="0x02-hadoop-archive"><a href="#0x02-hadoop-archive" class="headerlink" title="0x02 hadoop archive"></a>0x02 hadoop archive</h2><p>Hadoop archives是特殊的档案格式。一个Hadoop archive对应一个文件系统目录。Hadoop archive的扩展名是*.har。Hadoop archive包含元数据（形式是_index和_masterindx）和数据（part-*）文件。_index文件包含了档案中的文件的文件名和位置信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop archive -archiveName readme.har -p / external /archive</span><br><span class="line">hadoop fs -ls -R har:///archive/readme.har</span><br><span class="line">hadoop fs -cat har:///archive/readme.har/external/README.txt</span><br></pre></td></tr></table></figure><h2 id="0x03-hadoop-distcp"><a href="#0x03-hadoop-distcp" class="headerlink" title="0x03 hadoop distcp"></a>0x03 hadoop distcp</h2><p>DistCp（分布式拷贝）是用于大规模集群内部和集群之间拷贝的工具。它使用Map/Reduce实现文件分发，错误处理和恢复，以及报告生成。它把文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。由于使用了Map/Reduce方法，这个工具在语义和执行上都会有特殊的地方。</p><p>DistCp最常用在集群之间的拷贝：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop distcp hdfs://nn1:8020/foo/bar hdfs://nn2:8020/bar/foo</span><br></pre></td></tr></table></figure><p>这条命令会把nn1集群的/foo/bar目录下的所有文件或目录名展开并存储到一个临时文件中，这些文件内容的拷贝工作被分配给多个map任务，然后每个TaskTracker分别执行从nn1到nn2的拷贝操作。注意DistCp使用绝对路径进行操作。</p><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-common/CommandsManual.html" target="_blank" rel="noopener">Hadoop Commands Guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;编译并安装Hadoop分布式运行环境之后，第一个要用到的命令行就是hadoop。需要注意的是：每个发行版的命令行语法有些不一样，可以通过&lt;code&gt;hadoop -help&lt;/code&gt;进行查看。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>手动搭建Sqoop开发环境</title>
    <link href="https://jordenbruce.com/2019/09/15/sqoop-install/"/>
    <id>https://jordenbruce.com/2019/09/15/sqoop-install/</id>
    <published>2019-09-15T08:16:47.000Z</published>
    <updated>2019-09-17T14:16:05.939Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Sqoop 是一种工具，用于在Apache Hadoop和结构化数据存储（如关系数据库）之间高效传输批量数据。<br><a id="more"></a></p><h2 id="0x00-解压文件"><a href="#0x00-解压文件" class="headerlink" title="0x00 解压文件"></a>0x00 解压文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -xf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">mv sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop-1.4.7</span><br><span class="line">ln -s /data/sqoop-1.4.7/ /data/sqoop</span><br></pre></td></tr></table></figure><h2 id="0x01-添加环境变量"><a href="#0x01-添加环境变量" class="headerlink" title="0x01 添加环境变量"></a>0x01 添加环境变量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line">#export SQOOP_HOME=/data/sqoop/</span><br><span class="line">#export PATH=$&#123;SQOOP_HOME&#125;/bin:$PATH</span><br><span class="line">source ~/.bashrc</span><br><span class="line">sqoop version</span><br></pre></td></tr></table></figure><h2 id="0x02-修改sqoop-env-sh配置文件"><a href="#0x02-修改sqoop-env-sh配置文件" class="headerlink" title="0x02 修改sqoop-env.sh配置文件"></a>0x02 修改sqoop-env.sh配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cd conf/</span><br><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">vim sqoop-env.sh</span><br><span class="line"></span><br><span class="line">#Set path to where bin/hadoop is available</span><br><span class="line">export HADOOP_COMMON_HOME=/data/hadoop/</span><br><span class="line"></span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/data/hadoop/</span><br><span class="line"></span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">#export HBASE_HOME=</span><br><span class="line"></span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">export HIVE_HOME=/data/hive/</span><br><span class="line"></span><br><span class="line">#Set the path for where zookeper config dir is</span><br><span class="line">#export ZOOCFGDIR=</span><br></pre></td></tr></table></figure><h2 id="0x03-修改configure-sqoop文件"><a href="#0x03-修改configure-sqoop文件" class="headerlink" title="0x03 修改configure-sqoop文件"></a>0x03 修改configure-sqoop文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd bin/</span><br><span class="line">vim configure-sqoop</span><br></pre></td></tr></table></figure><p>注释掉以下代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#if [ -z &quot;$&#123;HBASE_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/hbase&quot; ]; then</span><br><span class="line">#    HBASE_HOME=/usr/lib/hbase</span><br><span class="line">#  else</span><br><span class="line">#    HBASE_HOME=$&#123;SQOOP_HOME&#125;/../hbase</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br><span class="line">#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then</span><br><span class="line">#    HCAT_HOME=/usr/lib/hive-hcatalog</span><br><span class="line">#  elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then</span><br><span class="line">#    HCAT_HOME=/usr/lib/hcatalog</span><br><span class="line">#  else</span><br><span class="line">#    HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog</span><br><span class="line">#    if [ ! -d $&#123;HCAT_HOME&#125; ]; then</span><br><span class="line">#       HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog</span><br><span class="line">#    fi</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br><span class="line">#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/accumulo&quot; ]; then</span><br><span class="line">#    ACCUMULO_HOME=/usr/lib/accumulo</span><br><span class="line">#  else</span><br><span class="line">#    ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br><span class="line">#if [ -z &quot;$&#123;ZOOKEEPER_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/zookeeper&quot; ]; then</span><br><span class="line">#    ZOOKEEPER_HOME=/usr/lib/zookeeper</span><br><span class="line">#  else</span><br><span class="line">#    ZOOKEEPER_HOME=$&#123;SQOOP_HOME&#125;/../zookeeper</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br></pre></td></tr></table></figure><h2 id="0x04-拷贝数据库连接jar包"><a href="#0x04-拷贝数据库连接jar包" class="headerlink" title="0x04 拷贝数据库连接jar包"></a>0x04 拷贝数据库连接jar包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /data/hive/lib/mysql-connector-java-5.1.44-bin.jar ./lib/</span><br></pre></td></tr></table></figure><h2 id="0x05-启动测试"><a href="#0x05-启动测试" class="headerlink" title="0x05 启动测试"></a>0x05 启动测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root --password</span><br><span class="line">sqoop list-tables --connect jdbc:mysql://localhost:3306/hive --username root --password</span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://mdw01:3306/hive \</span><br><span class="line">--username hive --password mysql \</span><br><span class="line">--table TBLS \</span><br><span class="line">-m 1 \</span><br><span class="line">--hive-import \</span><br><span class="line">--create-hive-table \</span><br><span class="line">--hive-table hive_tables</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Sqoop 是一种工具，用于在Apache Hadoop和结构化数据存储（如关系数据库）之间高效传输批量数据。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="sqoop" scheme="https://jordenbruce.com/tags/sqoop/"/>
    
  </entry>
  
  <entry>
    <title>手动搭建Hive开发环境</title>
    <link href="https://jordenbruce.com/2019/09/15/hive-install/"/>
    <id>https://jordenbruce.com/2019/09/15/hive-install/</id>
    <published>2019-09-15T07:36:31.000Z</published>
    <updated>2019-09-17T14:16:28.583Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive 数据仓库软件有助于使用 SQL 读取，编写和管理驻留在分布式存储中的大型数据集。可以将结构投影到已存储的数据中。提供了命令行工具和 JDBC 驱动程序以将用户连接到 Hive<br><a id="more"></a></p><h2 id="0x00-安装-MySQL-5-6"><a href="#0x00-安装-MySQL-5-6" class="headerlink" title="0x00 安装 MySQL 5.6"></a>0x00 安装 MySQL 5.6</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">yum update gcc gcc-c++ make cmake openssl openssl-devel -y</span><br><span class="line">yum update bison bison-devel ncurses ncurses-devel zlib zlib-devel libaio libaio-devel -y</span><br><span class="line"></span><br><span class="line">yum install gcc gcc-c++ make cmake openssl openssl-devel -y</span><br><span class="line">yum install bison bison-devel ncurses ncurses-devel zlib zlib-devel libaio libaio-devel -y</span><br><span class="line"></span><br><span class="line">tar -xf MySQL-5.6.24-1.el6.x86_64.rpm-bundle.tar</span><br><span class="line">rpm -ihv MySQL-shared-compat-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">rpm -e mysql-libs-5.1.71-1.el6.x86_64</span><br><span class="line"></span><br><span class="line">groupadd mysql</span><br><span class="line">useradd mysql -s /sbin/nologin -M -g mysql</span><br><span class="line"></span><br><span class="line">rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">rpm -ihv MySQL-devel-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">rpm -ihv MySQL-shared-5.6.24-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">cat /root/.mysql_secret</span><br><span class="line">service mysql start</span><br><span class="line">/usr/bin/mysql_secure_installation --user=mysql</span><br></pre></td></tr></table></figure><p><em>注意：mysql默认字符集必须设置为 latin1</em></p><h2 id="0x01-配置Hive元数据库"><a href="#0x01-配置Hive元数据库" class="headerlink" title="0x01 配置Hive元数据库"></a>0x01 配置Hive元数据库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysql -uroot -p --default-character-set=latin1</span><br><span class="line">mysql&gt; CREATE USER &apos;hive&apos; IDENTIFIED BY &apos;mysql&apos;;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON hive.* TO &apos;hive&apos;@&apos;%&apos; WITH GRANT OPTION;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br><span class="line">mysql&gt; EXIT;</span><br></pre></td></tr></table></figure><h2 id="0x02-配置Hive"><a href="#0x02-配置Hive" class="headerlink" title="0x02 配置Hive"></a>0x02 配置Hive</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf apache-hive-1.2.2-bin.tar.gz -C /data/</span><br><span class="line">ln -s /data/apache-hive-1.2.2-bin/ /data/hive</span><br><span class="line">cd /data/hive/conf/</span><br><span class="line">cp hive-default.xml.template hive-site.xml</span><br><span class="line">vim hive-site.xml</span><br><span class="line"></span><br><span class="line">mv mysql-connector-java-5.1.44-bin.jar /data/hive/lib/</span><br><span class="line">cp lib/jline-2.12.jar /data/hadoop/share/hadoop/yarn/lib/</span><br><span class="line">vim ./bin/hive</span><br></pre></td></tr></table></figure><h2 id="0x03-启动Hive"><a href="#0x03-启动Hive" class="headerlink" title="0x03 启动Hive"></a>0x03 启动Hive</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line">source ~/.bashrc</span><br><span class="line"></span><br><span class="line">hadoop fs -mkdir       /tmp</span><br><span class="line">hadoop fs -mkdir -p    /user/hive/warehouse</span><br><span class="line">hadoop fs -chmod g+w   /tmp</span><br><span class="line">hadoop fs -chmod g+w   /user/hive/warehouse</span><br><span class="line"></span><br><span class="line">hive</span><br><span class="line">$HIVE_HOME/bin/beeline -u jdbc:hive2://</span><br></pre></td></tr></table></figure><h2 id="0x04-hive-site-xml"><a href="#0x04-hive-site-xml" class="headerlink" title="0x04 hive-site.xml"></a>0x04 hive-site.xml</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;jdbc:mysql://mdw01:3306/hive?characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;mysql&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt; </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="0x05-参考"><a href="#0x05-参考" class="headerlink" title="0x05 参考"></a>0x05 参考</h2><p><a href="https://blog.csdn.net/wjqwinn/article/details/52692308" target="_blank" rel="noopener">Hive在spark2.0.0启动时无法访问spark-assembly-*.jar的解决办法</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Hive 数据仓库软件有助于使用 SQL 读取，编写和管理驻留在分布式存储中的大型数据集。可以将结构投影到已存储的数据中。提供了命令行工具和 JDBC 驱动程序以将用户连接到 Hive&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>手动搭建Hadoop分布式运行环境</title>
    <link href="https://jordenbruce.com/2019/09/15/hadoop-install/"/>
    <id>https://jordenbruce.com/2019/09/15/hadoop-install/</id>
    <published>2019-09-15T04:56:42.000Z</published>
    <updated>2019-09-22T09:02:38.409Z</updated>
    
    <content type="html"><![CDATA[<p>当开始着手实践 Hadoop 时，安装 Hadoop 往往会成为新手的一道门槛。尽管安装其实很简单，书上有写到，官方网站也有 Hadoop 安装配置教程，但由于对 Linux 环境不熟悉，书上跟官网上简略的安装步骤新手往往 Hold 不住。加之网上不少教程也甚是坑，导致新手折腾老几天愣是没装好，很是打击学习热情。<br><a id="more"></a></p><h2 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h2><p>本教程详细记录了 hadoop 安装的全过程，还有配置文件的参数设置，一次性解决安装过程的所有问题。</p><h2 id="0x01-虚拟服务器"><a href="#0x01-虚拟服务器" class="headerlink" title="0x01 虚拟服务器"></a>0x01 虚拟服务器</h2><p>VMware Workstation 11.0</p><table><thead><tr><th>host</th><th>ip</th><th>os</th><th>role</th><th>cpu</th><th>memory</th><th>disk</th></tr></thead><tbody><tr><td>mdw01</td><td>192.168.100.186</td><td>CentOS 6.8 x64</td><td>master</td><td>1*2</td><td>8GB</td><td>30GB</td></tr><tr><td>sdw02</td><td>192.168.100.187</td><td>CentOS 6.8 x64</td><td>slave</td><td>1*2</td><td>4GB</td><td>30GB</td></tr><tr><td>sdw03</td><td>192.168.100.188</td><td>CentOS 6.8 x64</td><td>slave</td><td>1*2</td><td>4GB</td><td>30GB</td></tr></tbody></table><h2 id="0x02-系统配置"><a href="#0x02-系统配置" class="headerlink" title="0x02 系统配置"></a>0x02 系统配置</h2><p>(1) 修改主机名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br><span class="line">cat /etc/sysconfig/network</span><br><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p>(2) 关闭SELinux</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">vim /etc/selinux/config</span><br><span class="line">sestatus</span><br></pre></td></tr></table></figure><p>(3) 关闭iptables</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p>(4) 安装JDK</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/</span><br><span class="line">tar -xf jdk-8u112-linux-x64.tar.gz</span><br><span class="line">chown -R root:root jdk1.8.0_112/</span><br><span class="line">ln -s /opt/jdk1.8.0_112/ /opt/java</span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">java -version</span><br><span class="line">rm -f jdk-8u112-linux-x64.tar.gz</span><br></pre></td></tr></table></figure><p>(5) 免密登陆ssh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br><span class="line">/etc/init.d/sshd restart</span><br><span class="line"></span><br><span class="line">useradd hadoop</span><br><span class="line">passwd hadoop</span><br><span class="line"></span><br><span class="line">ls -l /etc/sudoers</span><br><span class="line">chmod 640 /etc/sudoers</span><br><span class="line">vim /etc/sudoers</span><br><span class="line">chmod 0440 /etc/sudoers</span><br><span class="line"></span><br><span class="line">su hadoop</span><br><span class="line">ssh-keygen</span><br><span class="line">cd ~/.ssh/</span><br><span class="line">cat id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line">chmod 700 ~/.ssh</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br><span class="line">scp authorized_keys hadoop@sdw02:~/.ssh/</span><br><span class="line">ssh hadoop@sdw03</span><br></pre></td></tr></table></figure><p>(6) 时间同步NTP服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rpm -q ntp</span><br><span class="line">chkconfig ntpd on</span><br><span class="line">ntpdate -u 202.112.10.36</span><br><span class="line">hwclock -w</span><br><span class="line">vim /etc/ntp.conf</span><br><span class="line">vim /etc/sysconfig/ntpd</span><br><span class="line">service ntpd start</span><br><span class="line">netstat -tlunp | grep ntp</span><br><span class="line">ntpq -p</span><br></pre></td></tr></table></figure><h2 id="0x03-安装Hadoop"><a href="#0x03-安装Hadoop" class="headerlink" title="0x03 安装Hadoop"></a>0x03 安装Hadoop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">chown -R hadoop:hadoop /data/</span><br><span class="line">cd /data/</span><br><span class="line">tar -xf hadoop-2.6.4.tar.gz</span><br><span class="line">ln -s /data/hadoop-2.6.4/ /data/hadoop</span><br><span class="line">vim ~/.bashrc</span><br><span class="line"></span><br><span class="line">cd /data/hadoop/etc/hadoop</span><br><span class="line">vim hadoop-env.sh</span><br><span class="line">#vim yarn-env.sh</span><br><span class="line">vim core-site.xml</span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">vim mapred-site.xml</span><br><span class="line">vim yarn-site.xml</span><br><span class="line">vim slaves</span><br><span class="line"></span><br><span class="line">cd /data/hadoop/bin/</span><br><span class="line">./hdfs namenode -format</span><br><span class="line">cd /data/hadoop/sbin/</span><br><span class="line">./start-dfs.sh</span><br><span class="line">./start-yarn.sh</span><br><span class="line">./mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">./yarn-daemon.sh start proxyserver</span><br><span class="line"></span><br><span class="line">./stop-all.sh</span><br></pre></td></tr></table></figure><p>配置文件的详细参数设置，请参考 <a href="https://github.com/jordenbruce/archive/blob/master/hadoop_conf_file.zip" target="_blank" rel="noopener">hadoop_conf_files</a></p><p>安装完成后，访问下HDFS和Yarn地址：<br>HDFS：<a href="http://mdw01:50070/dfshealth.html#tab-overview" target="_blank" rel="noopener">http://mdw01:50070/dfshealth.html#tab-overview</a><br>Yarn：<a href="http://mdw01:8088/cluster" target="_blank" rel="noopener">http://mdw01:8088/cluster</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当开始着手实践 Hadoop 时，安装 Hadoop 往往会成为新手的一道门槛。尽管安装其实很简单，书上有写到，官方网站也有 Hadoop 安装配置教程，但由于对 Linux 环境不熟悉，书上跟官网上简略的安装步骤新手往往 Hold 不住。加之网上不少教程也甚是坑，导致新手折腾老几天愣是没装好，很是打击学习热情。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>编译Hadoop源码包</title>
    <link href="https://jordenbruce.com/2019/09/15/hadoop-build/"/>
    <id>https://jordenbruce.com/2019/09/15/hadoop-build/</id>
    <published>2019-09-15T04:02:01.000Z</published>
    <updated>2019-09-15T08:31:16.981Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop社区不提供64位编译好的版本，只能用源码自行编译64位版本。学习一项技术从安装开始，学习Hadoop要从编译开始。<br><a id="more"></a></p><h2 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h2><p>本文档编译的hadoop版本是hadoop-2.6.4-src.tar.gz</p><p><strong>重要提示</strong>：源码包编译的官方文档是压缩包的根目录下BUILDING.txt说明书。</p><p><img src="https://i.loli.net/2019/09/15/V4LJv1n2lOpqiyg.png" alt="Build instructions for Hadoop"></p><p><strong>安装问题的引入</strong>：Hadoop社区不提供64位编译好的版本，只能用源码自行编译64位版本。学习一项技术从安装开始，学习Hadoop要从编译开始。</p><h2 id="0x01-编译环境说明"><a href="#0x01-编译环境说明" class="headerlink" title="0x01 编译环境说明"></a>0x01 编译环境说明</h2><p>操作系统：Red Hat Enterprise Linux Server release 6.5 (Santiago)<br>核心信息：Kernel 2.6.32-431.el6.x86_64 on an x86_64</p><h2 id="0x02-安装系统支持包"><a href="#0x02-安装系统支持包" class="headerlink" title="0x02 安装系统支持包"></a>0x02 安装系统支持包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum -y install autoconf automake libtool cmake</span><br><span class="line">yum -y install ncurses-devel</span><br><span class="line">yum -y install openssl-devel</span><br><span class="line">yum -y install lzo-devel zlib-devel gcc gcc-c++</span><br><span class="line">yum -y install gcc gcc-c++ make</span><br></pre></td></tr></table></figure><h2 id="0x03-组件安装"><a href="#0x03-组件安装" class="headerlink" title="0x03 组件安装"></a>0x03 组件安装</h2><p>将所有组件包上传到主机/usr/local/src/目录下。</p><p>（1）安装JDK</p><p><em>注意：只能用1.7，否则编译会出错。</em> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf jdk-7u80-linux-x64.tar.gz -C /opt/modules/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export JAVA_HOME=/opt/modules/jdk1.7.0_80</span><br><span class="line">export JRE_HOME=$JAVA_HOME/jre</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</span><br></pre></td></tr></table></figure><p>（2）安装Maven</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf apache-maven-3.3.1-bin.tar.gz -C /opt/modules/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export MAVEN_HOME=/opt/modules/apache-maven-3.3.1</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line">vi /opt/modules/apache-maven-3.3.1/conf/settings.xml</span><br></pre></td></tr></table></figure><p>更改maven资料库，在<mirrors>里添加如下内容：</mirrors></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">&lt;id&gt;nexus-osc&lt;/id&gt;</span><br><span class="line">&lt;mirrorOf&gt;*&lt;/mirrorOf&gt;</span><br><span class="line">&lt;name&gt;Nexus osc&lt;/name&gt;</span><br><span class="line">&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><p>在<code>&lt;profiles&gt;&lt;/profiles&gt;</code>内新添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;profile&gt;</span><br><span class="line">&lt;id&gt;jdk-1.7&lt;/id&gt;</span><br><span class="line">&lt;activation&gt;</span><br><span class="line">&lt;jdk&gt;1.7&lt;/jdk&gt;</span><br><span class="line">&lt;/activation&gt;</span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">&lt;id&gt;nexus&lt;/id&gt;</span><br><span class="line">&lt;name&gt;local private nexus&lt;/name&gt;</span><br><span class="line">&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</span><br><span class="line">&lt;releases&gt;</span><br><span class="line">&lt;enabled&gt;true&lt;/enabled&gt;</span><br><span class="line">&lt;/releases&gt;</span><br><span class="line">&lt;snapshots&gt;</span><br><span class="line">&lt;enabled&gt;false&lt;/enabled&gt;</span><br><span class="line">&lt;/snapshots&gt;</span><br><span class="line">&lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br><span class="line">&lt;pluginRepositories&gt;</span><br><span class="line">&lt;pluginRepository&gt;</span><br><span class="line">&lt;id&gt;nexus&lt;/id&gt;</span><br><span class="line">&lt;name&gt;local private nexus&lt;/name&gt;</span><br><span class="line">&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</span><br><span class="line">&lt;releases&gt;</span><br><span class="line">&lt;enabled&gt;true&lt;/enabled&gt;</span><br><span class="line">&lt;/releases&gt;</span><br><span class="line">&lt;snapshots&gt;</span><br><span class="line">&lt;enabled&gt;false&lt;/enabled&gt;</span><br><span class="line">&lt;/snapshots&gt;</span><br><span class="line">&lt;/pluginRepository&gt;</span><br><span class="line">&lt;/pluginRepositories&gt;</span><br><span class="line">&lt;/profile&gt;</span><br></pre></td></tr></table></figure><p>如果不是第一次编译，可以配置本地仓库：<br><code>&lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt;</code></p><p>（3）安装Findbugs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf findbugs-3.0.1.tar.gz -C /opt/modules/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export FINDBUGS_HOME=/opt/modules/findbugs-3.0.1</span><br><span class="line">export PATH=$PATH:$FINDBUGS_HOME/bin</span><br></pre></td></tr></table></figure><p>（4）安装ProtocolBuffer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tar xvf protobuf-2.5.0.tar.gz</span><br><span class="line">cd protobuf-2.5.0</span><br><span class="line">./configure --prefix=/opt/modules/protobuf</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">ldconfig</span><br><span class="line">protoc --version</span><br></pre></td></tr></table></figure><p>（5）<font color="red">上网</font>  </p><p>由于编译Hadoop过程中，Maven需要下载依赖库，所以必须保证主机能上网。</p><p>（6）安装Snappy（可选）</p><h2 id="0x04-源码编译"><a href="#0x04-源码编译" class="headerlink" title="0x04 源码编译"></a>0x04 源码编译</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">mvn -version</span><br><span class="line">findbugs -version</span><br><span class="line">protoc --version</span><br><span class="line">ping www.baidu.com</span><br><span class="line"></span><br><span class="line">tar zxvf hadoop-2.6.4-src.tar.gz -C /opt/</span><br><span class="line">cd hadoop-2.6.4-src/</span><br><span class="line">export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;</span><br><span class="line">mvn clean package -Pdist,native,docs -DskipTests -Dtar</span><br></pre></td></tr></table></figure><p>剩下的就交给电脑，人可以出去锻炼身体了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] Apache Hadoop Main ................................. SUCCESS [06:32 min]</span><br><span class="line">[INFO] Apache Hadoop Project POM .......................... SUCCESS [03:46 min]</span><br><span class="line">[INFO] Apache Hadoop Annotations .......................... SUCCESS [01:29 min]</span><br><span class="line">[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.329 s]</span><br><span class="line">[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [07:17 min]</span><br><span class="line">[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 57.156 s]</span><br><span class="line">[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [06:10 min]</span><br><span class="line">[INFO] Apache Hadoop Auth ................................. SUCCESS [05:18 min]</span><br><span class="line">[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 27.119 s]</span><br><span class="line">[INFO] Apache Hadoop Common ............................... SUCCESS [09:30 min]</span><br><span class="line">[INFO] Apache Hadoop NFS .................................. SUCCESS [  6.677 s]</span><br><span class="line">[INFO] Apache Hadoop KMS .................................. SUCCESS [04:45 min]</span><br><span class="line">[INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.040 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS ................................. SUCCESS [13:03 min]</span><br><span class="line">[INFO] Apache Hadoop HttpFS ............................... SUCCESS [04:10 min]</span><br><span class="line">[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [01:44 min]</span><br><span class="line">[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  5.085 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.033 s]</span><br><span class="line">[INFO] hadoop-yarn ........................................ SUCCESS [  0.059 s]</span><br><span class="line">[INFO] hadoop-yarn-api .................................... SUCCESS [01:25 min]</span><br><span class="line">[INFO] hadoop-yarn-common ................................. SUCCESS [01:29 min]</span><br><span class="line">[INFO] hadoop-yarn-server ................................. SUCCESS [  0.095 s]</span><br><span class="line">[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 42.211 s]</span><br><span class="line">[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [01:51 min]</span><br><span class="line">[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [  2.956 s]</span><br><span class="line">[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [  7.023 s]</span><br><span class="line">[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 23.905 s]</span><br><span class="line">[INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 45.162 s]</span><br><span class="line">[INFO] hadoop-yarn-client ................................. SUCCESS [  8.784 s]</span><br><span class="line">[INFO] hadoop-yarn-applications ........................... SUCCESS [  0.047 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  2.790 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [  2.169 s]</span><br><span class="line">[INFO] hadoop-yarn-site ................................... SUCCESS [  0.052 s]</span><br><span class="line">[INFO] hadoop-yarn-registry ............................... SUCCESS [  5.526 s]</span><br><span class="line">[INFO] hadoop-yarn-project ................................ SUCCESS [  5.919 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client ............................ SUCCESS [  0.083 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 25.201 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 19.914 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [  3.998 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 11.686 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [  8.481 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 28.587 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  1.978 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  6.412 s]</span><br><span class="line">[INFO] hadoop-mapreduce ................................... SUCCESS [  4.931 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 44.811 s]</span><br><span class="line">[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [  8.613 s]</span><br><span class="line">[INFO] Apache Hadoop Archives ............................. SUCCESS [  2.769 s]</span><br><span class="line">[INFO] Apache Hadoop Rumen ................................ SUCCESS [  6.654 s]</span><br><span class="line">[INFO] Apache Hadoop Gridmix .............................. SUCCESS [  5.080 s]</span><br><span class="line">[INFO] Apache Hadoop Data Join ............................ SUCCESS [  3.253 s]</span><br><span class="line">[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [  2.646 s]</span><br><span class="line">[INFO] Apache Hadoop Extras ............................... SUCCESS [  4.990 s]</span><br><span class="line">[INFO] Apache Hadoop Pipes ................................ SUCCESS [  8.460 s]</span><br><span class="line">[INFO] Apache Hadoop OpenStack support .................... SUCCESS [  5.232 s]</span><br><span class="line">[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [06:09 min]</span><br><span class="line">[INFO] Apache Hadoop Client ............................... SUCCESS [  8.045 s]</span><br><span class="line">[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  0.145 s]</span><br><span class="line">[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  7.135 s]</span><br><span class="line">[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.856 s]</span><br><span class="line">[INFO] Apache Hadoop Tools ................................ SUCCESS [  0.027 s]</span><br><span class="line">[INFO] Apache Hadoop Distribution ......................... SUCCESS [02:45 min]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 01:27 h</span><br><span class="line">[INFO] Finished at: 2016-11-05T15:02:45+08:00</span><br><span class="line">[INFO] Final Memory: 112M/369M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>编译成功后会打包，放在hadoop-dist/target目录下。</p><p><img src="https://i.loli.net/2019/09/15/lus6tF8BawgU7YK.png" alt="dist_target_dir"></p><p>hadoop-2.6.4.tar.gz 就是编译成功的二进制安装包，大功告成！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop社区不提供64位编译好的版本，只能用源码自行编译64位版本。学习一项技术从安装开始，学习Hadoop要从编译开始。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop发行版的选取</title>
    <link href="https://jordenbruce.com/2019/09/15/hadoop-release/"/>
    <id>https://jordenbruce.com/2019/09/15/hadoop-release/</id>
    <published>2019-09-15T01:19:22.000Z</published>
    <updated>2019-09-17T14:21:44.756Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop的开源协议决定了任何人可以对其进行修改，并作为开源或者商业版发布/销售。故而，目前Hadoop的发行版非常多，除了Apache的开源版本之外，还有Cloudera发行版(CDH)、Hortonworks发行版（HDP）、MapR等，这些发行版都是基于Apache Hadoop衍生出来的。<br><a id="more"></a></p><h2 id="0x00-综述"><a href="#0x00-综述" class="headerlink" title="0x00 综述"></a>0x00 综述</h2><p>其中，不收费的Hadoop发行版主要有三个，分别是：</p><ul><li>Apache基金会hadoop</li><li>Cloudera版本（Cloudera’s Distribution Including Apache Hadoop，简称“CDH”）</li><li>Hortonworks版本（Hortonworks Data Platform，简称“HDP”）</li></ul><p>在我任职过的公司当中，telecom使用了CDH，analysys使用了HDP，qtt使用了Apache Hadoop</p><h2 id="0x01-发行版的比较"><a href="#0x01-发行版的比较" class="headerlink" title="0x01 发行版的比较"></a>0x01 发行版的比较</h2><table><thead><tr><th>\</th><th>Apache Hadoop</th><th>CDH</th><th>HDP</th></tr></thead><tbody><tr><td>开源情况</td><td>100%开源</td><td>100%开源</td><td>100%开源</td></tr><tr><td>收费情况</td><td>完全免费</td><td>免费版和企业版</td><td>完全免费</td></tr><tr><td>管理工具</td><td>Apache Ambari</td><td>Cloudera Manager</td><td>Ambari</td></tr><tr><td>稳定性</td><td>中</td><td>高</td><td>高</td></tr><tr><td>运维成本</td><td>高</td><td>中</td><td>中</td></tr><tr><td>生态支持</td><td>兼容性差</td><td>完善</td><td>完善</td></tr></tbody></table><h2 id="0x02-选择决定"><a href="#0x02-选择决定" class="headerlink" title="0x02 选择决定"></a>0x02 选择决定</h2><p>考虑到大数据平台高效的部署和安装，中心化的配置管理，使用过程中的稳定性、兼容性、扩展性，<br>以及未来较为简单、高效的运维，遇到问题低廉的解决成本；建议使用第三方发行版本。</p><p>然而，本系列教程选取了Apache Hadoop社区版，考虑的是完全开源免费、社区活跃、文档与资料详实，便于深入学习。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Hadoop的开源协议决定了任何人可以对其进行修改，并作为开源或者商业版发布/销售。故而，目前Hadoop的发行版非常多，除了Apache的开源版本之外，还有Cloudera发行版(CDH)、Hortonworks发行版（HDP）、MapR等，这些发行版都是基于Apache Hadoop衍生出来的。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
  </entry>
  
</feed>
