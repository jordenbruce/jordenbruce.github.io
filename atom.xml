<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JordenBruce</title>
  
  <subtitle>A thousand miles begins with a single step.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jordenbruce.com/"/>
  <updated>2019-03-05T15:36:31.342Z</updated>
  <id>https://jordenbruce.com/</id>
  
  <author>
    <name>JordenBruce</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hive分区表</title>
    <link href="https://jordenbruce.com/2019/03/04/hive-partition/"/>
    <id>https://jordenbruce.com/2019/03/04/hive-partition/</id>
    <published>2019-03-04T14:57:21.000Z</published>
    <updated>2019-03-05T15:36:31.342Z</updated>
    
    <content type="html"><![CDATA[<p>分区作为Hive数据的一种组织形式，每个表可以有一个或多个分区键，用于确定数据如何被存储；例如，带有日期分区列ds的表T具有存储在HDFS中的<code>&lt;table location&gt;/ds=&lt;date&gt;</code>目录中的特定日期的数据文件。<br><a id="more"></a><br>分区允许Hive根据查询条件选择要扫描的分区数据，比如一个需要访问T表中满足<code>T.ds=&#39;2008-09-01&#39;</code>条件的查询，Hive只需扫描HDFS中<code>&lt;table location&gt;/ds=2008-09-01/</code>目录中的文件。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分区作为Hive数据的一种组织形式，每个表可以有一个或多个分区键，用于确定数据如何被存储；例如，带有日期分区列ds的表T具有存储在HDFS中的&lt;code&gt;&amp;lt;table location&amp;gt;/ds=&amp;lt;date&amp;gt;&lt;/code&gt;目录中的特定日期的数据文件。&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="https://jordenbruce.com/categories/Hive/"/>
    
    
      <category term="Partition" scheme="https://jordenbruce.com/tags/Partition/"/>
    
  </entry>
  
  <entry>
    <title>HQL内存溢出的参数调优</title>
    <link href="https://jordenbruce.com/2019/02/25/hive-oom/"/>
    <id>https://jordenbruce.com/2019/02/25/hive-oom/</id>
    <published>2019-02-24T17:26:20.000Z</published>
    <updated>2019-03-04T14:58:57.062Z</updated>
    
    <content type="html"><![CDATA[<p>我们在使用Hive进行ETL开发的过程中，关注更多的是使用HQL语言来准确地表达业务逻辑，而很少考虑到Hive对HQL语句的执行情况。当你辛辛苦苦地码完，将HQL语句扔给Hive去执行时，就有可能出现各种各样的报错，而其中一种比较常见的错误就是内存溢出（OOM，out of memory），通俗地讲就是内存不够。<br><a id="more"></a></p><h2 id="1-写在前面"><a href="#1-写在前面" class="headerlink" title="1. 写在前面"></a>1. 写在前面</h2><p>本文采用的软件版本如下：</p><ul><li>hive-2.0.1</li><li>hadoop-2.7.2</li></ul><p>hive使用MapReduce执行引擎，hadoop使用Yarn进行资源调度。</p><p>下文将从客户端提交HQL语句开始，Hive生成物理执行计划、Yarn资源分配、MapReduce执行，到执行结束，三个重点过程进行阐述，先理论再参数，希望OOM参数调优的问题得到收敛。</p><h2 id="2-Hive生成物理执行计划"><a href="#2-Hive生成物理执行计划" class="headerlink" title="2. Hive生成物理执行计划"></a>2. Hive生成物理执行计划</h2><p>先给出官网上关于Hive架构的经典流程图：</p><p><img src="https://i.loli.net/2019/02/24/5c72ad01e0c56.png" alt="Hive架构"></p><p>从这张图中，我们只需要明白一点即可：客户端提交的HQL语句，在Hive端的最终输出是物理执行计划，或者说是Job的有向无环图（a DAG of stages）。主要包括三种操作：</p><ul><li>MapReduce作业（a map/reduce job）</li><li>元数据操作（a metadata operation）</li><li>HDFS操作（an operation on HDFS）</li></ul><p>需要注意的是：这个过程主要是Hive优化器的执行，没有相关参数去控制内存的使用。当然，对于某些HQL语句适当地设置一些参数，可以得到更优的物理执行计划。比如常见的Map Join参数<code>hive.auto.convert.join</code>等。</p><h2 id="3-Yarn资源分配"><a href="#3-Yarn资源分配" class="headerlink" title="3. Yarn资源分配"></a>3. Yarn资源分配</h2><p>YARN是对Mapreduce V1重构得到的，有时候也成为MapReduce V2。由Hive生成物理执行计划，其中的MapReduce作业提交给Yarn来执行，详细的执行过程如下：</p><p><img src="https://i.loli.net/2019/02/24/5c72b4b3294ea.jpg" alt="MapReduce在Yarn下执行过程"></p><p>从上图可以看出，Yarn（以Container方式分配）控制着NodeManager、ApplicationMaster、Map和Reduce的内存使用，相关的内存参数有：</p><table><thead><tr><th>重要级别</th><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>中</td><td>yarn.nodemanager.resource.memory-mb</td><td>8192</td></tr><tr><td>中</td><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td></tr><tr><td>中</td><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td></tr><tr><td>中</td><td>yarn.scheduler.increment-allocation-mb</td><td>1024</td></tr><tr><td>高</td><td>yarn.app.mapreduce.am.resource.mb</td><td>1536</td></tr><tr><td>中</td><td>yarn.app.mapreduce.am.command-opts</td><td>-Xmx1024m</td></tr><tr><td>低</td><td>yarn.app.mapreduce.am.admin-command-opts</td><td></td></tr><tr><td>中</td><td>yarn.nodemanager.vmem-pmem-ratio</td><td>2.1</td></tr><tr><td>低</td><td>yarn.nodemanager.pmem-check-enabled</td><td>true</td></tr><tr><td>低</td><td>yarn.nodemanager.vmem-check-enabled</td><td>true</td></tr><tr><td>高</td><td>mapreduce.reduce.memory.mb</td><td>1024</td></tr><tr><td>中</td><td>mapreduce.reduce.java.opts</td><td></td></tr><tr><td>高</td><td>mapreduce.map.memory.mb</td><td>1024</td></tr><tr><td>中</td><td>mapreduce.map.java.opts</td></tr></tbody></table><h3 id="3-1-基础"><a href="#3-1-基础" class="headerlink" title="3.1 基础"></a>3.1 基础</h3><ul><li>NodeManager可用于分配的最大内存是yarn.nodemanager.resource.memory-mb；</li><li>Yarn的ResourceManger（简称RM）通过逻辑上的队列分配内存等资源给application，默认情况下RM允许最大AM申请Container资源为8192MB(“yarn.scheduler.maximum-allocation-mb“)，默认情况下的最小分配资源为1024M(“yarn.scheduler.minimum-allocation-mb“)，如果参数中需要的资源在此范围之外，在任务submit的时候会被直接拒绝掉；</li><li>AM只能以增量 (“yarn.scheduler.minimum-allocation-mb”) + (“yarn.scheduler.increment-allocation-mb”) 规整每个task需要的内存，并且申请的内存只能在（”yarn.scheduler.minimum-allocation-mb“）和(“yarn.scheduler.maximum-allocation-mb“) 的范围内向RM申请资源；</li><li>每个Map任务或Reduce任务分配的内存为mapreduce.reduce.memory.mb或mapreduce.map.memory.mb；</li></ul><h3 id="3-2-mapreduce-map-java-opts和mapreduce-map-memory-mb区别"><a href="#3-2-mapreduce-map-java-opts和mapreduce-map-memory-mb区别" class="headerlink" title="3.2 mapreduce.map.java.opts和mapreduce.map.memory.mb区别"></a>3.2 mapreduce.map.java.opts和mapreduce.map.memory.mb区别</h3><p>JVM进程跑在container中，mapreduce.map.java.opts能够通过Xmx设置JVM最大的heap的使用，一般设置为0.75倍的mapreduce.map.memory.mb ，因为需要为java code，非JVM内存使用等预留些空间；mapreduce.reduce.java.opts和mapreduce.reduce.memory.mb同理。</p><h3 id="3-3-虚拟内存"><a href="#3-3-虚拟内存" class="headerlink" title="3.3 虚拟内存"></a>3.3 虚拟内存</h3><p>默认的(“yarn.nodemanager.vmem-pmem-ratio“)设置为2.1，意味则map container或者reduce container分配的虚拟内存超过2.1倍的(“mapreduce.reduce.memory.mb“)或(“mapreduce.map.memory.mb“)就会被NM给KILL掉，如果 (“mapreduce.map.memory.mb”) 被设置为1536M那么总的虚拟内存为2.1*1536=3225.6MB</p><h3 id="3-4-内存检查"><a href="#3-4-内存检查" class="headerlink" title="3.4 内存检查"></a>3.4 内存检查</h3><p>如果虚拟内存检查被打开（yarn.nodemanager.vmem-check-enabled默认情况下为true），然后YARN将把抽取出来的容器及其子进程的VSIZE加起来和容器最大允许使用的虚拟内存进行比较。最大允许使用的虚拟内存是容器最大可使用的物理内存乘以 yarn.nodemanager.vmem-pmem-ratio（默认值是2.1）。所以，如果你的YARN容器配置的最大可使用物理内存为2GB，然后我们乘以 2.1 得到的就是容器最大可用的虚拟内存 4.2G 。</p><p>如果物理内存检查被打开（yarn.nodemanager.pmem-check-enabled默认情况为true），然后YARN将把抽取出来的容器及其子进程的RSS加起来和容器最大允许使用的物理内存进行比较。</p><p>如果物理内存或者虚拟内存其中一个的使用大于最大允许使用的，YARN将会被这个容器杀掉。</p><h3 id="3-5-参数全局图"><a href="#3-5-参数全局图" class="headerlink" title="3.5 参数全局图"></a>3.5 参数全局图</h3><p>参数多不要慌，下面来张图梳理下：</p><p><img src="https://i.loli.net/2019/02/25/5c72c07905cf3.jpg" alt="Yarn内存参数"></p><h2 id="4-MapReduce执行"><a href="#4-MapReduce执行" class="headerlink" title="4. MapReduce执行"></a>4. MapReduce执行</h2><p>MapReduce作业的重点是Shuffle过程，还是老套路，先给出官网上关于这个过程的经典流程图：</p><p><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/pNFrlaAqE9YU.png" alt="Shuffle过程"></p><p>当Map任务或Reduce任务以Container方式申请到相应的内存资源后，就进入了实际的执行过程中，其中涉及的参数有：</p><table><thead><tr><th>重要级别</th><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>高</td><td>mapreduce.job.maps</td><td>2</td></tr><tr><td>中</td><td>mapreduce.input.fileinputformat.split.minsize</td><td>1</td></tr><tr><td>中</td><td>dfs.blocksize</td><td>134217728</td></tr><tr><td>高</td><td>mapreduce.job.reduces</td><td>1</td></tr><tr><td>中</td><td>mapreduce.task.io.sort.mb</td><td>100</td></tr><tr><td>中</td><td>mapreduce.map.sort.spill.percent</td><td>0.80</td></tr><tr><td>中</td><td>mapreduce.task.io.sort.factor</td><td>10</td></tr><tr><td>中</td><td>mapreduce.map.output.compress</td><td>false</td></tr><tr><td>中</td><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>低</td><td>mapreduce.job.reduce.slowstart.completedmaps</td><td>0.05</td></tr><tr><td>中</td><td>mapreduce.reduce.shuffle.parallelcopies</td><td>5</td></tr><tr><td>高</td><td>mapreduce.reduce.shuffle.input.buffer.percent</td><td>0.70</td></tr></tbody></table><p>为了更好地理解每个参数作用的阶段，建议先阅读 <a href="https://jordenbruce.com/2019/02/19/hadoop-shuffle/">MapReduce之Shuffle过程详解</a>。</p><h3 id="4-1-Map任务"><a href="#4-1-Map任务" class="headerlink" title="4.1 Map任务"></a>4.1 Map任务</h3><p>（1）split分片：split是在逻辑上对输入数据进行的分片，并不会在磁盘上将其切分成分片进行存储。每个split都作为一个独立单位分配给一个map task去处理。决定split分片大小的参数有：</p><ul><li>mapreduce.job.maps</li><li>mapreduce.input.fileinputformat.split.minsize</li><li>dfs.blocksize (会话级别不可设置)</li></ul><p>（2）内存缓冲区：经过map处理后的键值对，不会立马写入磁盘，而是暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，设置缓冲区大小的参数有：</p><ul><li>mapreduce.task.io.sort.mb</li><li>mapreduce.map.sort.spill.percent</li></ul><p>（3）压缩：map端在写磁盘的时候采用压缩的方式将map的输出结果进行压缩是一个减少网络开销很有效的方法。其实，在Hadoop中早已为我们提供了一些压缩算法的实现，直接配置参数即可。</p><ul><li>mapreduce.map.output.compress</li><li>mapreduce.map.output.compress.codec</li></ul><h3 id="4-2-Reduce任务"><a href="#4-2-Reduce任务" class="headerlink" title="4.2 Reduce任务"></a>4.2 Reduce任务</h3><p>（1）文件拷贝：默认情况下，当整个MapReduce作业的所有已执行完成的Map Task任务数超过Map Task总数的 <code>mapreduce.job.reduce.slowstart.completedmaps</code> (默认为0.05) 后，ApplicationMaster便会开始调度执行Reduce Task任务。然后Reduce Task任务默认启动 <code>mapred.reduce.parallel.copies</code> (默认为5) 个MapOutputCopier线程到已完成的Map Task任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</p><p>（2）内存缓冲区：这个内存缓冲区大小的控制就不像map那样可以通过 <code>mapreduce.task.io.sort.mb</code> 来设定了，而是通过另外一个参数来设置：<code>mapred.job.shuffle.input.buffer.percent</code>（default 0.7）， 这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。</p><h2 id="5-HQL语句的日志输出"><a href="#5-HQL语句的日志输出" class="headerlink" title="5. HQL语句的日志输出"></a>5. HQL语句的日志输出</h2><p>经过漫长的理论铺垫，终于要到解决问题的时候了，HQL语句的内存溢出主要从日志分析开始。</p><ul><li>HQL语句的执行过程中，有哪些日志输出呢？分别存放在什么地方？如何分析出有用信息？</li><li>内存溢出包括哪几类？典型日志有哪些？调优什么参数可以解决？</li><li>小文件太多是如何产生的？调优什么参数可以合并小文件？</li></ul><p>等等一系列有关问题，且听下回分解。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Design" target="_blank" rel="noopener">Hive Architecture Overview</a><br><a href="https://segmentfault.com/a/1190000003777237" target="_blank" rel="noopener">Yarn下Mapreduce的内存参数理解</a><br><a href="https://jordenbruce.com/2019/02/19/hadoop-shuffle/">MapReduce之Shuffle过程详解</a><br><a href="https://blog.csdn.net/lazythinker/article/details/75497774" target="_blank" rel="noopener">HIVE参数调优（汇总）</a><br><a href="https://blog.csdn.net/aijiudu/article/details/72353510" target="_blank" rel="noopener">MapReduce过程详解及其性能优化</a><br><a href="https://my.oschina.net/OttoWu/blog/816049" target="_blank" rel="noopener">hadoop fair scheduler 的坑</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在使用Hive进行ETL开发的过程中，关注更多的是使用HQL语言来准确地表达业务逻辑，而很少考虑到Hive对HQL语句的执行情况。当你辛辛苦苦地码完，将HQL语句扔给Hive去执行时，就有可能出现各种各样的报错，而其中一种比较常见的错误就是内存溢出（OOM，out of memory），通俗地讲就是内存不够。&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="https://jordenbruce.com/categories/Hive/"/>
    
    
      <category term="OOM" scheme="https://jordenbruce.com/tags/OOM/"/>
    
  </entry>
  
  <entry>
    <title>YARN架构详解</title>
    <link href="https://jordenbruce.com/2019/02/21/hadoop-yarn/"/>
    <id>https://jordenbruce.com/2019/02/21/hadoop-yarn/</id>
    <published>2019-02-20T16:36:20.000Z</published>
    <updated>2019-02-21T12:48:37.517Z</updated>
    
    <content type="html"><![CDATA[<p>YARN（Yet Another Resource Negotiator）是一个通用的资源管理平台，可为各类计算框架提供资源的管理和调度。 其核心出发点是为了分离资源管理与作业调度/监控，实现分离的做法是拥有一个全局的资源管理器（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM），应用程序由一个作业（Job）或者Job的有向无环图（DAG）组成。<br><a id="more"></a></p><p>YARN可以将多种计算框架(如离线处理MapReduce、在线处理的Storm、迭代式计算框架Spark、流式处理框架S4等) 部署到一个公共集群中，共享集群的资源。并提供如下功能：</p><ul><li><p><strong>资源的统一管理和调度</strong>：集群中所有节点的资源(内存、CPU、磁盘、网络等)抽象为Container。计算框架需要资源进行运算任务时需要向YARN申请Container， YARN按照特定的策略对资源进行调度进行Container的分配。</p></li><li><p><strong>资源隔离</strong>：YARN使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。</p></li></ul><p>YARN是对Mapreduce V1重构得到的，有时候也成为MapReduce V2。 </p><p>YARN可以看成一个云操作系统，由一个ResourceManager和多个NodeManager组成， 它负责管理所有NodeManger上多维度资源， 并以Container(启动一个Container相当于启动一个进程)方式分配给应用程序启动ApplicationMaster(相当于主进程中运行逻辑) 或运行ApplicationMaster切分的各Task(相当于子进程中运行逻辑)。</p><h2 id="YARN体系架构"><a href="#YARN体系架构" class="headerlink" title="YARN体系架构"></a>YARN体系架构</h2><p>YARN架构如下图所示：<br><img src="https://i.loli.net/2019/02/20/5c6d021102002.png" alt="YARN架构"></p><p>YARN总体上是Master/Slave结构，主要由ResourceManager、NodeManager、 ApplicationMaster和Container等几个组件构成。</p><ul><li><p><strong>ResourceManager(RM)</strong>：负责对各NM上的资源进行统一管理和调度。将AM分配空闲的Container运行并监控其运行状态。对AM申请的资源请求分配相应的空闲Container。主要由两个组件构成：调度器和应用程序管理器：</p><ol><li>调度器(Scheduler)：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container，从而限定每个任务使用的资源量。Shceduler不负责监控或者跟踪应用程序的状态，也不负责任务因为各种原因而需要的重启（由ApplicationMaster负责）。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。<br>调度器是可插拔的，例如CapacityScheduler、FairScheduler。具体看下文的调度算法。</li><li>应用程序管理器(Applications Manager)：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动AM、监控AM运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。</li></ol></li><li><p><strong>NodeManager (NM)</strong>：NM是每个节点上的资源和任务管理器。它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自AM的Container 启动/停止等请求。</p></li><li><p><strong>ApplicationMaster (AM)</strong>：用户提交的应用程序均包含一个AM，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。MapReduce就是原生支持的一种框架，可以在YARN上运行Mapreduce作业。有很多分布式应用都开发了对应的应用程序框架，用于在YARN上运行任务，例如Spark，Storm等。如果需要，我们也可以自己写一个符合规范的YARN application。</p></li><li><p><strong>Container</strong>：Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container 表示的。 YARN会为每个任务分配一个Container且该任务只能使用该Container中描述的资源。</p></li></ul><h2 id="YARN应用工作流程"><a href="#YARN应用工作流程" class="headerlink" title="YARN应用工作流程"></a>YARN应用工作流程</h2><p>如下图所示用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：</p><ul><li>启动AM ，如下步骤1~3；</li><li>由AM创建应用程序为它申请资源并监控它的整个运行过程，直到运行完成，如下步骤4~7。</li></ul><p><img src="https://i.loli.net/2019/02/20/5c6d03e204bfc.png" alt="YARN工作流程"></p><ol><li><p>用户向YARN中提交应用程序，其中包括AM程序、启动AM的命令、命令参数、用户程序等；事实上，需要准确描述运行ApplicationMaster的unix进程的所有信息。提交工作通常由YarnClient来完成。</p></li><li><p>RM为该应用程序分配第一个Container，并与对应的NM通信，要求它在这个Container中启动AM；</p></li><li><p>AM首先向RM注册，这样用户可以直接通过RM査看应用程序的运行状态，运行状态通过 AMRMClientAsync.CallbackHandler的getProgress() 方法来传递给RM。 然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4〜7；</p></li><li><p>AM采用轮询的方式通过RPC协议向RM申请和领取资源；资源的协调通过 AMRMClientAsync异步完成,相应的处理方法封装在AMRMClientAsync.CallbackHandler中。</p></li><li><p>—旦AM申请到资源后，便与对应的NM通信，要求它启动任务；通常需要指定一个ContainerLaunchContext，提供Container启动时需要的信息。</p></li><li><p>NM为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</p></li><li><p>各个任务通过某个RPC协议向AM汇报自己的状态和进度，以让AM随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；ApplicationMaster与NM的通信通过NMClientAsync object来完成，容器的所有事件通过NMClientAsync.CallbackHandler来处理。例如启动、状态更新、停止等。</p></li><li><p>应用程序运行完成后，AM向RM注销并关闭自己。</p></li></ol><h2 id="YARN资源调度模型"><a href="#YARN资源调度模型" class="headerlink" title="YARN资源调度模型"></a>YARN资源调度模型</h2><p>YARN提供了一个资源管理平台能够将集群中的资源统一进行管理。所有节点上的多维度资源都会根据申请抽象为一个个Container。</p><p>YARN采用了双层资源调度模型：</p><ul><li><p>RM中的资源调度器将资源分配给各个AM：资源分配过程是异步的。资源调度器将资源分配给一个应用程序后，它不会立刻push给对应的AM，而是暂时放到一个缓冲区中，等待AM通过周期性的心跳主动来取；</p></li><li><p>AM领取到资源后再进一步分配给它内部的各个任务：不属于YARN平台的范畴，由用户自行实现。</p></li></ul><p>也就是说，ResourceManager分配集群资源的时候，以抽象的Container形式分配给各应用程序，至于应用程序的子任务如何使用这些资源，由应用程序自行决定。</p><p>YARN目前采用的资源分配算法有三种。但真实的调度器实现中还对算法做了一定程度的优化。</p><ol><li><p>Capacity Scheduler：该调度器用于在共享、多租户（multi-tenant）的集群环境中运行Hadoop应用，对运营尽可能友好的同时最大化吞吐量和效用。 </p></li><li><p>Fair Scheduler：公平调度FAIR，该算法的思想是尽可能地公平调度，即已分配资源量少的优先级高。也就是说，在考虑如何分配资源时，调度器尽可能使得每个应用程序都能够得到大致相当的资源。默认情况下，公平性只通过内存来衡量，但是可以配置成内存和CPU。 </p></li></ol><h2 id="YARN的资源管理"><a href="#YARN的资源管理" class="headerlink" title="YARN的资源管理"></a>YARN的资源管理</h2><ol><li><p>资源调度和隔离是yarn作为一个资源管理系统，最重要且最基础的两个功能。资源调度由resourcemanager完成，而资源隔离由各个nodemanager实现。</p></li><li><p>Resourcemanager将某个nodemanager上资源分配给任务（这就是所谓的“资源调度”）后，nodemanager需按照要求为任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础和保证，这就是所谓的资源隔离。</p></li><li><p>当谈及到资源时，我们通常指内存、cpu、io三种资源。Hadoop yarn目前为止仅支持cpu和内存两种资源管理和调度。</p></li><li><p>内存资源多少决定任务的生死，如果内存不够，任务可能运行失败；相比之下，cpu资源则不同，它只会决定任务的快慢，不会对任务的生死产生影响。</p></li></ol><p>以上内容来自：<br><a href="https://blog.csdn.net/bingduanlbd/article/details/51880019" target="_blank" rel="noopener">理解Hadoop YARN架构</a><br><a href="http://www.cnblogs.com/wcwen1990/p/6737985.html" target="_blank" rel="noopener">YARN架构设计详解</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YARN（Yet Another Resource Negotiator）是一个通用的资源管理平台，可为各类计算框架提供资源的管理和调度。 其核心出发点是为了分离资源管理与作业调度/监控，实现分离的做法是拥有一个全局的资源管理器（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM），应用程序由一个作业（Job）或者Job的有向无环图（DAG）组成。&lt;br&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://jordenbruce.com/categories/hadoop/"/>
    
    
      <category term="YARN" scheme="https://jordenbruce.com/tags/YARN/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce之Shuffle过程详解</title>
    <link href="https://jordenbruce.com/2019/02/19/hadoop-shuffle/"/>
    <id>https://jordenbruce.com/2019/02/19/hadoop-shuffle/</id>
    <published>2019-02-19T13:44:20.000Z</published>
    <updated>2019-02-20T16:38:31.622Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce计算模型一般包括两个重要的阶段：Map是映射，负责数据的过滤分发，将原始数据转化为键值对；Reduce是规约，将具有相同key值的value进行处理后再输出新的键值对作为最终结果。为了让Reduce可以并行处理Map的结果，必须对Map的输出进行一定的排序与分割，然后再交给对应的Reduce，而这个将Map输出进行进一步整理并交给Reduce的过程就是Shuffle。<br><a id="more"></a></p><h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p>Shuffle过程是MapReduce的核心，也被称为奇迹发生的地方。要想理解MapReduce，Shuffle是必须要了解的。这里先给出官网上关于这个过程的经典流程图：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/pNFrlaAqE9YU.png" alt="Shuffle过程"><br>上图是把MapReduce过程分为两个部分，而实际上从两边的Map和Reduce到中间的那一大块都属于Shuffle过程，也就是说，Shuffle过程有一部分是在Map端，有一部分是在Reduce端，下文也将会分两部分来介绍Shuffle过程。</p><p>对于Hadoop集群，当我们在运行作业时，大部分的情况下，map task与reduce task的执行是分布在不同的节点上的，因此，很多情况下，reduce执行时需要跨节点去拉取其他节点上的map task结果，这样造成了集群内部的网络资源消耗很严重，而且在节点的内部，相比于内存，磁盘IO对性能的影响是非常严重的。如果集群中运行的作业有很多，那么task的执行对于集群内部网络的资源消费非常大。因此，我们对于MapRedue作业Shuffle过程的期望是：</p><ul><li>完整地从map task端拉取数据到Reduce端；</li><li>在跨节点拉取数据时，尽可能地减少对带宽的不必要消耗；</li><li>减少磁盘IO对task执行的影响。</li></ul><h2 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h2><p>在进行海量数据处理时，外存文件数据I/O访问会成为一个制约系统性能的瓶颈，因此，Hadoop的Map过程实现的一个重要原则就是：计算靠近数据，这里主要指两个方面：</p><ol><li>代码靠近数据：<ul><li>原则：本地化数据处理（locality），即一个计算节点尽可能处理本地磁盘上所存储的数据；</li><li>尽量选择数据所在DataNode启动Map任务；</li><li>这样可以减少数据通信，提高计算效率；</li></ul></li><li>数据靠近代码：<ul><li>当本地没有数据处理时，尽可能从同一机架或最近其他节点传输数据进行处理（host选择算法）。</li></ul></li></ol><p>下面，我们分块去介绍Hadoop的Map过程，map的经典流程图如下：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/Hc0RIDhvW7CU.jpg" alt="map-shuffle"></p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ol><li>map task只读取split分片，split与block（hdfs的最小存储单位，默认为64MB）可能是一对一也能是一对多，但是对于一个split只会对应一个文件的一个block或多个block，不允许一个split对应多个文件的多个block；</li><li>这里切分和输入数据的时会涉及到InputFormat的文件切分算法和host选择算法。</li></ol><p>文件切分算法，主要用于确定InputSplit的个数以及每个InputSplit对应的数据段。FileInputFormat以文件为单位切分生成InputSplit，对于每个文件，由以下三个属性值决定其对应的InputSplit的个数：</p><ul><li>goalSize：它是根据用户期望的InputSplit数目计算出来的，即totalSize/numSplits。其中，totalSize为文件的总大小；numSplits为用户设定的Map Task个数，默认情况下是1；</li><li>minSize：InputSplit的最小值，由配置参数mapred.min.split.size确定，默认是1；</li><li>blockSize：文件在hdfs中存储的block大小，不同文件可能不同，默认是64MB。</li></ul><p>这三个参数共同决定InputSplit的最终大小，计算方法如下：<br><code>splitSize = max{minSize, min{gogalSize,blockSize}}</code></p><p>细节请参考《<a href="https://blog.csdn.net/xingliang_li/article/details/53285447" target="_blank" rel="noopener">FileInputFormat类中split切分算法和host选择算法介绍</a>》</p><h3 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h3><ul><li>作用：将map的结果发送到相应的reduce端，总的partition的数目等于reducer的数量。</li><li>实现功能：<ol><li>map输出的是key/value对，决定于当前的mapper的part交给哪个reduce的方法是：mapreduce提供的Partitioner接口，对key进行hash后，再以reducetask数量取模，然后到指定的job上（HashPartitioner，可以通过job.setPartitionerClass(MyPartition.class)自定义）。</li><li>然后将数据写入到内存缓冲区，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。key/value对以及Partition的结果都会被写入缓冲区。在写入之前，key与value值都会被序列化成字节数组。</li></ol></li><li>要求：负载均衡，效率；</li></ul><h3 id="spill（溢写）：sort-amp-combiner"><a href="#spill（溢写）：sort-amp-combiner" class="headerlink" title="spill（溢写）：sort &amp; combiner"></a>spill（溢写）：sort &amp; combiner</h3><ul><li>作用：把内存缓冲区中的数据写入到本地磁盘，在写入本地磁盘时先按照partition、再按照key进行排序（quick sort）；</li><li>注意：<ol><li>这个spill是由另外单独的线程来完成，不影响往缓冲区写map结果的线程；</li><li>内存缓冲区默认大小限制为100MB，它有个溢写比例（spill.percent），默认为0.8，当缓冲区的数据达到阈值时，溢写线程就会启动，先锁定这80MB的内存，执行溢写过程，maptask的输出结果还可以往剩下的20MB内存中写，互不影响。然后再重新利用这块缓冲区，因此Map的内存缓冲区又叫做环形缓冲区（两个指针的方向不会变，下面会详述）；</li><li>在将数据写入磁盘之前，先要对要写入磁盘的数据进行一次排序操作，先按&lt;key,value,partition&gt;中的partition分区号排序，然后再按key排序，这个就是sort操作，最后溢出的小文件是分区的，且同一个分区内是保证key有序的；</li></ol></li></ul><p>combine：执行combine操作要求开发者必须在程序中设置了combine（程序中通过job.setCombinerClass(myCombine.class)自定义combine操作）。</p><ul><li>程序中有两个阶段可能会执行combine操作：<ol><li>map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作（前提是作业中设置了这个操作）；</li><li>如果map输出比较大，溢出文件个数大于3（此值可以通过属性min.num.spills.for.combine配置）时，在merge的过程（多个spill文件合并为一个大文件）中还会执行combine操作；</li></ol></li><li>combine主要是把形如&lt;aa,1&gt;,&lt;aa,2&gt;这样的key值相同的数据进行计算，计算规则与reduce一致，比如：当前计算是求key对应的值求和，则combine操作后得到&lt;aa,3&gt;这样的结果。</li><li>注意事项：不是每种作业都可以做combine操作的，只有满足以下条件才可以：<ol><li>reduce的输入输出类型都一样，因为combine本质上就是reduce操作；</li><li>计算逻辑上，combine操作后不会影响计算结果，像求和就不会影响；</li></ol></li></ul><h3 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h3><ul><li>merge过程：当map很大时，每次溢写会产生一个spill_file，这样会有多个spill_file，而最终的一个map task输出只有一个文件，因此，最终的结果输出之前会对多个中间过程进行多次溢写文件（spill_file）的合并，此过程就是merge过程。也即是，待Map Task任务的所有数据都处理完后，会对任务产生的所有中间数据文件做一次合并操作，以确保一个Map Task最终只生成一个中间数据文件。</li><li>注意：<ol><li>如果生成的文件太多，可能会执行多次合并，每次最多能合并的文件数默认为10，可以通过属性min.num.spills.for.combine配置；</li><li>多个溢出文件合并时，会进行一次排序，排序算法是多路归并排序；</li><li>是否还需要做combine操作，一是看是否设置了combine，二是看溢出的文件数是否大于等于3；</li><li>最终生成的文件格式与单个溢出文件一致，也是按分区顺序存储，并且输出文件会有一个对应的索引文件，记录每个分区数据的起始位置，长度以及压缩长度，这个索引文件名叫做file.out.index。</li></ol></li></ul><h3 id="内存缓冲区"><a href="#内存缓冲区" class="headerlink" title="内存缓冲区"></a>内存缓冲区</h3><ol><li>在Map Task任务的业务处理方法map()中，最后一步通过OutputCollector.collect(key,value)或context.write(key,value)输出Map Task的中间处理结果，在相关的collect(key,value)方法中，会调用Partitioner.getPartition(K2 key, V2 value, int numPartitions)方法获得输出的key/value对应的分区号(分区号可以认为对应着一个要执行Reduce Task的节点)，然后将&lt;key,value,partition&gt;暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，该缓冲区的默认大小是100MB，可以通过参数io.sort.mb来调整其大小。</li><li>当缓冲区中的数据使用率达到一定阀值后，触发一次Spill操作，将环形缓冲区中的部分数据写到磁盘上，生成一个临时的Linux本地数据的spill文件；然后在缓冲区的使用率再次达到阀值后，再次生成一个spill文件。直到数据处理完毕，在磁盘上会生成很多的临时文件。</li><li>缓存有一个阀值比例配置，当达到整个缓存的这个比例时，会触发spill操作；触发时，map输出还会接着往剩下的空间写入，但是写满的空间会被锁定，数据溢出写入磁盘。当这部分溢出的数据写完后，空出的内存空间可以接着被使用，形成像环一样的被循环使用的效果，所以又叫做环形内存缓冲区；</li><li>MapOutputBuffe内部存数的数据采用了两个索引结构，涉及三个环形内存缓冲区。下来看一下两级索引结构：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/RSrtmYI28b5L.jpg" alt="MapOutputBuffer的两级索引结构"></li></ol><p>写入到缓冲区的数据采取了压缩算法 <a href="http://www.cnblogs.com/edisonchou/p/4298423.html" target="_blank" rel="noopener">http://www.cnblogs.com/edisonchou/p/4298423.html</a><br>这三个环形缓冲区的含义分别如下：</p><ol><li>kvoffsets缓冲区：也叫偏移量索引数组，用于保存key/value信息在位置索引 kvindices 中的偏移量。当 kvoffsets 的使用率超过 io.sort.spill.percent (默认为80%)后，便会触发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li><li>kvindices缓冲区：也叫位置索引数组，用于保存 key/value 在数据缓冲区 kvbuffer 中的起始位置。</li><li>kvbuffer即数据缓冲区：用于保存实际的 key/value 的值。默认情况下该缓冲区最多可以使用 io.sort.mb 的95%，当 kvbuffer 使用率超过 io.sort.spill.percent (默认为80%)后，便会出发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li></ol><p>写入到本地磁盘时，对数据进行排序，实际上是对kvoffsets这个偏移量索引数组进行排序。</p><h2 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h2><p>Reduce过程的经典流程图如下：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/aUlSEGON9iXl.png" alt="reduce-shuffle"></p><h3 id="copy过程"><a href="#copy过程" class="headerlink" title="copy过程"></a>copy过程</h3><ul><li>作用：拉取数据；</li><li>过程：Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为这时map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。</li><li>默认情况下，当整个MapReduce作业的所有已执行完成的Map Task任务数超过Map Task总数的5%后，JobTracker便会开始调度执行Reduce Task任务。然后Reduce Task任务默认启动mapred.reduce.parallel.copies(默认为5）个MapOutputCopier线程到已完成的Map Task任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</li></ul><h4 id="内存缓冲区-1"><a href="#内存缓冲区-1" class="headerlink" title="内存缓冲区"></a>内存缓冲区</h4><ul><li>这个内存缓冲区大小的控制就不像map那样可以通过io.sort.mb来设定了，而是通过另外一个参数来设置：mapred.job.shuffle.input.buffer.percent（default 0.7）， 这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。</li><li>如果该reduce task的最大heap使用量（通常通过mapred.child.java.opts来设置，比如设置为-Xmx1024m）的一定比例用来缓存数据。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。如果reduce的heap由于业务原因调整的比较大，相应的缓存大小也会变大，这也是为什么reduce用来做缓存的参数是一个百分比，而不是一个固定的值了。</li></ul><h3 id="merge过程"><a href="#merge过程" class="headerlink" title="merge过程"></a>merge过程</h3><ul><li>Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比 map 端的更为灵活，它基于 JVM 的heap size设置，因为 Shuffle 阶段 Reducer 不运行，所以应该把绝大部分的内存都给 Shuffle 用。</li><li>这里需要强调的是，merge 有三种形式：1)内存到内存 2)内存到磁盘 3)磁盘到磁盘。默认情况下第一种形式是不启用的。当内存中的数据量到达一定阈值，就启动内存到磁盘的 merge（图中的第一个merge，之所以进行merge是因为reduce端在从多个map端copy数据的时候，并没有进行sort，只是把它们加载到内存，当达到阈值写入磁盘时，需要进行merge） 。这和map端的很类似，这实际上就是溢写的过程，在这个过程中如果你设置有Combiner，它也是会启用的，然后在磁盘中生成了众多的溢写文件，这种merge方式一直在运行，直到没有 map 端的数据时才结束，然后才会启动第三种磁盘到磁盘的 merge （图中的第二个merge）方式生成最终的那个文件。</li><li>在远程copy数据的同时，Reduce Task在后台启动了两个后台线程对内存和磁盘上的数据文件做合并操作，以防止内存使用过多或磁盘生的文件过多。</li></ul><h3 id="reducer的输入文件"><a href="#reducer的输入文件" class="headerlink" title="reducer的输入文件"></a>reducer的输入文件</h3><ul><li>merge的最后会生成一个文件，大多数情况下存在于磁盘中，但是需要将其放入内存中。当reducer 输入文件已定，整个 Shuffle 阶段才算结束。然后就是 Reducer 执行，把结果放到 HDFS 上。</li></ul><p>以上内容来自：<a href="http://matt33.com/2016/03/02/hadoop-shuffle/" target="_blank" rel="noopener">MapReduce之Shuffle过程详述</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MapReduce计算模型一般包括两个重要的阶段：Map是映射，负责数据的过滤分发，将原始数据转化为键值对；Reduce是规约，将具有相同key值的value进行处理后再输出新的键值对作为最终结果。为了让Reduce可以并行处理Map的结果，必须对Map的输出进行一定的排序与分割，然后再交给对应的Reduce，而这个将Map输出进行进一步整理并交给Reduce的过程就是Shuffle。&lt;br&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://jordenbruce.com/categories/hadoop/"/>
    
    
      <category term="MapReduce" scheme="https://jordenbruce.com/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce执行过程详解</title>
    <link href="https://jordenbruce.com/2019/02/17/hadoop-mapreduce/"/>
    <id>https://jordenbruce.com/2019/02/17/hadoop-mapreduce/</id>
    <published>2019-02-17T14:57:51.000Z</published>
    <updated>2019-02-20T16:34:11.925Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分析MapReduce执行过程"><a href="#分析MapReduce执行过程" class="headerlink" title="分析MapReduce执行过程"></a>分析MapReduce执行过程</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。整个流程如图：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190218/ttzqIIqCPiXv.png" alt="MapReduce执行过程"></p><h3 id="Mapper任务的执行过程详解"><a href="#Mapper任务的执行过程详解" class="headerlink" title="Mapper任务的执行过程详解"></a>Mapper任务的执行过程详解</h3><p><strong>每个Mapper任务是一个java进程</strong>，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下几个阶段，如图所示：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190218/cTWHrBBqmvhM.png" alt="Mapper执行过程"></p><p>在上图中，把Mapper任务的运行过程分为六个阶段。</p><ol><li><p>第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值64MB，输入文件有两个，一个是32MB，一个是72MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p></li><li><p>第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一行的起始位置(单位是字节)，“值”是本行的文本内容。</p></li><li><p>第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p></li><li><p>第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer任务运行的数量。默认只有一个Reducer任务。</p></li><li><p>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入第六阶段；如果没有，直接输出到本地的linux文件中。</p></li><li><p>第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。</p></li></ol><h3 id="Reducer任务的执行过程详解"><a href="#Reducer任务的执行过程详解" class="headerlink" title="Reducer任务的执行过程详解"></a>Reducer任务的执行过程详解</h3><p><strong>每个Reducer任务是一个java进程</strong>。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为如下图所示的几个阶段。<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190218/UPvIszDbzRJW.png" alt="Reducer执行过程"></p><ol><li><p>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。</p></li><li><p>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。</p></li><li><p>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS文件中。</p></li></ol><p>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p><h3 id="键值对的编号"><a href="#键值对的编号" class="headerlink" title="键值对的编号"></a>键值对的编号</h3><p>在对Mapper任务、Reducer任务的分析过程中，会看到很多阶段都出现了键值对，读者容易混淆，所以这里对键值对进行编号，方便大家理解键值对的变化情况，如下图所示。<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190218/QXkSATq7bUWL.png" alt="键值对"></p><p>在上图中，对于Mapper任务输入的键值对，定义为key1和value1。在map方法中处理后，输出的键值对，定义为key2和value2。reduce方法接收key2和value2，处理后，输出key3和value3。在下文讨论键值对时，可能把key1和value1简写为&lt;k1,v1&gt;，key2和value2简写为&lt;k2,v2&gt;，key3和value3简写为&lt;k3,v3&gt;。</p><p>以上内容来自：<a href="https://my.oschina.net/itblog/blog/275294" target="_blank" rel="noopener">Hadoop MapReduce执行过程详解（带hadoop例子）</a></p>]]></content>
    
    <summary type="html">
    
      MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。
    
    </summary>
    
      <category term="hadoop" scheme="https://jordenbruce.com/categories/hadoop/"/>
    
    
      <category term="MapReduce" scheme="https://jordenbruce.com/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://jordenbruce.com/2019/02/13/hello-world/"/>
    <id>https://jordenbruce.com/2019/02/13/hello-world/</id>
    <published>2019-02-13T14:23:16.089Z</published>
    <updated>2019-02-17T13:05:34.745Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
      <category term="Hexo" scheme="https://jordenbruce.com/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="https://jordenbruce.com/tags/Hexo/"/>
    
  </entry>
  
</feed>
