<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JordenBruce</title>
  
  <subtitle>A thousand miles begins with a single step.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jordenbruce.com/"/>
  <updated>2019-10-08T15:33:38.689Z</updated>
  <id>https://jordenbruce.com/</id>
  
  <author>
    <name>JordenBruce</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Sqoop命令行的导入与导出</title>
    <link href="https://jordenbruce.com/2019/10/08/sqoop-cli/"/>
    <id>https://jordenbruce.com/2019/10/08/sqoop-cli/</id>
    <published>2019-10-08T13:18:01.000Z</published>
    <updated>2019-10-08T15:33:38.689Z</updated>
    
    <content type="html"><![CDATA[<p>Sqoop是一种被设计为在Hadoop与关系数据库之间传输数据的工具。您可以使用Sqoop将数据从MySQL或Oracle等关系数据库管理系统（RDBMS）导入Hadoop分布式文件系统（HDFS），在Hadoop MapReduce中转换数据，然后将数据导出回RDBMS 。Sqoop使用MapReduce导入和导出数据，还提供了并行操作以及容错能力。<br><a id="more"></a></p><h2 id="0x00-sqoop命令行的语法"><a href="#0x00-sqoop命令行的语法" class="headerlink" title="0x00 sqoop命令行的语法"></a>0x00 sqoop命令行的语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">usage: sqoop COMMAND [ARGS]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br><span class="line"></span><br><span class="line">See &apos;sqoop help COMMAND&apos; for information on a specific command.</span><br></pre></td></tr></table></figure><p>每个命令的具体含义如下：</p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>list-databases</td><td>列出所有数据库名</td></tr><tr><td>list-tables</td><td>列出某个数据库下所有表</td></tr><tr><td>create-hive-table</td><td>生成与关系数据库表结构对应的hive表结构</td></tr><tr><td>eval</td><td>执行一个SQL语句并显示结果</td></tr><tr><td>import</td><td>将关系型数据库中的数据导入到HDFS（包括Hive，HBase）中</td></tr><tr><td>import-all-tables</td><td>导入某个数据库下所有表到HDFS中</td></tr><tr><td>import-mainframe</td><td>从一台主机中导入数据集至HDFS</td></tr><tr><td>export</td><td>从HDFS（包括Hive和HBase）中将数据导出到关系型数据库中</td></tr><tr><td>job</td><td>用来生成一个sqoop任务，生成后不会立即执行，需要手动执行</td></tr><tr><td>merge</td><td>将HDFS中不同目录下面的数据合并在一起并放入指定目录中</td></tr><tr><td>codegen</td><td>将关系型数据库中的表映射为一个Java类，在该类中有各列对应的各个字段</td></tr><tr><td>metastore</td><td>启动元数据服务</td></tr><tr><td>version</td><td>查看版本</td></tr><tr><td>help</td><td>查看帮助</td></tr></tbody></table><p>常用命令有：list-tables, import, export 。</p><h2 id="0x01-sqoop-import"><a href="#0x01-sqoop-import" class="headerlink" title="0x01 sqoop import"></a>0x01 sqoop import</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">-D mapred.job.queue.name=root.data \</span><br><span class="line">--connect jdbc:mysql://mdw01:3306/common \</span><br><span class="line">--username sqoop -P \</span><br><span class="line">--table employee \</span><br><span class="line">--mapreduce-job-name sqoop_import_table_full \</span><br><span class="line">--create-hive-table --hive-import \</span><br><span class="line">--hive-database default --hive-table ods_employee_ds</span><br></pre></td></tr></table></figure><p>上述命令是全量导入，sqoop 还支持增量导入。</p><h2 id="0x02-sqoop-export"><a href="#0x02-sqoop-export" class="headerlink" title="0x02 sqoop export"></a>0x02 sqoop export</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">-D mapred.job.queue.name=root.data \</span><br><span class="line">--connect jdbc:mysql://mdw01:3306/common \</span><br><span class="line">--username sqoop -P \</span><br><span class="line">--table employee \</span><br><span class="line">--mapreduce-job-name sqoop_export_table \</span><br><span class="line">--export-dir /user/hive/warehouse/ods_employee_ds</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html" target="_blank" rel="noopener">Sqoop User Guide</a><br><a href="https://www.jianshu.com/p/b1fa9d853c89" target="_blank" rel="noopener">Sqoop 命令与参数详解</a><br><a href="https://www.cnblogs.com/xiaodf/p/6030102.html" target="_blank" rel="noopener">Sqoop 使用手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sqoop是一种被设计为在Hadoop与关系数据库之间传输数据的工具。您可以使用Sqoop将数据从MySQL或Oracle等关系数据库管理系统（RDBMS）导入Hadoop分布式文件系统（HDFS），在Hadoop MapReduce中转换数据，然后将数据导出回RDBMS 。Sqoop使用MapReduce导入和导出数据，还提供了并行操作以及容错能力。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="sqoop" scheme="https://jordenbruce.com/tags/sqoop/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的字符串函数</title>
    <link href="https://jordenbruce.com/2019/10/05/hql-function-string/"/>
    <id>https://jordenbruce.com/2019/10/05/hql-function-string/</id>
    <published>2019-10-05T04:39:39.000Z</published>
    <updated>2019-10-05T06:25:24.081Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 进行数据清洗时，使用最多的函数应该属字符串函数了。主要有：字符编码，字符拼接，字符查找，字符格式化，字符处理，字符截取，字符构造，字符切分，字符替换，编辑距离等。<br><a id="more"></a></p><h2 id="0x00-字符编码"><a href="#0x00-字符编码" class="headerlink" title="0x00 字符编码"></a>0x00 字符编码</h2><ul><li>ascii - 第一个字符的ASCII码</li><li>base64 - 从二进制转换为基本64字符串</li><li>unbase64 - 从基本64字符串转换为二进制</li><li>encode - 从字符串加密为指定字符集的二进制</li><li>decode - 从二进制解码为指定字符集的字符串</li></ul><h2 id="0x01-字符构造"><a href="#0x01-字符构造" class="headerlink" title="0x01 字符构造"></a>0x01 字符构造</h2><ul><li>repeat - 字符组重复n次的字符串</li><li>space - n个空格的字符串</li></ul><h2 id="0x02-字符拼接"><a href="#0x02-字符拼接" class="headerlink" title="0x02 字符拼接"></a>0x02 字符拼接</h2><ul><li>concat - 按顺序串联字符组所得到的字符串</li><li>concat_ws - 以指定分隔符按顺序串联字符组所得到的字符串</li></ul><h2 id="0x03-字符格式化"><a href="#0x03-字符格式化" class="headerlink" title="0x03 字符格式化"></a>0x03 字符格式化</h2><ul><li>lower - 将所有字符都转换为小写形式的字符串</li><li>upper - 将所有字符都转换为大写形式的字符串</li><li>initcap - 首字母大写而其他字母小写的字符串</li><li>format_number</li><li>printf</li></ul><h2 id="0x04-字符查找"><a href="#0x04-字符查找" class="headerlink" title="0x04 字符查找"></a>0x04 字符查找</h2><ul><li>elt - 取第几个字符串</li><li>field - 第一个匹配上字符串的位置</li><li>find_in_set - 查找以逗号分隔字符串的第一个匹配位置</li><li>in_file - 判断字符串是否在文件中占一行</li><li>instr - 查找字符串中第一个字符组匹配上的位置</li><li>locate - 查找字符串中(某个位置之后)第一个字符组匹配上的位置</li><li>ngrams</li><li>context_ngrams</li></ul><h2 id="0x05-字符截取"><a href="#0x05-字符截取" class="headerlink" title="0x05 字符截取"></a>0x05 字符截取</h2><ul><li>substring (substr) - 截取从某个位置开始指定长度的字符串</li><li>substring_index</li><li>regexp_extract - 使用正则表达式提取指定位置的字符串</li><li>get_json_object - 以指定json路径截取json字符串的元素</li><li>parse_url - 截取URL中指定部分</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select regexp_extract(&apos;foothebar&apos;, &apos;foo(.*?)(bar)&apos;, 2);</span><br><span class="line">select get_json_object(src_json.json, &apos;$.owner&apos;) from src_json;</span><br><span class="line">select parse_url(&apos;http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&apos;, &apos;HOST&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x06-字符切分"><a href="#0x06-字符切分" class="headerlink" title="0x06 字符切分"></a>0x06 字符切分</h2><ul><li>split - 拆分指定字符(正则表达式)前后的字符串</li><li>str_to_map - 按照指定格式拆分字符串为Map结构</li><li>sentences</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select split(&apos;abc,dsf,wes&apos;,&apos;,&apos;);</span><br><span class="line">select str_to_map(&apos;abc:abc,dsf:dsf&apos;,&apos;,&apos;,&apos;:&apos;);</span><br><span class="line">select sentences(&apos;Hello there! How are you?&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x07-字符替换"><a href="#0x07-字符替换" class="headerlink" title="0x07 字符替换"></a>0x07 字符替换</h2><ul><li>replace</li><li>regexp_replace - 将按模式(正则表达式)匹配上的所有旧字符串替换为新字符串并返回</li><li>translate</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select regexp_replace(&quot;foobar&quot;, &quot;oo|ar&quot;, &quot;&quot;);</span><br></pre></td></tr></table></figure><h2 id="0x08-字符处理"><a href="#0x08-字符处理" class="headerlink" title="0x08 字符处理"></a>0x08 字符处理</h2><ul><li>length - 获取字符串的长度</li><li>trim - 去掉字符串左右两边的空格</li><li>ltrim - 去掉字符串左边的空格</li><li>rtrim - 去掉字符串右边的空格</li><li>reverse - 将字符串的所有字符反转</li><li>lpad - 在字符串左侧添加n个指定字符组</li><li>rpad - 在字符串右侧添加n个指定字符组</li><li>soundex</li></ul><h2 id="0x09-编辑距离"><a href="#0x09-编辑距离" class="headerlink" title="0x09 编辑距离"></a>0x09 编辑距离</h2><ul><li>levenshtein - 由一个字符串转成另一个字符串所需的最少编辑操作次数</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-StringFunctions" target="_blank" rel="noopener">String Functions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 进行数据清洗时，使用最多的函数应该属字符串函数了。主要有：字符编码，字符拼接，字符查找，字符格式化，字符处理，字符截取，字符构造，字符切分，字符替换，编辑距离等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的日期函数</title>
    <link href="https://jordenbruce.com/2019/10/05/hql-function-date/"/>
    <id>https://jordenbruce.com/2019/10/05/hql-function-date/</id>
    <published>2019-10-05T01:03:57.000Z</published>
    <updated>2019-10-05T06:28:04.931Z</updated>
    
    <content type="html"><![CDATA[<p>不管是数仓模型的日期维度，还是日常任务的调度时间，都会用到日期函数。主要有：当前时间，时间分段，时间加减，时间转换等。<br><a id="more"></a></p><h2 id="0x00-当前时间"><a href="#0x00-当前时间" class="headerlink" title="0x00 当前时间"></a>0x00 当前时间</h2><ul><li>current_timestamp - 当前时间戳</li><li>current_date - 当前日期</li><li>unix_timestamp</li></ul><h2 id="0x01-时间分段"><a href="#0x01-时间分段" class="headerlink" title="0x01 时间分段"></a>0x01 时间分段</h2><ul><li>year - 年</li><li>quarter - 季度</li><li>month - 月</li><li>day - 天</li><li>hour - 时</li><li>minute - 分</li><li>second - 秒</li><li>extract</li><li>weekofyear - 所属年的第几周</li><li>last_day - 日期所属月份的最后一天</li><li>next_day - 晚于start_date的下一个day_of_week</li><li>trunc - 截断为格式指定单位的日期 (月，年)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select hour(&apos;2019-10-05 09:38:37&apos;);</span><br><span class="line">select weekofyear(&apos;2019-10-05&apos;);</span><br><span class="line">select trunc(&apos;2019-10-05&apos;,&apos;YYYY&apos;);</span><br><span class="line">select trunc(&apos;2019-10-05&apos;,&apos;MM&apos;);</span><br><span class="line">select last_day(&apos;2019-10-05&apos;);</span><br><span class="line">select next_day(&apos;2019-10-05&apos;,&apos;FRIDAY&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x02-时间加减"><a href="#0x02-时间加减" class="headerlink" title="0x02 时间加减"></a>0x02 时间加减</h2><ul><li>date_add - 添加开始日期的天数</li><li>date_sub - 减去开始日期的天数</li><li>add_months - 起始日期之后num_months的日期</li><li>datediff - 从开始日期到结束日期的天数</li><li>months_between - 两个日期之间的月份数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select date_add(&apos;2019-10-05&apos;,3);</span><br><span class="line">select date_sub(&apos;2019-10-05&apos;,4);</span><br><span class="line">select add_months(&apos;2019-10-05&apos;,1,&apos;yyyy-MM-dd&apos;);</span><br><span class="line">select datediff(&apos;2019-12-31&apos;,&apos;2019-10-05&apos;);</span><br><span class="line">select months_between(&apos;2019-12-31&apos;,&apos;2019-10-05&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x03-时间转换"><a href="#0x03-时间转换" class="headerlink" title="0x03 时间转换"></a>0x03 时间转换</h2><ul><li>from_unixtime - 从时间戳转换为标准日期格式</li><li>unix_timestamp - 从标准日期格式转换为时间戳</li><li>to_date - 取时间戳字符串的日期部分</li><li>from_utc_timestamp</li><li>to_utc_timestamp</li><li>date_format</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select from_unixtime(1570240800, &apos;yyyy-MM-dd&apos;);</span><br><span class="line">select unix_timestamp(&apos;2019-10-05&apos;,&apos;yyyy-MM-dd&apos;);</span><br><span class="line">select to_date(&apos;2019-10-05 09:38:37&apos;);</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-DateFunctions" target="_blank" rel="noopener">Date Functions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不管是数仓模型的日期维度，还是日常任务的调度时间，都会用到日期函数。主要有：当前时间，时间分段，时间加减，时间转换等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的数学函数</title>
    <link href="https://jordenbruce.com/2019/10/05/hql-function-math/"/>
    <id>https://jordenbruce.com/2019/10/05/hql-function-math/</id>
    <published>2019-10-04T23:26:30.000Z</published>
    <updated>2019-10-05T06:28:13.497Z</updated>
    
    <content type="html"><![CDATA[<p>使用Hive进行数据分析时，经常会用到数学函数和聚合函数。Hive 支持的内置数学函数有很多，主要有随机函数，取整函数，数学函数，三角函数，进制函数，符号函数，位函数，多列最值函数，分桶函数等。<br><a id="more"></a></p><h2 id="0x00-随机函数"><a href="#0x00-随机函数" class="headerlink" title="0x00 随机函数"></a>0x00 随机函数</h2><ul><li>rand - 随机数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select cast(rand()*1000 as int);</span><br></pre></td></tr></table></figure><h2 id="0x01-取整函数"><a href="#0x01-取整函数" class="headerlink" title="0x01 取整函数"></a>0x01 取整函数</h2><ul><li>round - 保留几位小数</li><li>bround</li><li>floor - 向下取整</li><li>ceil - 向上取整</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select round(rand()*100, 2);</span><br><span class="line">select floor(rand()*100);</span><br><span class="line">select ceil(rand()*100);</span><br></pre></td></tr></table></figure><h2 id="0x02-数学函数"><a href="#0x02-数学函数" class="headerlink" title="0x02 数学函数"></a>0x02 数学函数</h2><ul><li>log - 对数</li><li>log2 - 以2为底的对数</li><li>log10 - 以10为底的对数</li><li>ln - 以e为底的对数</li><li>pow - 指数</li><li>exp - 以e为底的指数</li><li>sqrt - 平方根</li><li>cbrt - 立方根</li><li>e - 自然常数e</li><li>pi - 自然常数π</li><li>factorial - 阶乘</li></ul><h2 id="0x03-三角函数"><a href="#0x03-三角函数" class="headerlink" title="0x03 三角函数"></a>0x03 三角函数</h2><ul><li>sin - 正弦</li><li>cos - 余弦</li><li>tan - 正切</li><li>asin - 反正弦</li><li>acos - 反余弦</li><li>atan - 反正切</li><li>degrees - 从弧度转换为度</li><li>radians - 从度转换为弧度</li></ul><h2 id="0x04-进制函数"><a href="#0x04-进制函数" class="headerlink" title="0x04 进制函数"></a>0x04 进制函数</h2><ul><li>bin - 二进制</li><li>hex - 十六进制</li><li>unhex</li><li>conv - 进制转换</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select conv(cast(rand()*1000 as bigint), 10, 2);</span><br></pre></td></tr></table></figure><h2 id="0x05-符号函数"><a href="#0x05-符号函数" class="headerlink" title="0x05 符号函数"></a>0x05 符号函数</h2><ul><li>positive - 正数</li><li>negative - 负数</li><li>sign - 正负判断</li><li>abs - 绝对值</li></ul><h2 id="0x06-位函数"><a href="#0x06-位函数" class="headerlink" title="0x06 位函数"></a>0x06 位函数</h2><ul><li>shiftleft - 按位左移</li><li>shiftright - 按位右移</li><li>shiftrightunsigned - 按位无符号右移</li></ul><h2 id="0x07-多列最值函数"><a href="#0x07-多列最值函数" class="headerlink" title="0x07 多列最值函数"></a>0x07 多列最值函数</h2><ul><li>greatest - 一行多列取最大值</li><li>least - 一行多列取最小值</li></ul><h2 id="0x07-分桶函数"><a href="#0x07-分桶函数" class="headerlink" title="0x07 分桶函数"></a>0x07 分桶函数</h2><ul><li>pmod - 取模的正数</li><li>width_bucket</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-MathematicalFunctions" target="_blank" rel="noopener">Mathematical Functions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Hive进行数据分析时，经常会用到数学函数和聚合函数。Hive 支持的内置数学函数有很多，主要有随机函数，取整函数，数学函数，三角函数，进制函数，符号函数，位函数，多列最值函数，分桶函数等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的函数概览</title>
    <link href="https://jordenbruce.com/2019/09/30/hql-function/"/>
    <id>https://jordenbruce.com/2019/09/30/hql-function/</id>
    <published>2019-09-30T11:24:49.000Z</published>
    <updated>2019-09-30T11:30:20.371Z</updated>
    
    <content type="html"><![CDATA[<p>Select语句主要有三部分：Select子句 (WITH,SELECT,FROM,WHERE,GROUP BY,HAVING,ORDER BY,LIMIT)，Join语句，Function函数。其中，Function函数 是最精彩也是最丰富的部分，不仅官方内置了大量函数，而且用户还可以自定义函数。本文以内置函数为主。<br><a id="more"></a></p><h2 id="0x00-函数语法"><a href="#0x00-函数语法" class="headerlink" title="0x00 函数语法"></a>0x00 函数语法</h2><p>在命令行环境下，使用以下命令查看当前Hive版本的所有函数及其语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SHOW FUNCTIONS;</span><br><span class="line">DESCRIBE FUNCTION &lt;function_name&gt;;</span><br><span class="line">DESCRIBE FUNCTION EXTENDED &lt;function_name&gt;;</span><br></pre></td></tr></table></figure><h2 id="0x01-运算符-Operators"><a href="#0x01-运算符-Operators" class="headerlink" title="0x01 运算符 (Operators)"></a>0x01 运算符 (Operators)</h2><p>内置的运算符有5大类：</p><ul><li>关系运算符 (Relational Operators)</li><li>算术运算符 (Arithmetic Operators)</li><li>逻辑运算符 (Logical Operators)</li><li>字符串运算符 (String Operators)</li><li>复杂类型 (Complex Types)</li></ul><p>大部分都是经常使用的，比如 =,is not null,+,and 等，以下几个运算符不常用却很重要的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select * from default.managed_user where user_id rlike &apos;^(A|B).*&apos;;</span><br><span class="line"></span><br><span class="line">select from_unixtime(log_timestamp div 1000, &apos;yyyy-MM-dd HH:mm:ss&apos;) as log_time;</span><br><span class="line"></span><br><span class="line">select array(&apos;hadoop&apos;, &apos;hive&apos;, &apos;hql&apos;);</span><br><span class="line">select named_struct(&apos;kid&apos;,3001, &apos;user_id&apos;,&apos;C001&apos;, &apos;user_name&apos;,&apos;sqoop&apos;);</span><br><span class="line">select map(&apos;kid&apos;,3001, &apos;user_id&apos;,&apos;C001&apos;, &apos;user_name&apos;,&apos;sqoop&apos;);</span><br></pre></td></tr></table></figure><p>对于 array,struct,map 三种复杂数据类型，都有对应的运算符取其内的数值。</p><h2 id="0x02-标准函数-UDF-Functions"><a href="#0x02-标准函数-UDF-Functions" class="headerlink" title="0x02 标准函数 (UDF, Functions)"></a>0x02 标准函数 (UDF, Functions)</h2><p>内置的标准函数有8大类：</p><ul><li>数学函数 (Mathematical Functions)</li><li>日期函数 (Date Functions)</li><li>字符串函数 (String Functions)</li><li>字符串掩码函数 (Data Masking Functions)</li><li>条件函数 (Conditional Functions)</li><li>类型转换函数 (Type Conversion Functions)</li><li>集合函数 (Collection Functions)</li><li>其他函数 (Misc. Functions)</li></ul><p>常用三大类函数是：数学函数，字符串函数，日期函数；后面会对每一大类函数单独写一篇手册，因为函数是在太丰富了。另外5类函数常用的有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">select mask_last_n(&apos;13912345678&apos;, 4);</span><br><span class="line"></span><br><span class="line">select if(2019&gt;2014, true, false);</span><br><span class="line">select coalesce(user_name, &apos;unknown&apos;);</span><br><span class="line">select case when user_name = &apos;hql&apos; then &apos;hive&apos; else &apos;other&apos; end;</span><br><span class="line"></span><br><span class="line">select cast(user_id as string);</span><br><span class="line"></span><br><span class="line">select size(map_column_name);</span><br><span class="line"></span><br><span class="line">select md5(&apos;HiveQL&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x03-聚合函数-UDAF-Aggregate-Functions"><a href="#0x03-聚合函数-UDAF-Aggregate-Functions" class="headerlink" title="0x03 聚合函数 (UDAF, Aggregate Functions)"></a>0x03 聚合函数 (UDAF, Aggregate Functions)</h2><p>内置的聚合函数主要有：</p><ul><li>常见聚合函数 (count,sum,avg,max,min)</li><li>统计聚合函数 (方差，标准差，协方差，相关系数，分位数，直方图)</li><li>字符串聚合函数 (collect_list,collect_set)</li></ul><h2 id="0x04-表生成函数-UDTF-Table-Generating-Functions"><a href="#0x04-表生成函数-UDTF-Table-Generating-Functions" class="headerlink" title="0x04 表生成函数 (UDTF, Table-Generating Functions)"></a>0x04 表生成函数 (UDTF, Table-Generating Functions)</h2><p>内置的表生成函数有：</p><ul><li>explode</li><li>posexplode</li><li>inline</li><li>stack</li><li>json_tuple</li><li>parse_url_tuple</li></ul><p>最常用的函数是 explode，使用方式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select explode(array(&apos;A&apos;,&apos;B&apos;,&apos;C&apos;)) as col;</span><br><span class="line">select tf.* from (select 0) t lateral view explode(array(&apos;A&apos;,&apos;B&apos;,&apos;C&apos;)) tf as col;</span><br><span class="line"></span><br><span class="line">select explode(map(&apos;A&apos;,10,&apos;B&apos;,20,&apos;C&apos;,30)) as (key,value);</span><br><span class="line">select tf.* from (select 0) t lateral view explode(map(&apos;A&apos;,10,&apos;B&apos;,20,&apos;C&apos;,30)) tf as key,value;</span><br></pre></td></tr></table></figure><h2 id="0x05-XPath特定函数-XPath-specific-Functions"><a href="#0x05-XPath特定函数-XPath-specific-Functions" class="headerlink" title="0x05 XPath特定函数 (XPath-specific Functions)"></a>0x05 XPath特定函数 (XPath-specific Functions)</h2><p>内置的XPath特定函数有：</p><ul><li>xpath</li><li>xpath_string</li><li>xpath_boolean</li><li>xpath_short</li><li>xpath_int</li><li>xpath_long</li><li>xpath_float</li><li>xpath_double</li><li>xpath_number</li></ul><h2 id="0x06-窗口和分析函数-Windowing-and-Analytics-Functions"><a href="#0x06-窗口和分析函数-Windowing-and-Analytics-Functions" class="headerlink" title="0x06 窗口和分析函数 (Windowing and Analytics Functions)"></a>0x06 窗口和分析函数 (Windowing and Analytics Functions)</h2><p>内置的窗口和分析函数主要有：</p><ul><li>窗口函数 (Windowing functions)</li><li>OVER子句 (The OVER clause)</li><li>分析函数 (Analytics functions)</li></ul><p>以下是两个常用的使用方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select a, sum(b) over (partition by c) from t;</span><br><span class="line"></span><br><span class="line">select a, row_number() over (partition by b order by d desc) from t;</span><br></pre></td></tr></table></figure><h2 id="0x07-参考"><a href="#0x07-参考" class="headerlink" title="0x07 参考"></a>0x07 参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">LanguageManual UDF</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+XPathUDF" target="_blank" rel="noopener">LanguageManual XPathUDF</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">LanguageManual WindowingAndAnalytics</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Select语句主要有三部分：Select子句 (WITH,SELECT,FROM,WHERE,GROUP BY,HAVING,ORDER BY,LIMIT)，Join语句，Function函数。其中，Function函数 是最精彩也是最丰富的部分，不仅官方内置了大量函数，而且用户还可以自定义函数。本文以内置函数为主。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的Select语句</title>
    <link href="https://jordenbruce.com/2019/09/26/hql-select/"/>
    <id>https://jordenbruce.com/2019/09/26/hql-select/</id>
    <published>2019-09-26T13:30:42.000Z</published>
    <updated>2019-09-26T15:22:43.596Z</updated>
    
    <content type="html"><![CDATA[<p>前面文章已经解决了数据存储的问题，这篇将介绍查询数据的Select语句。当表中的数据越来越多时，如何查询想要的数据，或者进行数据分析呢？<br><a id="more"></a></p><h2 id="0x00-语法"><a href="#0x00-语法" class="headerlink" title="0x00 语法"></a>0x00 语法</h2><p>官方的语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[WITH CommonTableExpression (, CommonTableExpression)*]</span><br><span class="line">SELECT [ALL | DISTINCT] select_expr, select_expr, ...</span><br><span class="line">  FROM table_reference</span><br><span class="line">  [WHERE where_condition]</span><br><span class="line">  [GROUP BY col_list]</span><br><span class="line">    [HAVING having_condition]</span><br><span class="line">  [ORDER BY col_list]</span><br><span class="line">  [CLUSTER BY col_list</span><br><span class="line">    | [DISTRIBUTE BY col_list] [SORT BY col_list]</span><br><span class="line">  ]</span><br><span class="line"> [LIMIT [offset,] rows]</span><br></pre></td></tr></table></figure><p>主要包括：WITH子句 (公共临时表)，SELECT子句 (查询结果的输出列)，FROM子句 (查询的数据源)，WHERE子句 (过滤条件)，GROUP BY子句 (分组列表)，HAVING子句 (分组的过滤条件)，ORDER BY子句 (排序列表)，LIMIT子句 (限制输出行数) 等。需要说明的是：</p><ul><li>select语句可以是union查询的一部分或者是另一个查询的子查询。</li><li>table_reference指示查询的输入。它可以是普通的表，视图，join构造或者是子查询。</li></ul><h2 id="0x01-单表查询"><a href="#0x01-单表查询" class="headerlink" title="0x01 单表查询"></a>0x01 单表查询</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select reg_date, count(kid) as cnt</span><br><span class="line">from default.ods_user</span><br><span class="line">where reg_date &gt;= &apos;2019-09-20&apos;</span><br><span class="line">group by reg_date</span><br><span class="line">  having count(kid) &gt; 1</span><br><span class="line">order by reg_date desc</span><br><span class="line">limit 3 </span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>请问，这条SQL语句表达的是什么业务场景？</p><h2 id="0x02-多表连接查询"><a href="#0x02-多表连接查询" class="headerlink" title="0x02 多表连接查询"></a>0x02 多表连接查询</h2><p>HiveQL支持的JOIN方式有：</p><ul><li>内连接 ([INNER] JOIN)</li><li>外连接 ({LEFT|RIGHT|FULL} [OUTER] JOIN)</li><li>左半连接 (LEFT SEMI JOIN)</li><li>笛卡尔积关联 (CROSS JOIN)</li></ul><p>每一种连接的使用说明，请参考 <a href="https://www.cnblogs.com/liupengpengg/p/7908274.html" target="_blank" rel="noopener">Hive中Join的类型和用法</a></p><h2 id="0x03-函数"><a href="#0x03-函数" class="headerlink" title="0x03 函数"></a>0x03 函数</h2><p>HiveQL内置函数主要有6大类：</p><ul><li>运算符 (Operators)</li><li>标准函数 (UDF, Functions)</li><li>聚合函数 (UDAF, Aggregate Functions)</li><li>表生成函数 (UDTF, Table-Generating Functions)</li><li>XPath特定函数 (XPath-specific Functions)</li><li>窗口和分析函数 (Windowing and Analytics Functions)</li></ul><p>有关常用函数的使用说明，下一篇再继续写。</p><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select" target="_blank" rel="noopener">LanguageManual Select</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins" target="_blank" rel="noopener">LanguageManual Joins</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">LanguageManual UDF</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面文章已经解决了数据存储的问题，这篇将介绍查询数据的Select语句。当表中的数据越来越多时，如何查询想要的数据，或者进行数据分析呢？&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的导入与导出</title>
    <link href="https://jordenbruce.com/2019/09/24/hql-dml/"/>
    <id>https://jordenbruce.com/2019/09/24/hql-dml/</id>
    <published>2019-09-24T13:21:29.000Z</published>
    <updated>2019-09-24T14:44:08.526Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇说完了Table常用操作，创建了几种表，可是表中还没有数据，这就需要数据导入；数据按照业务逻辑经过Hive各种处理之后，还需要数据导出，方便进一步的分析处理。<br><a id="more"></a></p><h2 id="0x00-综述"><a href="#0x00-综述" class="headerlink" title="0x00 综述"></a>0x00 综述</h2><p>常见的数据导入方式有：</p><ul><li>本地文件导入到Hive表</li><li>HDFS文件导入到Hive表</li><li>Hive表导入到一个Hive表</li><li>创建表的过程中从其他表导入</li><li>Hive表导入到多个Hive表</li></ul><p>常见的数据导出方式有：</p><ul><li>Hive表导出到本地文件系统</li><li>Hive表导出到HDFS</li><li>命令行导出到本地文件系统</li></ul><h2 id="0x01-导入"><a href="#0x01-导入" class="headerlink" title="0x01 导入"></a>0x01 导入</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/home/hadoop/user1.txt&apos; into table default.ods_user;</span><br><span class="line">load data inpath &apos;/ods/user2.txt&apos; into table default.ods_user;</span><br><span class="line"></span><br><span class="line">insert overwrite table default.managed_user </span><br><span class="line">select kid,user_id,user_name,reg_date from default.ods_user;</span><br><span class="line"></span><br><span class="line">insert overwrite table default.partitioned_user </span><br><span class="line">partition( dt = &apos;2019-09-20&apos;) </span><br><span class="line">select kid,user_id,user_name,reg_date </span><br><span class="line">from default.managed_user</span><br><span class="line">where reg_date = &apos;2019-09-20&apos;;</span><br><span class="line"></span><br><span class="line">create table default.user_20190921 as </span><br><span class="line">select kid,user_id,user_name,reg_date </span><br><span class="line">from default.ods_user </span><br><span class="line">where reg_date = &apos;2019-09-21&apos;;</span><br><span class="line"></span><br><span class="line">from default.managed_user </span><br><span class="line">insert overwrite table default.partitioned_user partition( dt = &apos;2019-09-21&apos;) </span><br><span class="line">select kid,user_id,user_name,reg_date where reg_date = &apos;2019-09-21&apos; </span><br><span class="line">insert overwrite table default.partitioned_user partition( dt = &apos;2019-09-22&apos;) </span><br><span class="line">select kid,user_id,user_name,reg_date where reg_date = &apos;2019-09-22&apos; ;</span><br></pre></td></tr></table></figure><h2 id="0x02-导出"><a href="#0x02-导出" class="headerlink" title="0x02 导出"></a>0x02 导出</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory &apos;/home/hadoop/user&apos; </span><br><span class="line">row format delimited fields terminated by &apos;|&apos; </span><br><span class="line">stored as textfile </span><br><span class="line">select kid,user_id,user_name,reg_date from default.managed_user;</span><br><span class="line"></span><br><span class="line">insert overwrite directory &apos;/external/user&apos; </span><br><span class="line">stored as parquet </span><br><span class="line">select kid,user_id,user_name,reg_date from default.managed_user;</span><br><span class="line"></span><br><span class="line">hive -e &quot;select * from default.managed_user&quot; &gt;&gt; /home/hadoop/user/source.txt</span><br></pre></td></tr></table></figure><h2 id="0x03-参考"><a href="#0x03-参考" class="headerlink" title="0x03 参考"></a>0x03 参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML" target="_blank" rel="noopener">LanguageManual DML</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇说完了Table常用操作，创建了几种表，可是表中还没有数据，这就需要数据导入；数据按照业务逻辑经过Hive各种处理之后，还需要数据导出，方便进一步的分析处理。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的Table常用操作</title>
    <link href="https://jordenbruce.com/2019/09/23/hql-table/"/>
    <id>https://jordenbruce.com/2019/09/23/hql-table/</id>
    <published>2019-09-23T00:28:31.000Z</published>
    <updated>2019-09-24T13:21:42.720Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive数据仓库软件有助于读取，写入和管理驻留在分布式存储中并使用SQL语法查询的大型数据集。而Table是Hive组织数据存储的主要数据单元，是一种结构化存储，用二维表结构来表示。<br><a id="more"></a></p><h2 id="0x00-create-table"><a href="#0x00-create-table" class="headerlink" title="0x00 create table"></a>0x00 create table</h2><p>经常使用的表有：内部表(managed table)，外部表(external table)，分区表(partitioned table)，临时表(temporary table)等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">--1. managed table</span><br><span class="line">create table if not exists default.managed_user (</span><br><span class="line">     kid       bigint  comment &apos;主键&apos;</span><br><span class="line">    ,user_id   string  comment &apos;用户编号&apos;</span><br><span class="line">    ,user_name string  comment &apos;用户名称&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;内部用户表&apos;</span><br><span class="line">stored as parquet</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">--2. external table</span><br><span class="line">create external table if not exists default.external_user (</span><br><span class="line">     kid       bigint  comment &apos;主键&apos;</span><br><span class="line">    ,user_id   string  comment &apos;用户编号&apos;</span><br><span class="line">    ,user_name string  comment &apos;用户名称&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;外部用户表&apos;</span><br><span class="line">stored as textfile</span><br><span class="line">location &apos;/external/user&apos;</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">--3. partitioned table</span><br><span class="line">create table if not exists default.partitioned_user (</span><br><span class="line">     kid       bigint  comment &apos;主键&apos;</span><br><span class="line">    ,user_id   string  comment &apos;用户编号&apos;</span><br><span class="line">    ,user_name string  comment &apos;用户名称&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;分区用户表&apos;</span><br><span class="line">partitioned by (</span><br><span class="line">    dt string comment &apos;日期分区&apos;</span><br><span class="line">)</span><br><span class="line">stored as parquet</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">--4. temporary table</span><br><span class="line">create temporary table if not exists default.temporary_user </span><br><span class="line">like default.managed_user</span><br><span class="line">;</span><br><span class="line">create temporary table if not exists default.temporary_user as </span><br><span class="line">select kid, user_id from default.partitioned_user where dt = &apos;2019-09-22&apos;</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x01-alter-table"><a href="#0x01-alter-table" class="headerlink" title="0x01 alter table"></a>0x01 alter table</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">alter table default.external_user rename to default.external_user_ds;</span><br><span class="line"></span><br><span class="line">alter table default.managed_user set tblproperties(&quot;skip.header.line.count&quot;=&quot;1&quot;);</span><br><span class="line"></span><br><span class="line">alter table default.managed_user add columns (reg_date string comment &apos;注册日期&apos;);</span><br><span class="line">alter table default.partitioned_user add columns (reg_date string comment &apos;注册日期&apos;) cascade;</span><br></pre></td></tr></table></figure><h2 id="0x02-describe-table-amp-show-table"><a href="#0x02-describe-table-amp-show-table" class="headerlink" title="0x02 describe table &amp; show table"></a>0x02 describe table &amp; show table</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">desc default.managed_user;</span><br><span class="line">desc formatted default.managed_user;</span><br><span class="line"></span><br><span class="line">show tables;</span><br><span class="line">show create table default.managed_user;</span><br><span class="line">show partitions default.partitioned_user;</span><br></pre></td></tr></table></figure><h2 id="0x03-truncate-table-amp-drop-table"><a href="#0x03-truncate-table-amp-drop-table" class="headerlink" title="0x03 truncate table &amp; drop table"></a>0x03 truncate table &amp; drop table</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">truncate table default.managed_user;</span><br><span class="line"></span><br><span class="line">drop table default.partitioned_user;</span><br><span class="line">alter table default.partitioned_user drop partition(dt = &apos;2019-09-22&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">LanguageManual DDL</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Hive数据仓库软件有助于读取，写入和管理驻留在分布式存储中并使用SQL语法查询的大型数据集。而Table是Hive组织数据存储的主要数据单元，是一种结构化存储，用二维表结构来表示。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>yarn命令行的常用操作</title>
    <link href="https://jordenbruce.com/2019/09/22/yarn-cli/"/>
    <id>https://jordenbruce.com/2019/09/22/yarn-cli/</id>
    <published>2019-09-22T09:17:06.000Z</published>
    <updated>2019-09-22T10:39:56.718Z</updated>
    
    <content type="html"><![CDATA[<p>hadoop-0.23中引入的新架构将JobTracker的两个主要功能划分为：资源管理和作业生命周期管理。新的ResourceManager管理应用程序的全局计算资源分配，每个应用程序的ApplicationMaster管理应用程序的调度和协调。应用程序可以是传统yarnuce作业中的单个作业，也可以是此类作业的DAG。在该计算机上管理用户进程的ResourceManager和每台计算机的NodeManager守护程序构成了计算结构。实际上，每个应用程序的ApplicationMaster是特定于框架的库，其任务是与来自ResourceManager的资源进行协商，并与NodeManager一起执行和监视任务。<br><a id="more"></a></p><h2 id="0x00-yarn命令行的语法"><a href="#0x00-yarn命令行的语法" class="headerlink" title="0x00 yarn命令行的语法"></a>0x00 yarn命令行的语法</h2><p>YARN命令由bin/yarn脚本调用。在不带任何参数的情况下运行yarn脚本会打印所有命令的描述。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Usage: yarn [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line">  CLASSNAME                             run the class named CLASSNAME</span><br><span class="line"> or</span><br><span class="line">  where COMMAND is one of:</span><br><span class="line">  resourcemanager -format-state-store   deletes the RMStateStore</span><br><span class="line">  resourcemanager                       run the ResourceManager</span><br><span class="line">  nodemanager                           run a nodemanager on each slave</span><br><span class="line">  timelineserver                        run the timeline server</span><br><span class="line">  rmadmin                               admin tools</span><br><span class="line">  sharedcachemanager                    run the SharedCacheManager daemon</span><br><span class="line">  scmadmin                              SharedCacheManager admin tools</span><br><span class="line">  version                               print the version</span><br><span class="line">  jar &lt;jar&gt;                             run a jar file</span><br><span class="line">  application                           prints application(s)</span><br><span class="line">                                        report/kill application</span><br><span class="line">  applicationattempt                    prints applicationattempt(s)</span><br><span class="line">                                        report</span><br><span class="line">  container                             prints container(s) report</span><br><span class="line">  node                                  prints node report(s)</span><br><span class="line">  queue                                 prints queue information</span><br><span class="line">  logs                                  dump container logs</span><br><span class="line">  classpath                             prints the class path needed to</span><br><span class="line">                                        get the Hadoop jar and the</span><br><span class="line">                                        required libraries</span><br><span class="line">  cluster                               prints cluster information</span><br><span class="line">  daemonlog                             get/set the log level for each</span><br><span class="line">                                        daemon</span><br><span class="line"></span><br><span class="line">Most commands print help when invoked w/o parameters.</span><br></pre></td></tr></table></figure><h2 id="0x01-yarn-queue"><a href="#0x01-yarn-queue" class="headerlink" title="0x01 yarn queue"></a>0x01 yarn queue</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn queue -status default</span><br></pre></td></tr></table></figure><h2 id="0x02-yarn-application"><a href="#0x02-yarn-application" class="headerlink" title="0x02 yarn application"></a>0x02 yarn application</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yarn application -list</span><br><span class="line">yarn application -status application_id</span><br><span class="line">yarn application -kill application_id</span><br></pre></td></tr></table></figure><h2 id="0x03-yarn-jar"><a href="#0x03-yarn-jar" class="headerlink" title="0x03 yarn jar"></a>0x03 yarn jar</h2><p>运行一个jar文件。用户可以将其YARN代码捆绑在jar文件中，并使用此命令执行它。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar wordcount /input/ /output/</span><br></pre></td></tr></table></figure><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-site/YarnCommands.html" target="_blank" rel="noopener">YARN Commands Guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;hadoop-0.23中引入的新架构将JobTracker的两个主要功能划分为：资源管理和作业生命周期管理。新的ResourceManager管理应用程序的全局计算资源分配，每个应用程序的ApplicationMaster管理应用程序的调度和协调。应用程序可以是传统yarnuce作业中的单个作业，也可以是此类作业的DAG。在该计算机上管理用户进程的ResourceManager和每台计算机的NodeManager守护程序构成了计算结构。实际上，每个应用程序的ApplicationMaster是特定于框架的库，其任务是与来自ResourceManager的资源进行协商，并与NodeManager一起执行和监视任务。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="yarn" scheme="https://jordenbruce.com/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>hdfs命令行的常用操作</title>
    <link href="https://jordenbruce.com/2019/09/22/hdfs-cli/"/>
    <id>https://jordenbruce.com/2019/09/22/hdfs-cli/</id>
    <published>2019-09-22T07:26:53.000Z</published>
    <updated>2019-09-22T08:57:26.965Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS是Hadoop应用程序使用的主要分布式存储。HDFS群集主要由管理文件系统元数据的NameNode和存储实际数据的DataNode组成。客户端与NameNode联系以获取文件元数据或文件修改，并直接与DataNode执行实际的文件I/O。<br><a id="more"></a></p><h2 id="0x00-hdfs命令行的语法"><a href="#0x00-hdfs命令行的语法" class="headerlink" title="0x00 hdfs命令行的语法"></a>0x00 hdfs命令行的语法</h2><p>所有HDFS命令均由bin/hdfs脚本调用。运行不带任何参数的hdfs脚本会打印所有命令的描述。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND</span><br><span class="line">       where COMMAND is one of:</span><br><span class="line">  dfs                  run a filesystem command on the file systems supported in Hadoop.</span><br><span class="line">  classpath            prints the classpath</span><br><span class="line">  namenode -format     format the DFS filesystem</span><br><span class="line">  secondarynamenode    run the DFS secondary namenode</span><br><span class="line">  namenode             run the DFS namenode</span><br><span class="line">  journalnode          run the DFS journalnode</span><br><span class="line">  zkfc                 run the ZK Failover Controller daemon</span><br><span class="line">  datanode             run a DFS datanode</span><br><span class="line">  dfsadmin             run a DFS admin client</span><br><span class="line">  haadmin              run a DFS HA admin client</span><br><span class="line">  fsck                 run a DFS filesystem checking utility</span><br><span class="line">  balancer             run a cluster balancing utility</span><br><span class="line">  jmxget               get JMX exported values from NameNode or DataNode.</span><br><span class="line">  mover                run a utility to move block replicas across</span><br><span class="line">                       storage types</span><br><span class="line">  oiv                  apply the offline fsimage viewer to an fsimage</span><br><span class="line">  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage</span><br><span class="line">  oev                  apply the offline edits viewer to an edits file</span><br><span class="line">  fetchdt              fetch a delegation token from the NameNode</span><br><span class="line">  getconf              get config values from configuration</span><br><span class="line">  groups               get the groups which users belong to</span><br><span class="line">  snapshotDiff         diff two snapshots of a directory or diff the</span><br><span class="line">                       current directory contents with a snapshot</span><br><span class="line">  lsSnapshottableDir   list all snapshottable dirs owned by the current user</span><br><span class="line">                                                Use -help to see options</span><br><span class="line">  portmap              run a portmap service</span><br><span class="line">  nfs3                 run an NFS version 3 gateway</span><br><span class="line">  cacheadmin           configure the HDFS cache</span><br><span class="line">  crypto               configure HDFS encryption zones</span><br><span class="line">  storagepolicies      list/get/set block storage policies</span><br><span class="line">  version              print the version</span><br><span class="line"></span><br><span class="line">Most commands print help when invoked w/o parameters.</span><br></pre></td></tr></table></figure><h2 id="0x01-hdfs-dfs"><a href="#0x01-hdfs-dfs" class="headerlink" title="0x01 hdfs dfs"></a>0x01 hdfs dfs</h2><p>在Hadoop支持的文件系统上运行文件系统命令。目前Hadoop兼容文件系统有：Amazon S3，Azure Blob Storage，OpenStack Swift 。常用操作命令与 hadoop fs 类似，也建议使用 hadoop fs 命令。</p><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-common/FileSystemShell.html" target="_blank" rel="noopener">File System Shell Guide</a></p><h2 id="0x02-hdfs-balancer"><a href="#0x02-hdfs-balancer" class="headerlink" title="0x02 hdfs balancer"></a>0x02 hdfs balancer</h2><p>HDFS数据不一定总是在整个DataNode上均匀地放置。一个常见的原因是向现有群集中添加了新的DataNode。HDFS为管理员提供了一个工具balancer，可以分析整个DataNode上的块放置和重新平衡数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs balancer -policy datanode -threshold 20 -include -f /tmp/hdfs-blancer.txt</span><br></pre></td></tr></table></figure><h2 id="0x03-hdfs-dfsadmin"><a href="#0x03-hdfs-dfsadmin" class="headerlink" title="0x03 hdfs dfsadmin"></a>0x03 hdfs dfsadmin</h2><p>dfsadmin 命令用于管理HDFS集群，这些命令常用于管理员。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -report -live</span><br><span class="line">hdfs dfsadmin -printTopology</span><br><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">hdfs dfsadmin -safemode get</span><br><span class="line">hdfs dfsadmin -setBalancerBandwidth 6250000</span><br></pre></td></tr></table></figure><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfsadmin" target="_blank" rel="noopener">HDFS Commands Guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HDFS是Hadoop应用程序使用的主要分布式存储。HDFS群集主要由管理文件系统元数据的NameNode和存储实际数据的DataNode组成。客户端与NameNode联系以获取文件元数据或文件修改，并直接与DataNode执行实际的文件I/O。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hdfs" scheme="https://jordenbruce.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>hadoop命令行的常用操作</title>
    <link href="https://jordenbruce.com/2019/09/20/hadoop-cli/"/>
    <id>https://jordenbruce.com/2019/09/20/hadoop-cli/</id>
    <published>2019-09-20T14:49:53.000Z</published>
    <updated>2019-09-22T08:57:48.186Z</updated>
    
    <content type="html"><![CDATA[<p>编译并安装Hadoop分布式运行环境之后，第一个要用到的命令行就是hadoop。需要注意的是：每个发行版的命令行语法有些不一样，可以通过<code>hadoop -help</code>进行查看。<br><a id="more"></a></p><h2 id="0x00-hadoop命令行的语法"><a href="#0x00-hadoop命令行的语法" class="headerlink" title="0x00 hadoop命令行的语法"></a>0x00 hadoop命令行的语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line">  CLASSNAME            run the class named CLASSNAME</span><br><span class="line"> or</span><br><span class="line">  where COMMAND is one of:</span><br><span class="line">  fs                   run a generic filesystem user client</span><br><span class="line">  version              print the version</span><br><span class="line">  jar &lt;jar&gt;            run a jar file</span><br><span class="line">                       note: please use &quot;yarn jar&quot; to launch</span><br><span class="line">                             YARN applications, not this command.</span><br><span class="line">  checknative [-a|-h]  check native hadoop and compression libraries availability</span><br><span class="line">  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively</span><br><span class="line">  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span><br><span class="line">  classpath            prints the class path needed to get the</span><br><span class="line">  credential           interact with credential providers</span><br><span class="line">                       Hadoop jar and the required libraries</span><br><span class="line">  daemonlog            get/set the log level for each daemon</span><br><span class="line">  trace                view and modify Hadoop tracing settings</span><br><span class="line"></span><br><span class="line">Most commands print help when invoked w/o parameters.</span><br></pre></td></tr></table></figure><p>每个命令的具体含义如下：</p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>version</td><td>打印hadoop版本</td></tr><tr><td>checknative</td><td>检测native库和压缩库的可用性</td></tr><tr><td>fs</td><td>hdfs命令行的客户端</td></tr><tr><td>jar</td><td>运行jar包里的mapreduce程序(推荐使用yarn jar)</td></tr><tr><td>distcp</td><td>用于大规模集群内部和集群之间拷贝的工具</td></tr><tr><td>archive</td><td>将小文件进行hadoop存档</td></tr><tr><td>classpath</td><td>打印类路径</td></tr><tr><td>credential</td><td>管理凭证供应商</td></tr><tr><td>daemonlog</td><td>获取/设置每个守护程序的日志级别</td></tr><tr><td>trace</td><td>查看和修改Hadoop跟踪设置</td></tr></tbody></table><p>其中，最常用的有 fs jar archive distcp 。</p><h2 id="0x01-hadoop-fs"><a href="#0x01-hadoop-fs" class="headerlink" title="0x01 hadoop fs"></a>0x01 hadoop fs</h2><p>调用文件系统(FS)Shell命令应使用 bin/hadoop fs <args>的形式。所有的的FS shell命令使用URI路径作为参数。URI格式是scheme://authority/path。对HDFS文件系统，scheme是hdfs，对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。一个HDFS文件或目录比如/parent/child可以表示成hdfs://namenode:namenodeport/parent/child，或者更简单的/parent/child（假设你配置文件中的默认值是namenode:namenodeport）。大多数FS Shell命令的行为和对应的Unix Shell命令类似。</args></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /</span><br><span class="line">hadoop fs -mkdir /external</span><br><span class="line">hadoop fs -touchz /external/readme</span><br><span class="line">hadoop fs -put $&#123;HADOOP_HOME&#125;README.txt /external</span><br><span class="line">hadoop fs -du -h /external</span><br><span class="line">hadoop fs -find /external readme</span><br><span class="line">hadoop fs -tail /external/README.txt</span><br><span class="line">hadoop fs -rm /external/readme</span><br></pre></td></tr></table></figure><p>还有很多命令，这里就不一一演示了。</p><h2 id="0x02-hadoop-archive"><a href="#0x02-hadoop-archive" class="headerlink" title="0x02 hadoop archive"></a>0x02 hadoop archive</h2><p>Hadoop archives是特殊的档案格式。一个Hadoop archive对应一个文件系统目录。Hadoop archive的扩展名是*.har。Hadoop archive包含元数据（形式是_index和_masterindx）和数据（part-*）文件。_index文件包含了档案中的文件的文件名和位置信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop archive -archiveName readme.har -p / external /archive</span><br><span class="line">hadoop fs -ls -R har:///archive/readme.har</span><br><span class="line">hadoop fs -cat har:///archive/readme.har/external/README.txt</span><br></pre></td></tr></table></figure><h2 id="0x03-hadoop-distcp"><a href="#0x03-hadoop-distcp" class="headerlink" title="0x03 hadoop distcp"></a>0x03 hadoop distcp</h2><p>DistCp（分布式拷贝）是用于大规模集群内部和集群之间拷贝的工具。它使用Map/Reduce实现文件分发，错误处理和恢复，以及报告生成。它把文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。由于使用了Map/Reduce方法，这个工具在语义和执行上都会有特殊的地方。</p><p>DistCp最常用在集群之间的拷贝：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop distcp hdfs://nn1:8020/foo/bar hdfs://nn2:8020/bar/foo</span><br></pre></td></tr></table></figure><p>这条命令会把nn1集群的/foo/bar目录下的所有文件或目录名展开并存储到一个临时文件中，这些文件内容的拷贝工作被分配给多个map任务，然后每个TaskTracker分别执行从nn1到nn2的拷贝操作。注意DistCp使用绝对路径进行操作。</p><h2 id="0x04-参考"><a href="#0x04-参考" class="headerlink" title="0x04 参考"></a>0x04 参考</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-common/CommandsManual.html" target="_blank" rel="noopener">Hadoop Commands Guide</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;编译并安装Hadoop分布式运行环境之后，第一个要用到的命令行就是hadoop。需要注意的是：每个发行版的命令行语法有些不一样，可以通过&lt;code&gt;hadoop -help&lt;/code&gt;进行查看。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>手动搭建Sqoop开发环境</title>
    <link href="https://jordenbruce.com/2019/09/15/sqoop-install/"/>
    <id>https://jordenbruce.com/2019/09/15/sqoop-install/</id>
    <published>2019-09-15T08:16:47.000Z</published>
    <updated>2019-09-17T14:16:05.939Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Sqoop 是一种工具，用于在Apache Hadoop和结构化数据存储（如关系数据库）之间高效传输批量数据。<br><a id="more"></a></p><h2 id="0x00-解压文件"><a href="#0x00-解压文件" class="headerlink" title="0x00 解压文件"></a>0x00 解压文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -xf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br><span class="line">mv sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop-1.4.7</span><br><span class="line">ln -s /data/sqoop-1.4.7/ /data/sqoop</span><br></pre></td></tr></table></figure><h2 id="0x01-添加环境变量"><a href="#0x01-添加环境变量" class="headerlink" title="0x01 添加环境变量"></a>0x01 添加环境变量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line">#export SQOOP_HOME=/data/sqoop/</span><br><span class="line">#export PATH=$&#123;SQOOP_HOME&#125;/bin:$PATH</span><br><span class="line">source ~/.bashrc</span><br><span class="line">sqoop version</span><br></pre></td></tr></table></figure><h2 id="0x02-修改sqoop-env-sh配置文件"><a href="#0x02-修改sqoop-env-sh配置文件" class="headerlink" title="0x02 修改sqoop-env.sh配置文件"></a>0x02 修改sqoop-env.sh配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cd conf/</span><br><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">vim sqoop-env.sh</span><br><span class="line"></span><br><span class="line">#Set path to where bin/hadoop is available</span><br><span class="line">export HADOOP_COMMON_HOME=/data/hadoop/</span><br><span class="line"></span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/data/hadoop/</span><br><span class="line"></span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">#export HBASE_HOME=</span><br><span class="line"></span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">export HIVE_HOME=/data/hive/</span><br><span class="line"></span><br><span class="line">#Set the path for where zookeper config dir is</span><br><span class="line">#export ZOOCFGDIR=</span><br></pre></td></tr></table></figure><h2 id="0x03-修改configure-sqoop文件"><a href="#0x03-修改configure-sqoop文件" class="headerlink" title="0x03 修改configure-sqoop文件"></a>0x03 修改configure-sqoop文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd bin/</span><br><span class="line">vim configure-sqoop</span><br></pre></td></tr></table></figure><p>注释掉以下代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#if [ -z &quot;$&#123;HBASE_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/hbase&quot; ]; then</span><br><span class="line">#    HBASE_HOME=/usr/lib/hbase</span><br><span class="line">#  else</span><br><span class="line">#    HBASE_HOME=$&#123;SQOOP_HOME&#125;/../hbase</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br><span class="line">#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then</span><br><span class="line">#    HCAT_HOME=/usr/lib/hive-hcatalog</span><br><span class="line">#  elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then</span><br><span class="line">#    HCAT_HOME=/usr/lib/hcatalog</span><br><span class="line">#  else</span><br><span class="line">#    HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog</span><br><span class="line">#    if [ ! -d $&#123;HCAT_HOME&#125; ]; then</span><br><span class="line">#       HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog</span><br><span class="line">#    fi</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br><span class="line">#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/accumulo&quot; ]; then</span><br><span class="line">#    ACCUMULO_HOME=/usr/lib/accumulo</span><br><span class="line">#  else</span><br><span class="line">#    ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br><span class="line">#if [ -z &quot;$&#123;ZOOKEEPER_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/zookeeper&quot; ]; then</span><br><span class="line">#    ZOOKEEPER_HOME=/usr/lib/zookeeper</span><br><span class="line">#  else</span><br><span class="line">#    ZOOKEEPER_HOME=$&#123;SQOOP_HOME&#125;/../zookeeper</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br></pre></td></tr></table></figure><h2 id="0x04-拷贝数据库连接jar包"><a href="#0x04-拷贝数据库连接jar包" class="headerlink" title="0x04 拷贝数据库连接jar包"></a>0x04 拷贝数据库连接jar包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /data/hive/lib/mysql-connector-java-5.1.44-bin.jar ./lib/</span><br></pre></td></tr></table></figure><h2 id="0x05-启动测试"><a href="#0x05-启动测试" class="headerlink" title="0x05 启动测试"></a>0x05 启动测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root --password</span><br><span class="line">sqoop list-tables --connect jdbc:mysql://localhost:3306/hive --username root --password</span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://mdw01:3306/hive \</span><br><span class="line">--username hive --password mysql \</span><br><span class="line">--table TBLS \</span><br><span class="line">-m 1 \</span><br><span class="line">--hive-import \</span><br><span class="line">--create-hive-table \</span><br><span class="line">--hive-table hive_tables</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Sqoop 是一种工具，用于在Apache Hadoop和结构化数据存储（如关系数据库）之间高效传输批量数据。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="sqoop" scheme="https://jordenbruce.com/tags/sqoop/"/>
    
  </entry>
  
  <entry>
    <title>手动搭建Hive开发环境</title>
    <link href="https://jordenbruce.com/2019/09/15/hive-install/"/>
    <id>https://jordenbruce.com/2019/09/15/hive-install/</id>
    <published>2019-09-15T07:36:31.000Z</published>
    <updated>2019-09-17T14:16:28.583Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive 数据仓库软件有助于使用 SQL 读取，编写和管理驻留在分布式存储中的大型数据集。可以将结构投影到已存储的数据中。提供了命令行工具和 JDBC 驱动程序以将用户连接到 Hive<br><a id="more"></a></p><h2 id="0x00-安装-MySQL-5-6"><a href="#0x00-安装-MySQL-5-6" class="headerlink" title="0x00 安装 MySQL 5.6"></a>0x00 安装 MySQL 5.6</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">yum update gcc gcc-c++ make cmake openssl openssl-devel -y</span><br><span class="line">yum update bison bison-devel ncurses ncurses-devel zlib zlib-devel libaio libaio-devel -y</span><br><span class="line"></span><br><span class="line">yum install gcc gcc-c++ make cmake openssl openssl-devel -y</span><br><span class="line">yum install bison bison-devel ncurses ncurses-devel zlib zlib-devel libaio libaio-devel -y</span><br><span class="line"></span><br><span class="line">tar -xf MySQL-5.6.24-1.el6.x86_64.rpm-bundle.tar</span><br><span class="line">rpm -ihv MySQL-shared-compat-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">rpm -e mysql-libs-5.1.71-1.el6.x86_64</span><br><span class="line"></span><br><span class="line">groupadd mysql</span><br><span class="line">useradd mysql -s /sbin/nologin -M -g mysql</span><br><span class="line"></span><br><span class="line">rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">rpm -ihv MySQL-devel-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">rpm -ihv MySQL-shared-5.6.24-1.el6.x86_64.rpm</span><br><span class="line"></span><br><span class="line">cat /root/.mysql_secret</span><br><span class="line">service mysql start</span><br><span class="line">/usr/bin/mysql_secure_installation --user=mysql</span><br></pre></td></tr></table></figure><p><em>注意：mysql默认字符集必须设置为 latin1</em></p><h2 id="0x01-配置Hive元数据库"><a href="#0x01-配置Hive元数据库" class="headerlink" title="0x01 配置Hive元数据库"></a>0x01 配置Hive元数据库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shell&gt; mysql -uroot -p --default-character-set=latin1</span><br><span class="line">mysql&gt; CREATE USER &apos;hive&apos; IDENTIFIED BY &apos;mysql&apos;;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON hive.* TO &apos;hive&apos;@&apos;%&apos; WITH GRANT OPTION;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br><span class="line">mysql&gt; EXIT;</span><br></pre></td></tr></table></figure><h2 id="0x02-配置Hive"><a href="#0x02-配置Hive" class="headerlink" title="0x02 配置Hive"></a>0x02 配置Hive</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf apache-hive-1.2.2-bin.tar.gz -C /data/</span><br><span class="line">ln -s /data/apache-hive-1.2.2-bin/ /data/hive</span><br><span class="line">cd /data/hive/conf/</span><br><span class="line">cp hive-default.xml.template hive-site.xml</span><br><span class="line">vim hive-site.xml</span><br><span class="line"></span><br><span class="line">mv mysql-connector-java-5.1.44-bin.jar /data/hive/lib/</span><br><span class="line">cp lib/jline-2.12.jar /data/hadoop/share/hadoop/yarn/lib/</span><br><span class="line">vim ./bin/hive</span><br></pre></td></tr></table></figure><h2 id="0x03-启动Hive"><a href="#0x03-启动Hive" class="headerlink" title="0x03 启动Hive"></a>0x03 启动Hive</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br><span class="line">source ~/.bashrc</span><br><span class="line"></span><br><span class="line">hadoop fs -mkdir       /tmp</span><br><span class="line">hadoop fs -mkdir -p    /user/hive/warehouse</span><br><span class="line">hadoop fs -chmod g+w   /tmp</span><br><span class="line">hadoop fs -chmod g+w   /user/hive/warehouse</span><br><span class="line"></span><br><span class="line">hive</span><br><span class="line">$HIVE_HOME/bin/beeline -u jdbc:hive2://</span><br></pre></td></tr></table></figure><h2 id="0x04-hive-site-xml"><a href="#0x04-hive-site-xml" class="headerlink" title="0x04 hive-site.xml"></a>0x04 hive-site.xml</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;jdbc:mysql://mdw01:3306/hive?characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;mysql&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt; </span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h2 id="0x05-参考"><a href="#0x05-参考" class="headerlink" title="0x05 参考"></a>0x05 参考</h2><p><a href="https://blog.csdn.net/wjqwinn/article/details/52692308" target="_blank" rel="noopener">Hive在spark2.0.0启动时无法访问spark-assembly-*.jar的解决办法</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Hive 数据仓库软件有助于使用 SQL 读取，编写和管理驻留在分布式存储中的大型数据集。可以将结构投影到已存储的数据中。提供了命令行工具和 JDBC 驱动程序以将用户连接到 Hive&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>手动搭建Hadoop分布式运行环境</title>
    <link href="https://jordenbruce.com/2019/09/15/hadoop-install/"/>
    <id>https://jordenbruce.com/2019/09/15/hadoop-install/</id>
    <published>2019-09-15T04:56:42.000Z</published>
    <updated>2019-09-22T09:02:38.409Z</updated>
    
    <content type="html"><![CDATA[<p>当开始着手实践 Hadoop 时，安装 Hadoop 往往会成为新手的一道门槛。尽管安装其实很简单，书上有写到，官方网站也有 Hadoop 安装配置教程，但由于对 Linux 环境不熟悉，书上跟官网上简略的安装步骤新手往往 Hold 不住。加之网上不少教程也甚是坑，导致新手折腾老几天愣是没装好，很是打击学习热情。<br><a id="more"></a></p><h2 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h2><p>本教程详细记录了 hadoop 安装的全过程，还有配置文件的参数设置，一次性解决安装过程的所有问题。</p><h2 id="0x01-虚拟服务器"><a href="#0x01-虚拟服务器" class="headerlink" title="0x01 虚拟服务器"></a>0x01 虚拟服务器</h2><p>VMware Workstation 11.0</p><table><thead><tr><th>host</th><th>ip</th><th>os</th><th>role</th><th>cpu</th><th>memory</th><th>disk</th></tr></thead><tbody><tr><td>mdw01</td><td>192.168.100.186</td><td>CentOS 6.8 x64</td><td>master</td><td>1*2</td><td>8GB</td><td>30GB</td></tr><tr><td>sdw02</td><td>192.168.100.187</td><td>CentOS 6.8 x64</td><td>slave</td><td>1*2</td><td>4GB</td><td>30GB</td></tr><tr><td>sdw03</td><td>192.168.100.188</td><td>CentOS 6.8 x64</td><td>slave</td><td>1*2</td><td>4GB</td><td>30GB</td></tr></tbody></table><h2 id="0x02-系统配置"><a href="#0x02-系统配置" class="headerlink" title="0x02 系统配置"></a>0x02 系统配置</h2><p>(1) 修改主机名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hostname</span><br><span class="line">cat /etc/sysconfig/network</span><br><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p>(2) 关闭SELinux</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">vim /etc/selinux/config</span><br><span class="line">sestatus</span><br></pre></td></tr></table></figure><p>(3) 关闭iptables</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><p>(4) 安装JDK</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/</span><br><span class="line">tar -xf jdk-8u112-linux-x64.tar.gz</span><br><span class="line">chown -R root:root jdk1.8.0_112/</span><br><span class="line">ln -s /opt/jdk1.8.0_112/ /opt/java</span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">java -version</span><br><span class="line">rm -f jdk-8u112-linux-x64.tar.gz</span><br></pre></td></tr></table></figure><p>(5) 免密登陆ssh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br><span class="line">/etc/init.d/sshd restart</span><br><span class="line"></span><br><span class="line">useradd hadoop</span><br><span class="line">passwd hadoop</span><br><span class="line"></span><br><span class="line">ls -l /etc/sudoers</span><br><span class="line">chmod 640 /etc/sudoers</span><br><span class="line">vim /etc/sudoers</span><br><span class="line">chmod 0440 /etc/sudoers</span><br><span class="line"></span><br><span class="line">su hadoop</span><br><span class="line">ssh-keygen</span><br><span class="line">cd ~/.ssh/</span><br><span class="line">cat id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line">chmod 700 ~/.ssh</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br><span class="line">scp authorized_keys hadoop@sdw02:~/.ssh/</span><br><span class="line">ssh hadoop@sdw03</span><br></pre></td></tr></table></figure><p>(6) 时间同步NTP服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rpm -q ntp</span><br><span class="line">chkconfig ntpd on</span><br><span class="line">ntpdate -u 202.112.10.36</span><br><span class="line">hwclock -w</span><br><span class="line">vim /etc/ntp.conf</span><br><span class="line">vim /etc/sysconfig/ntpd</span><br><span class="line">service ntpd start</span><br><span class="line">netstat -tlunp | grep ntp</span><br><span class="line">ntpq -p</span><br></pre></td></tr></table></figure><h2 id="0x03-安装Hadoop"><a href="#0x03-安装Hadoop" class="headerlink" title="0x03 安装Hadoop"></a>0x03 安装Hadoop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">chown -R hadoop:hadoop /data/</span><br><span class="line">cd /data/</span><br><span class="line">tar -xf hadoop-2.6.4.tar.gz</span><br><span class="line">ln -s /data/hadoop-2.6.4/ /data/hadoop</span><br><span class="line">vim ~/.bashrc</span><br><span class="line"></span><br><span class="line">cd /data/hadoop/etc/hadoop</span><br><span class="line">vim hadoop-env.sh</span><br><span class="line">#vim yarn-env.sh</span><br><span class="line">vim core-site.xml</span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">vim mapred-site.xml</span><br><span class="line">vim yarn-site.xml</span><br><span class="line">vim slaves</span><br><span class="line"></span><br><span class="line">cd /data/hadoop/bin/</span><br><span class="line">./hdfs namenode -format</span><br><span class="line">cd /data/hadoop/sbin/</span><br><span class="line">./start-dfs.sh</span><br><span class="line">./start-yarn.sh</span><br><span class="line">./mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">./yarn-daemon.sh start proxyserver</span><br><span class="line"></span><br><span class="line">./stop-all.sh</span><br></pre></td></tr></table></figure><p>配置文件的详细参数设置，请参考 <a href="https://github.com/jordenbruce/archive/blob/master/hadoop_conf_file.zip" target="_blank" rel="noopener">hadoop_conf_files</a></p><p>安装完成后，访问下HDFS和Yarn地址：<br>HDFS：<a href="http://mdw01:50070/dfshealth.html#tab-overview" target="_blank" rel="noopener">http://mdw01:50070/dfshealth.html#tab-overview</a><br>Yarn：<a href="http://mdw01:8088/cluster" target="_blank" rel="noopener">http://mdw01:8088/cluster</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当开始着手实践 Hadoop 时，安装 Hadoop 往往会成为新手的一道门槛。尽管安装其实很简单，书上有写到，官方网站也有 Hadoop 安装配置教程，但由于对 Linux 环境不熟悉，书上跟官网上简略的安装步骤新手往往 Hold 不住。加之网上不少教程也甚是坑，导致新手折腾老几天愣是没装好，很是打击学习热情。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>编译Hadoop源码包</title>
    <link href="https://jordenbruce.com/2019/09/15/hadoop-build/"/>
    <id>https://jordenbruce.com/2019/09/15/hadoop-build/</id>
    <published>2019-09-15T04:02:01.000Z</published>
    <updated>2019-09-15T08:31:16.981Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop社区不提供64位编译好的版本，只能用源码自行编译64位版本。学习一项技术从安装开始，学习Hadoop要从编译开始。<br><a id="more"></a></p><h2 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h2><p>本文档编译的hadoop版本是hadoop-2.6.4-src.tar.gz</p><p><strong>重要提示</strong>：源码包编译的官方文档是压缩包的根目录下BUILDING.txt说明书。</p><p><img src="https://i.loli.net/2019/09/15/V4LJv1n2lOpqiyg.png" alt="Build instructions for Hadoop"></p><p><strong>安装问题的引入</strong>：Hadoop社区不提供64位编译好的版本，只能用源码自行编译64位版本。学习一项技术从安装开始，学习Hadoop要从编译开始。</p><h2 id="0x01-编译环境说明"><a href="#0x01-编译环境说明" class="headerlink" title="0x01 编译环境说明"></a>0x01 编译环境说明</h2><p>操作系统：Red Hat Enterprise Linux Server release 6.5 (Santiago)<br>核心信息：Kernel 2.6.32-431.el6.x86_64 on an x86_64</p><h2 id="0x02-安装系统支持包"><a href="#0x02-安装系统支持包" class="headerlink" title="0x02 安装系统支持包"></a>0x02 安装系统支持包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum -y install autoconf automake libtool cmake</span><br><span class="line">yum -y install ncurses-devel</span><br><span class="line">yum -y install openssl-devel</span><br><span class="line">yum -y install lzo-devel zlib-devel gcc gcc-c++</span><br><span class="line">yum -y install gcc gcc-c++ make</span><br></pre></td></tr></table></figure><h2 id="0x03-组件安装"><a href="#0x03-组件安装" class="headerlink" title="0x03 组件安装"></a>0x03 组件安装</h2><p>将所有组件包上传到主机/usr/local/src/目录下。</p><p>（1）安装JDK</p><p><em>注意：只能用1.7，否则编译会出错。</em> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf jdk-7u80-linux-x64.tar.gz -C /opt/modules/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export JAVA_HOME=/opt/modules/jdk1.7.0_80</span><br><span class="line">export JRE_HOME=$JAVA_HOME/jre</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</span><br></pre></td></tr></table></figure><p>（2）安装Maven</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf apache-maven-3.3.1-bin.tar.gz -C /opt/modules/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export MAVEN_HOME=/opt/modules/apache-maven-3.3.1</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line">vi /opt/modules/apache-maven-3.3.1/conf/settings.xml</span><br></pre></td></tr></table></figure><p>更改maven资料库，在<mirrors>里添加如下内容：</mirrors></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">&lt;id&gt;nexus-osc&lt;/id&gt;</span><br><span class="line">&lt;mirrorOf&gt;*&lt;/mirrorOf&gt;</span><br><span class="line">&lt;name&gt;Nexus osc&lt;/name&gt;</span><br><span class="line">&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><p>在<code>&lt;profiles&gt;&lt;/profiles&gt;</code>内新添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;profile&gt;</span><br><span class="line">&lt;id&gt;jdk-1.7&lt;/id&gt;</span><br><span class="line">&lt;activation&gt;</span><br><span class="line">&lt;jdk&gt;1.7&lt;/jdk&gt;</span><br><span class="line">&lt;/activation&gt;</span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">&lt;id&gt;nexus&lt;/id&gt;</span><br><span class="line">&lt;name&gt;local private nexus&lt;/name&gt;</span><br><span class="line">&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</span><br><span class="line">&lt;releases&gt;</span><br><span class="line">&lt;enabled&gt;true&lt;/enabled&gt;</span><br><span class="line">&lt;/releases&gt;</span><br><span class="line">&lt;snapshots&gt;</span><br><span class="line">&lt;enabled&gt;false&lt;/enabled&gt;</span><br><span class="line">&lt;/snapshots&gt;</span><br><span class="line">&lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br><span class="line">&lt;pluginRepositories&gt;</span><br><span class="line">&lt;pluginRepository&gt;</span><br><span class="line">&lt;id&gt;nexus&lt;/id&gt;</span><br><span class="line">&lt;name&gt;local private nexus&lt;/name&gt;</span><br><span class="line">&lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;</span><br><span class="line">&lt;releases&gt;</span><br><span class="line">&lt;enabled&gt;true&lt;/enabled&gt;</span><br><span class="line">&lt;/releases&gt;</span><br><span class="line">&lt;snapshots&gt;</span><br><span class="line">&lt;enabled&gt;false&lt;/enabled&gt;</span><br><span class="line">&lt;/snapshots&gt;</span><br><span class="line">&lt;/pluginRepository&gt;</span><br><span class="line">&lt;/pluginRepositories&gt;</span><br><span class="line">&lt;/profile&gt;</span><br></pre></td></tr></table></figure><p>如果不是第一次编译，可以配置本地仓库：<br><code>&lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt;</code></p><p>（3）安装Findbugs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf findbugs-3.0.1.tar.gz -C /opt/modules/</span><br><span class="line">vi /etc/profile</span><br><span class="line">export FINDBUGS_HOME=/opt/modules/findbugs-3.0.1</span><br><span class="line">export PATH=$PATH:$FINDBUGS_HOME/bin</span><br></pre></td></tr></table></figure><p>（4）安装ProtocolBuffer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tar xvf protobuf-2.5.0.tar.gz</span><br><span class="line">cd protobuf-2.5.0</span><br><span class="line">./configure --prefix=/opt/modules/protobuf</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">ldconfig</span><br><span class="line">protoc --version</span><br></pre></td></tr></table></figure><p>（5）<font color="red">上网</font>  </p><p>由于编译Hadoop过程中，Maven需要下载依赖库，所以必须保证主机能上网。</p><p>（6）安装Snappy（可选）</p><h2 id="0x04-源码编译"><a href="#0x04-源码编译" class="headerlink" title="0x04 源码编译"></a>0x04 源码编译</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">mvn -version</span><br><span class="line">findbugs -version</span><br><span class="line">protoc --version</span><br><span class="line">ping www.baidu.com</span><br><span class="line"></span><br><span class="line">tar zxvf hadoop-2.6.4-src.tar.gz -C /opt/</span><br><span class="line">cd hadoop-2.6.4-src/</span><br><span class="line">export MAVEN_OPTS=&quot;-Xms256m -Xmx512m&quot;</span><br><span class="line">mvn clean package -Pdist,native,docs -DskipTests -Dtar</span><br></pre></td></tr></table></figure><p>剩下的就交给电脑，人可以出去锻炼身体了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] Apache Hadoop Main ................................. SUCCESS [06:32 min]</span><br><span class="line">[INFO] Apache Hadoop Project POM .......................... SUCCESS [03:46 min]</span><br><span class="line">[INFO] Apache Hadoop Annotations .......................... SUCCESS [01:29 min]</span><br><span class="line">[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.329 s]</span><br><span class="line">[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [07:17 min]</span><br><span class="line">[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 57.156 s]</span><br><span class="line">[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [06:10 min]</span><br><span class="line">[INFO] Apache Hadoop Auth ................................. SUCCESS [05:18 min]</span><br><span class="line">[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 27.119 s]</span><br><span class="line">[INFO] Apache Hadoop Common ............................... SUCCESS [09:30 min]</span><br><span class="line">[INFO] Apache Hadoop NFS .................................. SUCCESS [  6.677 s]</span><br><span class="line">[INFO] Apache Hadoop KMS .................................. SUCCESS [04:45 min]</span><br><span class="line">[INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.040 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS ................................. SUCCESS [13:03 min]</span><br><span class="line">[INFO] Apache Hadoop HttpFS ............................... SUCCESS [04:10 min]</span><br><span class="line">[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [01:44 min]</span><br><span class="line">[INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  5.085 s]</span><br><span class="line">[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.033 s]</span><br><span class="line">[INFO] hadoop-yarn ........................................ SUCCESS [  0.059 s]</span><br><span class="line">[INFO] hadoop-yarn-api .................................... SUCCESS [01:25 min]</span><br><span class="line">[INFO] hadoop-yarn-common ................................. SUCCESS [01:29 min]</span><br><span class="line">[INFO] hadoop-yarn-server ................................. SUCCESS [  0.095 s]</span><br><span class="line">[INFO] hadoop-yarn-server-common .......................... SUCCESS [ 42.211 s]</span><br><span class="line">[INFO] hadoop-yarn-server-nodemanager ..................... SUCCESS [01:51 min]</span><br><span class="line">[INFO] hadoop-yarn-server-web-proxy ....................... SUCCESS [  2.956 s]</span><br><span class="line">[INFO] hadoop-yarn-server-applicationhistoryservice ....... SUCCESS [  7.023 s]</span><br><span class="line">[INFO] hadoop-yarn-server-resourcemanager ................. SUCCESS [ 23.905 s]</span><br><span class="line">[INFO] hadoop-yarn-server-tests ........................... SUCCESS [ 45.162 s]</span><br><span class="line">[INFO] hadoop-yarn-client ................................. SUCCESS [  8.784 s]</span><br><span class="line">[INFO] hadoop-yarn-applications ........................... SUCCESS [  0.047 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-distributedshell .......... SUCCESS [  2.790 s]</span><br><span class="line">[INFO] hadoop-yarn-applications-unmanaged-am-launcher ..... SUCCESS [  2.169 s]</span><br><span class="line">[INFO] hadoop-yarn-site ................................... SUCCESS [  0.052 s]</span><br><span class="line">[INFO] hadoop-yarn-registry ............................... SUCCESS [  5.526 s]</span><br><span class="line">[INFO] hadoop-yarn-project ................................ SUCCESS [  5.919 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client ............................ SUCCESS [  0.083 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-core ....................... SUCCESS [ 25.201 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-common ..................... SUCCESS [ 19.914 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-shuffle .................... SUCCESS [  3.998 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-app ........................ SUCCESS [ 11.686 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs ......................... SUCCESS [  8.481 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-jobclient .................. SUCCESS [ 28.587 s]</span><br><span class="line">[INFO] hadoop-mapreduce-client-hs-plugins ................. SUCCESS [  1.978 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  6.412 s]</span><br><span class="line">[INFO] hadoop-mapreduce ................................... SUCCESS [  4.931 s]</span><br><span class="line">[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 44.811 s]</span><br><span class="line">[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [  8.613 s]</span><br><span class="line">[INFO] Apache Hadoop Archives ............................. SUCCESS [  2.769 s]</span><br><span class="line">[INFO] Apache Hadoop Rumen ................................ SUCCESS [  6.654 s]</span><br><span class="line">[INFO] Apache Hadoop Gridmix .............................. SUCCESS [  5.080 s]</span><br><span class="line">[INFO] Apache Hadoop Data Join ............................ SUCCESS [  3.253 s]</span><br><span class="line">[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [  2.646 s]</span><br><span class="line">[INFO] Apache Hadoop Extras ............................... SUCCESS [  4.990 s]</span><br><span class="line">[INFO] Apache Hadoop Pipes ................................ SUCCESS [  8.460 s]</span><br><span class="line">[INFO] Apache Hadoop OpenStack support .................... SUCCESS [  5.232 s]</span><br><span class="line">[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [06:09 min]</span><br><span class="line">[INFO] Apache Hadoop Client ............................... SUCCESS [  8.045 s]</span><br><span class="line">[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  0.145 s]</span><br><span class="line">[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  7.135 s]</span><br><span class="line">[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.856 s]</span><br><span class="line">[INFO] Apache Hadoop Tools ................................ SUCCESS [  0.027 s]</span><br><span class="line">[INFO] Apache Hadoop Distribution ......................... SUCCESS [02:45 min]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 01:27 h</span><br><span class="line">[INFO] Finished at: 2016-11-05T15:02:45+08:00</span><br><span class="line">[INFO] Final Memory: 112M/369M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>编译成功后会打包，放在hadoop-dist/target目录下。</p><p><img src="https://i.loli.net/2019/09/15/lus6tF8BawgU7YK.png" alt="dist_target_dir"></p><p>hadoop-2.6.4.tar.gz 就是编译成功的二进制安装包，大功告成！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop社区不提供64位编译好的版本，只能用源码自行编译64位版本。学习一项技术从安装开始，学习Hadoop要从编译开始。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop发行版的选取</title>
    <link href="https://jordenbruce.com/2019/09/15/hadoop-release/"/>
    <id>https://jordenbruce.com/2019/09/15/hadoop-release/</id>
    <published>2019-09-15T01:19:22.000Z</published>
    <updated>2019-09-17T14:21:44.756Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop的开源协议决定了任何人可以对其进行修改，并作为开源或者商业版发布/销售。故而，目前Hadoop的发行版非常多，除了Apache的开源版本之外，还有Cloudera发行版(CDH)、Hortonworks发行版（HDP）、MapR等，这些发行版都是基于Apache Hadoop衍生出来的。<br><a id="more"></a></p><h2 id="0x00-综述"><a href="#0x00-综述" class="headerlink" title="0x00 综述"></a>0x00 综述</h2><p>其中，不收费的Hadoop发行版主要有三个，分别是：</p><ul><li>Apache基金会hadoop</li><li>Cloudera版本（Cloudera’s Distribution Including Apache Hadoop，简称“CDH”）</li><li>Hortonworks版本（Hortonworks Data Platform，简称“HDP”）</li></ul><p>在我任职过的公司当中，telecom使用了CDH，analysys使用了HDP，qtt使用了Apache Hadoop</p><h2 id="0x01-发行版的比较"><a href="#0x01-发行版的比较" class="headerlink" title="0x01 发行版的比较"></a>0x01 发行版的比较</h2><table><thead><tr><th>\</th><th>Apache Hadoop</th><th>CDH</th><th>HDP</th></tr></thead><tbody><tr><td>开源情况</td><td>100%开源</td><td>100%开源</td><td>100%开源</td></tr><tr><td>收费情况</td><td>完全免费</td><td>免费版和企业版</td><td>完全免费</td></tr><tr><td>管理工具</td><td>Apache Ambari</td><td>Cloudera Manager</td><td>Ambari</td></tr><tr><td>稳定性</td><td>中</td><td>高</td><td>高</td></tr><tr><td>运维成本</td><td>高</td><td>中</td><td>中</td></tr><tr><td>生态支持</td><td>兼容性差</td><td>完善</td><td>完善</td></tr></tbody></table><h2 id="0x02-选择决定"><a href="#0x02-选择决定" class="headerlink" title="0x02 选择决定"></a>0x02 选择决定</h2><p>考虑到大数据平台高效的部署和安装，中心化的配置管理，使用过程中的稳定性、兼容性、扩展性，<br>以及未来较为简单、高效的运维，遇到问题低廉的解决成本；建议使用第三方发行版本。</p><p>然而，本系列教程选取了Apache Hadoop社区版，考虑的是完全开源免费、社区活跃、文档与资料详实，便于深入学习。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Hadoop的开源协议决定了任何人可以对其进行修改，并作为开源或者商业版发布/销售。故而，目前Hadoop的发行版非常多，除了Apache的开源版本之外，还有Cloudera发行版(CDH)、Hortonworks发行版（HDP）、MapR等，这些发行版都是基于Apache Hadoop衍生出来的。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数</title>
    <link href="https://jordenbruce.com/2019/03/14/hive-udf/"/>
    <id>https://jordenbruce.com/2019/03/14/hive-udf/</id>
    <published>2019-03-14T11:11:30.000Z</published>
    <updated>2019-09-22T09:10:33.216Z</updated>
    
    <content type="html"><![CDATA[<p>Hive自带了许多内置函数（Built-in Functions），方便数据的处理分析。可是，有时候内置函数无法满足需求，这就需要自定义函数（User-Defined Functions , UDF）来实现想要的功能。<br><a id="more"></a></p><h2 id="1-创建自定义UDF"><a href="#1-创建自定义UDF" class="headerlink" title="1. 创建自定义UDF"></a>1. 创建自定义UDF</h2><p>编写UDF需要下面两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.exec.UDF</code> </li><li>实现 <code>evaluate</code> 函数，这个函数必须要有返回值，不能设置为void。同时建议使用mapreduce编程模型中的数据类型(Text,IntWritable等)，因为hive语句会被转换为mapreduce任务。</li></ul><p>完整的开发过程 请参考 <a href="https://blog.csdn.net/qq_32653877/article/details/87182898" target="_blank" rel="noopener">Java编写Hive的UDF</a></p><h2 id="2-自定义UDF的部署方式"><a href="#2-自定义UDF的部署方式" class="headerlink" title="2. 自定义UDF的部署方式"></a>2. 自定义UDF的部署方式</h2><p>官方提供了两种部署UDF的方式：</p><ul><li>临时部署（Temporary Functions）</li><li>永久部署（Permanent Functions）</li></ul><p>两者的区别在于：临时部署的方式，只会在当前Session下有效并可用；永久部署的方式，在部署成功后任何一个Hive客户端（重新启动的Hive客户端，已经启动的客户端需要重新加载）都可以使用。</p><h3 id="2-1-临时部署"><a href="#2-1-临时部署" class="headerlink" title="2.1 临时部署"></a>2.1 临时部署</h3><p>这个是最常见的Hive使用方式，通过hive命令来完成UDF的部署；<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; add jar /path/to/local.jar; </span><br><span class="line">hive&gt; create temporary function ua_parser as &apos;net.qutoutiao.data.hive.ParseUserAgent&apos;;</span><br></pre></td></tr></table></figure></p><p>建议函数名使用 <strong>下划线命名法</strong>（全部小写字母）。</p><h3 id="2-2-永久部署"><a href="#2-2-永久部署" class="headerlink" title="2.2 永久部署"></a>2.2 永久部署</h3><p>这种方式是 hive 0.13版本以后开始支持的注册方法；<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create function udf.ua_parser </span><br><span class="line">hive&gt; as &apos;net.qutoutiao.data.hive.ParseUserAgent&apos; </span><br><span class="line">hive&gt; using jar &apos;hdfs:///path/to/hive-udf-1.0.jar&apos;;</span><br></pre></td></tr></table></figure></p><p>需要注意的是：函数名称前面一定要带上数据库名称。</p><h2 id="3-函数相关的HQL语句"><a href="#3-函数相关的HQL语句" class="headerlink" title="3. 函数相关的HQL语句"></a>3. 函数相关的HQL语句</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">-- 查看所有函数(内置函数+自定义函数)</span><br><span class="line">show functions;</span><br><span class="line">-- 查看某个函数的使用说明</span><br><span class="line">describe function function_name;</span><br><span class="line">-- 创建临时自定义函数</span><br><span class="line">create temporary function function_name as class_name;</span><br><span class="line">-- 删除临时自定义函数</span><br><span class="line">drop temporary function [if exists] function_name;</span><br><span class="line">-- 创建永久自定义函数</span><br><span class="line">create function [db_name.]function_name as class_name</span><br><span class="line">  [using jar|file|archive &apos;file_uri&apos; [, jar|file|archive &apos;file_uri&apos;] ];</span><br><span class="line">-- 删除永久自定义函数</span><br><span class="line">drop function [if exists] function_name;</span><br><span class="line">-- 重载函数</span><br><span class="line">reload function;</span><br></pre></td></tr></table></figure><h2 id="4-扩展与延伸"><a href="#4-扩展与延伸" class="headerlink" title="4. 扩展与延伸"></a>4. 扩展与延伸</h2><p>Hive自定义函数的扩展，不仅有UDF（User-Defined Functions），还有UDAF（User-Defined Aggregate Functions）和UDTF（User-Defined Table-Generating Functions），详情请参考官方说明。<br>另外，如果数据处理在函数级别不能解决，还可以借助 <code>TRANSFORM</code> 自定义Map和Reduce函数。</p><h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5. 参考"></a>5. 参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins" target="_blank" rel="noopener">Creating Custom UDFs</a><br><a href="https://blog.csdn.net/qq_32653877/article/details/87182898" target="_blank" rel="noopener">Java编写Hive的UDF</a><br><a href="http://chaozi204.github.io/blog/hive-udf-deploy/" target="_blank" rel="noopener">Hive UDF 部署方式小结</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy" target="_blank" rel="noopener">Writing GenericUDAFs</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform" target="_blank" rel="noopener">Hive’s Transform functionality</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive自带了许多内置函数（Built-in Functions），方便数据的处理分析。可是，有时候内置函数无法满足需求，这就需要自定义函数（User-Defined Functions , UDF）来实现想要的功能。&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="https://jordenbruce.com/categories/Hive/"/>
    
    
      <category term="udf" scheme="https://jordenbruce.com/tags/udf/"/>
    
  </entry>
  
  <entry>
    <title>Hive分区表</title>
    <link href="https://jordenbruce.com/2019/03/04/hive-partition/"/>
    <id>https://jordenbruce.com/2019/03/04/hive-partition/</id>
    <published>2019-03-04T14:57:21.000Z</published>
    <updated>2019-09-22T09:10:43.826Z</updated>
    
    <content type="html"><![CDATA[<p>分区作为Hive数据的一种组织形式，每个表可以有一个或多个分区键，用于确定数据如何被存储；例如，带有日期分区列ds的表T具有存储在HDFS中的<code>&lt;table location&gt;/ds=&lt;date&gt;</code>目录中的特定日期的数据文件。分区允许Hive根据查询条件选择要扫描的分区数据，比如一个需要访问T表中满足<code>T.ds=&#39;2008-09-01&#39;</code>条件的查询，Hive只需扫描HDFS中<code>&lt;table location&gt;/ds=2008-09-01/</code>目录中的文件即可。<br><a id="more"></a></p><h2 id="1-分区表操作语句"><a href="#1-分区表操作语句" class="headerlink" title="1. 分区表操作语句"></a>1. 分区表操作语句</h2><p>1) 新建分区表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists default.dws_bhv_user_active_di (</span><br><span class="line">     imei           string comment &apos;设备号&apos;</span><br><span class="line">    ,act_cnt        bigint comment &apos;活跃次数&apos;</span><br><span class="line">    ,etl_time       string comment &apos;数据仓库更新时间&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;活跃设备的日增量&apos;</span><br><span class="line">partitioned by (</span><br><span class="line">    dt string comment &apos;日期分区&apos;</span><br><span class="line">)</span><br><span class="line">stored as parquet</span><br><span class="line">tblproperties(&apos;parquet.compression&apos;=&apos;SNAPPY&apos;)</span><br><span class="line">;</span><br></pre></td></tr></table></figure></p><p>2) 查询所有分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show partitions default.dws_bhv_user_active_di;</span><br></pre></td></tr></table></figure></p><p>3) 添加分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table default.dws_bhv_user_active_di add if not exists partition (dt=&apos;2019-03-04&apos;);</span><br></pre></td></tr></table></figure></p><p>4) 删除分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table default.dws_bhv_user_active_di drop if exists partition (dt=&apos;2019-03-04&apos;);</span><br></pre></td></tr></table></figure></p><p>5) 清空分区数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate table default.dws_bhv_user_active_di partition (dt=&apos;2019-03-04&apos;);</span><br></pre></td></tr></table></figure></p><p>6) 重写分区数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table default.dws_bhv_user_active_di partition (dt=&apos;2019-03-05&apos;)</span><br><span class="line">select imei,act_cnt,etl_time from default.dws_bhv_user_active_di</span><br><span class="line">where dt = &apos;2019-03-04&apos;;</span><br></pre></td></tr></table></figure></p><p>7) 修复分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msck table default.dws_bhv_user_active_di;</span><br></pre></td></tr></table></figure></p><h2 id="2-静态分区与动态分区"><a href="#2-静态分区与动态分区" class="headerlink" title="2. 静态分区与动态分区"></a>2. 静态分区与动态分区</h2><p>2.1) 术语</p><ul><li>静态分区（SP）列：在涉及多个分区列的<code>DML/DDL</code>中，其值在COMPILE TIME（由用户给出）处已知的列。</li><li>动态分区（DP）列：其值仅在执行时已知的列。</li></ul><p>2.2) 语法</p><p>在partition子句中，DP列的指定方式与SP列相同。唯一的区别是DP列没有值，而SP列有值。在partition子句中，我们需要指定所有分区列，即使它们都是DP列。<br>在<code>INSERT...SELECT...</code>查询中，动态分区列必须在SELECT语句的列中最后指定，并且与它们在<code>PARTITION()</code>子句中出现的顺序相同。</p><p>2.3) 动态分区写入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table default.dws_bhv_user_active_di partition (dt)</span><br><span class="line">select imei</span><br><span class="line">      ,act_cnt</span><br><span class="line">      ,etl_time</span><br><span class="line">      ,dt</span><br><span class="line">from default.dws_bhv_user_active_di</span><br><span class="line">where dt between &apos;2019-03-04&apos; and &apos;2019-03-05&apos;</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>2.4) 动态分区的参数</p><table><thead><tr><th>重要级别</th><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>高</td><td>hive.exec.dynamic.partition</td><td>true</td></tr><tr><td>高</td><td>hive.exec.dynamic.partition.mode</td><td>strict</td></tr><tr><td>中</td><td>hive.exec.max.dynamic.partitions.pernode</td><td>100</td></tr><tr><td>中</td><td>hive.exec.max.dynamic.partitions</td><td>1000</td></tr><tr><td>中</td><td>hive.exec.max.created.files</td><td>100000</td></tr><tr><td>低</td><td>hive.error.on.empty.partition</td><td>false</td></tr></tbody></table><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Design" target="_blank" rel="noopener">Hive Architecture Overview</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">Hive Data Definition Language</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML" target="_blank" rel="noopener">Hive DML: Dynamic Partition Inserts</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分区作为Hive数据的一种组织形式，每个表可以有一个或多个分区键，用于确定数据如何被存储；例如，带有日期分区列ds的表T具有存储在HDFS中的&lt;code&gt;&amp;lt;table location&amp;gt;/ds=&amp;lt;date&amp;gt;&lt;/code&gt;目录中的特定日期的数据文件。分区允许Hive根据查询条件选择要扫描的分区数据，比如一个需要访问T表中满足&lt;code&gt;T.ds=&amp;#39;2008-09-01&amp;#39;&lt;/code&gt;条件的查询，Hive只需扫描HDFS中&lt;code&gt;&amp;lt;table location&amp;gt;/ds=2008-09-01/&lt;/code&gt;目录中的文件即可。&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="https://jordenbruce.com/categories/Hive/"/>
    
    
      <category term="partition" scheme="https://jordenbruce.com/tags/partition/"/>
    
  </entry>
  
  <entry>
    <title>HQL内存溢出的参数调优</title>
    <link href="https://jordenbruce.com/2019/02/25/hive-oom/"/>
    <id>https://jordenbruce.com/2019/02/25/hive-oom/</id>
    <published>2019-02-24T17:26:20.000Z</published>
    <updated>2019-09-22T09:08:37.634Z</updated>
    
    <content type="html"><![CDATA[<p>我们在使用Hive进行ETL开发的过程中，关注更多的是使用HQL语言来准确地表达业务逻辑，而很少考虑到Hive对HQL语句的执行情况。当你辛辛苦苦地码完，将HQL语句扔给Hive去执行时，就有可能出现各种各样的报错，而其中一种比较常见的错误就是内存溢出（OOM，out of memory），通俗地讲就是内存不够。<br><a id="more"></a></p><h2 id="1-写在前面"><a href="#1-写在前面" class="headerlink" title="1. 写在前面"></a>1. 写在前面</h2><p>本文采用的软件版本如下：</p><ul><li>hive-2.0.1</li><li>hadoop-2.7.2</li></ul><p>hive使用MapReduce执行引擎，hadoop使用Yarn进行资源调度。</p><p>下文将从客户端提交HQL语句开始，Hive生成物理执行计划、Yarn资源分配、MapReduce执行，到执行结束，三个重点过程进行阐述，先理论再参数，希望OOM参数调优的问题得到收敛。</p><h2 id="2-Hive生成物理执行计划"><a href="#2-Hive生成物理执行计划" class="headerlink" title="2. Hive生成物理执行计划"></a>2. Hive生成物理执行计划</h2><p>先给出官网上关于Hive架构的经典流程图：</p><p><img src="https://i.loli.net/2019/02/24/5c72ad01e0c56.png" alt="Hive架构"></p><p>从这张图中，我们只需要明白一点即可：客户端提交的HQL语句，在Hive端的最终输出是物理执行计划，或者说是Job的有向无环图（a DAG of stages）。主要包括三种操作：</p><ul><li>MapReduce作业（a map/reduce job）</li><li>元数据操作（a metadata operation）</li><li>HDFS操作（an operation on HDFS）</li></ul><p>需要注意的是：这个过程主要是Hive优化器的执行，没有相关参数去控制内存的使用。当然，对于某些HQL语句适当地设置一些参数，可以得到更优的物理执行计划。比如常见的Map Join参数<code>hive.auto.convert.join</code>等。</p><h2 id="3-Yarn资源分配"><a href="#3-Yarn资源分配" class="headerlink" title="3. Yarn资源分配"></a>3. Yarn资源分配</h2><p>YARN是对Mapreduce V1重构得到的，有时候也成为MapReduce V2。由Hive生成物理执行计划，其中的MapReduce作业提交给Yarn来执行，详细的执行过程如下：</p><p><img src="https://i.loli.net/2019/02/24/5c72b4b3294ea.jpg" alt="MapReduce在Yarn下执行过程"></p><p>从上图可以看出，Yarn（以Container方式分配）控制着NodeManager、ApplicationMaster、Map和Reduce的内存使用，相关的内存参数有：</p><table><thead><tr><th>重要级别</th><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>中</td><td>yarn.nodemanager.resource.memory-mb</td><td>8192</td></tr><tr><td>中</td><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td></tr><tr><td>中</td><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td></tr><tr><td>中</td><td>yarn.scheduler.increment-allocation-mb</td><td>1024</td></tr><tr><td>高</td><td>yarn.app.mapreduce.am.resource.mb</td><td>1536</td></tr><tr><td>中</td><td>yarn.app.mapreduce.am.command-opts</td><td>-Xmx1024m</td></tr><tr><td>低</td><td>yarn.app.mapreduce.am.admin-command-opts</td><td></td></tr><tr><td>中</td><td>yarn.nodemanager.vmem-pmem-ratio</td><td>2.1</td></tr><tr><td>低</td><td>yarn.nodemanager.pmem-check-enabled</td><td>true</td></tr><tr><td>低</td><td>yarn.nodemanager.vmem-check-enabled</td><td>true</td></tr><tr><td>高</td><td>mapreduce.reduce.memory.mb</td><td>1024</td></tr><tr><td>中</td><td>mapreduce.reduce.java.opts</td><td></td></tr><tr><td>高</td><td>mapreduce.map.memory.mb</td><td>1024</td></tr><tr><td>中</td><td>mapreduce.map.java.opts</td></tr></tbody></table><h3 id="3-1-基础"><a href="#3-1-基础" class="headerlink" title="3.1 基础"></a>3.1 基础</h3><ul><li>NodeManager可用于分配的最大内存是yarn.nodemanager.resource.memory-mb；</li><li>Yarn的ResourceManger（简称RM）通过逻辑上的队列分配内存等资源给application，默认情况下RM允许最大AM申请Container资源为8192MB(“yarn.scheduler.maximum-allocation-mb“)，默认情况下的最小分配资源为1024M(“yarn.scheduler.minimum-allocation-mb“)，如果参数中需要的资源在此范围之外，在任务submit的时候会被直接拒绝掉；</li><li>AM只能以增量 (“yarn.scheduler.minimum-allocation-mb”) + (“yarn.scheduler.increment-allocation-mb”) 规整每个task需要的内存，并且申请的内存只能在（”yarn.scheduler.minimum-allocation-mb“）和(“yarn.scheduler.maximum-allocation-mb“) 的范围内向RM申请资源；</li><li>每个Map任务或Reduce任务分配的内存为mapreduce.reduce.memory.mb或mapreduce.map.memory.mb；</li></ul><h3 id="3-2-mapreduce-map-java-opts和mapreduce-map-memory-mb区别"><a href="#3-2-mapreduce-map-java-opts和mapreduce-map-memory-mb区别" class="headerlink" title="3.2 mapreduce.map.java.opts和mapreduce.map.memory.mb区别"></a>3.2 mapreduce.map.java.opts和mapreduce.map.memory.mb区别</h3><p>JVM进程跑在container中，mapreduce.map.java.opts能够通过Xmx设置JVM最大的heap的使用，一般设置为0.75倍的mapreduce.map.memory.mb ，因为需要为java code，非JVM内存使用等预留些空间；mapreduce.reduce.java.opts和mapreduce.reduce.memory.mb同理。</p><h3 id="3-3-虚拟内存"><a href="#3-3-虚拟内存" class="headerlink" title="3.3 虚拟内存"></a>3.3 虚拟内存</h3><p>默认的(“yarn.nodemanager.vmem-pmem-ratio“)设置为2.1，意味则map container或者reduce container分配的虚拟内存超过2.1倍的(“mapreduce.reduce.memory.mb“)或(“mapreduce.map.memory.mb“)就会被NM给KILL掉，如果 (“mapreduce.map.memory.mb”) 被设置为1536M那么总的虚拟内存为2.1*1536=3225.6MB</p><h3 id="3-4-内存检查"><a href="#3-4-内存检查" class="headerlink" title="3.4 内存检查"></a>3.4 内存检查</h3><p>如果虚拟内存检查被打开（yarn.nodemanager.vmem-check-enabled默认情况下为true），然后YARN将把抽取出来的容器及其子进程的VSIZE加起来和容器最大允许使用的虚拟内存进行比较。最大允许使用的虚拟内存是容器最大可使用的物理内存乘以 yarn.nodemanager.vmem-pmem-ratio（默认值是2.1）。所以，如果你的YARN容器配置的最大可使用物理内存为2GB，然后我们乘以 2.1 得到的就是容器最大可用的虚拟内存 4.2G 。</p><p>如果物理内存检查被打开（yarn.nodemanager.pmem-check-enabled默认情况为true），然后YARN将把抽取出来的容器及其子进程的RSS加起来和容器最大允许使用的物理内存进行比较。</p><p>如果物理内存或者虚拟内存其中一个的使用大于最大允许使用的，YARN将会被这个容器杀掉。</p><h3 id="3-5-参数全局图"><a href="#3-5-参数全局图" class="headerlink" title="3.5 参数全局图"></a>3.5 参数全局图</h3><p>参数多不要慌，下面来张图梳理下：</p><p><img src="https://i.loli.net/2019/02/25/5c72c07905cf3.jpg" alt="Yarn内存参数"></p><h2 id="4-MapReduce执行"><a href="#4-MapReduce执行" class="headerlink" title="4. MapReduce执行"></a>4. MapReduce执行</h2><p>MapReduce作业的重点是Shuffle过程，还是老套路，先给出官网上关于这个过程的经典流程图：</p><p><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/pNFrlaAqE9YU.png" alt="Shuffle过程"></p><p>当Map任务或Reduce任务以Container方式申请到相应的内存资源后，就进入了实际的执行过程中，其中涉及的参数有：</p><table><thead><tr><th>重要级别</th><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>高</td><td>mapreduce.job.maps</td><td>2</td></tr><tr><td>中</td><td>mapreduce.input.fileinputformat.split.minsize</td><td>1</td></tr><tr><td>中</td><td>dfs.blocksize</td><td>134217728</td></tr><tr><td>高</td><td>mapreduce.job.reduces</td><td>1</td></tr><tr><td>中</td><td>mapreduce.task.io.sort.mb</td><td>100</td></tr><tr><td>中</td><td>mapreduce.map.sort.spill.percent</td><td>0.80</td></tr><tr><td>中</td><td>mapreduce.task.io.sort.factor</td><td>10</td></tr><tr><td>中</td><td>mapreduce.map.output.compress</td><td>false</td></tr><tr><td>中</td><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>低</td><td>mapreduce.job.reduce.slowstart.completedmaps</td><td>0.05</td></tr><tr><td>中</td><td>mapreduce.reduce.shuffle.parallelcopies</td><td>5</td></tr><tr><td>高</td><td>mapreduce.reduce.shuffle.input.buffer.percent</td><td>0.70</td></tr></tbody></table><p>为了更好地理解每个参数作用的阶段，建议先阅读 <a href="https://jordenbruce.com/2019/02/19/hadoop-shuffle/">MapReduce之Shuffle过程详解</a>。</p><h3 id="4-1-Map任务"><a href="#4-1-Map任务" class="headerlink" title="4.1 Map任务"></a>4.1 Map任务</h3><p>（1）split分片：split是在逻辑上对输入数据进行的分片，并不会在磁盘上将其切分成分片进行存储。每个split都作为一个独立单位分配给一个map task去处理。决定split分片大小的参数有：</p><ul><li>mapreduce.job.maps</li><li>mapreduce.input.fileinputformat.split.minsize</li><li>dfs.blocksize (会话级别不可设置)</li></ul><p>（2）内存缓冲区：经过map处理后的键值对，不会立马写入磁盘，而是暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，设置缓冲区大小的参数有：</p><ul><li>mapreduce.task.io.sort.mb</li><li>mapreduce.map.sort.spill.percent</li></ul><p>（3）压缩：map端在写磁盘的时候采用压缩的方式将map的输出结果进行压缩是一个减少网络开销很有效的方法。其实，在Hadoop中早已为我们提供了一些压缩算法的实现，直接配置参数即可。</p><ul><li>mapreduce.map.output.compress</li><li>mapreduce.map.output.compress.codec</li></ul><h3 id="4-2-Reduce任务"><a href="#4-2-Reduce任务" class="headerlink" title="4.2 Reduce任务"></a>4.2 Reduce任务</h3><p>（1）文件拷贝：默认情况下，当整个MapReduce作业的所有已执行完成的Map Task任务数超过Map Task总数的 <code>mapreduce.job.reduce.slowstart.completedmaps</code> (默认为0.05) 后，ApplicationMaster便会开始调度执行Reduce Task任务。然后Reduce Task任务默认启动 <code>mapred.reduce.parallel.copies</code> (默认为5) 个MapOutputCopier线程到已完成的Map Task任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</p><p>（2）内存缓冲区：这个内存缓冲区大小的控制就不像map那样可以通过 <code>mapreduce.task.io.sort.mb</code> 来设定了，而是通过另外一个参数来设置：<code>mapred.job.shuffle.input.buffer.percent</code>（default 0.7）， 这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。</p><h2 id="5-HQL语句的日志输出"><a href="#5-HQL语句的日志输出" class="headerlink" title="5. HQL语句的日志输出"></a>5. HQL语句的日志输出</h2><p>经过漫长的理论铺垫，终于要到解决问题的时候了，HQL语句的内存溢出主要从日志分析开始。</p><ul><li>HQL语句的执行过程中，有哪些日志输出呢？分别存放在什么地方？如何分析出有用信息？</li><li>内存溢出包括哪几类？典型日志有哪些？调优什么参数可以解决？</li><li>小文件太多是如何产生的？调优什么参数可以合并小文件？</li></ul><p>等等一系列有关问题，且听下回分解。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Design" target="_blank" rel="noopener">Hive Architecture Overview</a><br><a href="https://segmentfault.com/a/1190000003777237" target="_blank" rel="noopener">Yarn下Mapreduce的内存参数理解</a><br><a href="https://jordenbruce.com/2019/02/19/hadoop-shuffle/">MapReduce之Shuffle过程详解</a><br><a href="https://blog.csdn.net/lazythinker/article/details/75497774" target="_blank" rel="noopener">HIVE参数调优（汇总）</a><br><a href="https://blog.csdn.net/aijiudu/article/details/72353510" target="_blank" rel="noopener">MapReduce过程详解及其性能优化</a><br><a href="https://my.oschina.net/OttoWu/blog/816049" target="_blank" rel="noopener">hadoop fair scheduler 的坑</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在使用Hive进行ETL开发的过程中，关注更多的是使用HQL语言来准确地表达业务逻辑，而很少考虑到Hive对HQL语句的执行情况。当你辛辛苦苦地码完，将HQL语句扔给Hive去执行时，就有可能出现各种各样的报错，而其中一种比较常见的错误就是内存溢出（OOM，out of memory），通俗地讲就是内存不够。&lt;br&gt;
    
    </summary>
    
      <category term="Hive" scheme="https://jordenbruce.com/categories/Hive/"/>
    
    
      <category term="oom" scheme="https://jordenbruce.com/tags/oom/"/>
    
  </entry>
  
  <entry>
    <title>YARN架构详解</title>
    <link href="https://jordenbruce.com/2019/02/21/hadoop-yarn/"/>
    <id>https://jordenbruce.com/2019/02/21/hadoop-yarn/</id>
    <published>2019-02-20T16:36:20.000Z</published>
    <updated>2019-09-22T09:09:00.580Z</updated>
    
    <content type="html"><![CDATA[<p>YARN（Yet Another Resource Negotiator）是一个通用的资源管理平台，可为各类计算框架提供资源的管理和调度。 其核心出发点是为了分离资源管理与作业调度/监控，实现分离的做法是拥有一个全局的资源管理器（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM），应用程序由一个作业（Job）或者Job的有向无环图（DAG）组成。<br><a id="more"></a></p><p>YARN可以将多种计算框架(如离线处理MapReduce、在线处理的Storm、迭代式计算框架Spark、流式处理框架S4等) 部署到一个公共集群中，共享集群的资源。并提供如下功能：</p><ul><li><p><strong>资源的统一管理和调度</strong>：集群中所有节点的资源(内存、CPU、磁盘、网络等)抽象为Container。计算框架需要资源进行运算任务时需要向YARN申请Container， YARN按照特定的策略对资源进行调度进行Container的分配。</p></li><li><p><strong>资源隔离</strong>：YARN使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。</p></li></ul><p>YARN是对Mapreduce V1重构得到的，有时候也成为MapReduce V2。 </p><p>YARN可以看成一个云操作系统，由一个ResourceManager和多个NodeManager组成， 它负责管理所有NodeManger上多维度资源， 并以Container(启动一个Container相当于启动一个进程)方式分配给应用程序启动ApplicationMaster(相当于主进程中运行逻辑) 或运行ApplicationMaster切分的各Task(相当于子进程中运行逻辑)。</p><h2 id="YARN体系架构"><a href="#YARN体系架构" class="headerlink" title="YARN体系架构"></a>YARN体系架构</h2><p>YARN架构如下图所示：<br><img src="https://i.loli.net/2019/02/20/5c6d021102002.png" alt="YARN架构"></p><p>YARN总体上是Master/Slave结构，主要由ResourceManager、NodeManager、 ApplicationMaster和Container等几个组件构成。</p><ul><li><p><strong>ResourceManager(RM)</strong>：负责对各NM上的资源进行统一管理和调度。将AM分配空闲的Container运行并监控其运行状态。对AM申请的资源请求分配相应的空闲Container。主要由两个组件构成：调度器和应用程序管理器：</p><ol><li>调度器(Scheduler)：调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container，从而限定每个任务使用的资源量。Shceduler不负责监控或者跟踪应用程序的状态，也不负责任务因为各种原因而需要的重启（由ApplicationMaster负责）。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。<br>调度器是可插拔的，例如CapacityScheduler、FairScheduler。具体看下文的调度算法。</li><li>应用程序管理器(Applications Manager)：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动AM、监控AM运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。</li></ol></li><li><p><strong>NodeManager (NM)</strong>：NM是每个节点上的资源和任务管理器。它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自AM的Container 启动/停止等请求。</p></li><li><p><strong>ApplicationMaster (AM)</strong>：用户提交的应用程序均包含一个AM，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。MapReduce就是原生支持的一种框架，可以在YARN上运行Mapreduce作业。有很多分布式应用都开发了对应的应用程序框架，用于在YARN上运行任务，例如Spark，Storm等。如果需要，我们也可以自己写一个符合规范的YARN application。</p></li><li><p><strong>Container</strong>：Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container 表示的。 YARN会为每个任务分配一个Container且该任务只能使用该Container中描述的资源。</p></li></ul><h2 id="YARN应用工作流程"><a href="#YARN应用工作流程" class="headerlink" title="YARN应用工作流程"></a>YARN应用工作流程</h2><p>如下图所示用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：</p><ul><li>启动AM ，如下步骤1~3；</li><li>由AM创建应用程序为它申请资源并监控它的整个运行过程，直到运行完成，如下步骤4~7。</li></ul><p><img src="https://i.loli.net/2019/02/20/5c6d03e204bfc.png" alt="YARN工作流程"></p><ol><li><p>用户向YARN中提交应用程序，其中包括AM程序、启动AM的命令、命令参数、用户程序等；事实上，需要准确描述运行ApplicationMaster的unix进程的所有信息。提交工作通常由YarnClient来完成。</p></li><li><p>RM为该应用程序分配第一个Container，并与对应的NM通信，要求它在这个Container中启动AM；</p></li><li><p>AM首先向RM注册，这样用户可以直接通过RM査看应用程序的运行状态，运行状态通过 AMRMClientAsync.CallbackHandler的getProgress() 方法来传递给RM。 然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4〜7；</p></li><li><p>AM采用轮询的方式通过RPC协议向RM申请和领取资源；资源的协调通过 AMRMClientAsync异步完成,相应的处理方法封装在AMRMClientAsync.CallbackHandler中。</p></li><li><p>—旦AM申请到资源后，便与对应的NM通信，要求它启动任务；通常需要指定一个ContainerLaunchContext，提供Container启动时需要的信息。</p></li><li><p>NM为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</p></li><li><p>各个任务通过某个RPC协议向AM汇报自己的状态和进度，以让AM随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；ApplicationMaster与NM的通信通过NMClientAsync object来完成，容器的所有事件通过NMClientAsync.CallbackHandler来处理。例如启动、状态更新、停止等。</p></li><li><p>应用程序运行完成后，AM向RM注销并关闭自己。</p></li></ol><h2 id="YARN资源调度模型"><a href="#YARN资源调度模型" class="headerlink" title="YARN资源调度模型"></a>YARN资源调度模型</h2><p>YARN提供了一个资源管理平台能够将集群中的资源统一进行管理。所有节点上的多维度资源都会根据申请抽象为一个个Container。</p><p>YARN采用了双层资源调度模型：</p><ul><li><p>RM中的资源调度器将资源分配给各个AM：资源分配过程是异步的。资源调度器将资源分配给一个应用程序后，它不会立刻push给对应的AM，而是暂时放到一个缓冲区中，等待AM通过周期性的心跳主动来取；</p></li><li><p>AM领取到资源后再进一步分配给它内部的各个任务：不属于YARN平台的范畴，由用户自行实现。</p></li></ul><p>也就是说，ResourceManager分配集群资源的时候，以抽象的Container形式分配给各应用程序，至于应用程序的子任务如何使用这些资源，由应用程序自行决定。</p><p>YARN目前采用的资源分配算法有三种。但真实的调度器实现中还对算法做了一定程度的优化。</p><ol><li><p>Capacity Scheduler：该调度器用于在共享、多租户（multi-tenant）的集群环境中运行Hadoop应用，对运营尽可能友好的同时最大化吞吐量和效用。 </p></li><li><p>Fair Scheduler：公平调度FAIR，该算法的思想是尽可能地公平调度，即已分配资源量少的优先级高。也就是说，在考虑如何分配资源时，调度器尽可能使得每个应用程序都能够得到大致相当的资源。默认情况下，公平性只通过内存来衡量，但是可以配置成内存和CPU。 </p></li></ol><h2 id="YARN的资源管理"><a href="#YARN的资源管理" class="headerlink" title="YARN的资源管理"></a>YARN的资源管理</h2><ol><li><p>资源调度和隔离是yarn作为一个资源管理系统，最重要且最基础的两个功能。资源调度由resourcemanager完成，而资源隔离由各个nodemanager实现。</p></li><li><p>Resourcemanager将某个nodemanager上资源分配给任务（这就是所谓的“资源调度”）后，nodemanager需按照要求为任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础和保证，这就是所谓的资源隔离。</p></li><li><p>当谈及到资源时，我们通常指内存、cpu、io三种资源。Hadoop yarn目前为止仅支持cpu和内存两种资源管理和调度。</p></li><li><p>内存资源多少决定任务的生死，如果内存不够，任务可能运行失败；相比之下，cpu资源则不同，它只会决定任务的快慢，不会对任务的生死产生影响。</p></li></ol><p>以上内容来自：<br><a href="https://blog.csdn.net/bingduanlbd/article/details/51880019" target="_blank" rel="noopener">理解Hadoop YARN架构</a><br><a href="http://www.cnblogs.com/wcwen1990/p/6737985.html" target="_blank" rel="noopener">YARN架构设计详解</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YARN（Yet Another Resource Negotiator）是一个通用的资源管理平台，可为各类计算框架提供资源的管理和调度。 其核心出发点是为了分离资源管理与作业调度/监控，实现分离的做法是拥有一个全局的资源管理器（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM），应用程序由一个作业（Job）或者Job的有向无环图（DAG）组成。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="yarn" scheme="https://jordenbruce.com/tags/yarn/"/>
    
  </entry>
  
</feed>
