<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JordenBruce</title>
  
  <subtitle>A thousand miles begins with a single step.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jordenbruce.com/"/>
  <updated>2019-11-09T11:41:45.847Z</updated>
  <id>https://jordenbruce.com/</id>
  
  <author>
    <name>JordenBruce</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hive自定义函数GenericUDAF</title>
    <link href="https://jordenbruce.com/2019/11/09/hive-genericudaf/"/>
    <id>https://jordenbruce.com/2019/11/09/hive-genericudaf/</id>
    <published>2019-11-09T11:37:59.000Z</published>
    <updated>2019-11-09T11:41:45.847Z</updated>
    
    <content type="html"><![CDATA[<p>UDAF 是 Hive 中用户自定义的聚合函数，特点是输入多行输出一行，Hive 内置 UDAF 函数包括有 sum() 与 count() 等。UDAF 实现有简单与通用两种方式，简单 UDAF 因为使用 Java 反射导致性能损失，而且有些特性不能使用，已经被弃用了；在本文中我们将关注 Hive 中自定义聚合函数-GenericUDAF，即通用方式。<br><a id="more"></a></p><h2 id="0x00-自定义-GenericUDAF-开发"><a href="#0x00-自定义-GenericUDAF-开发" class="headerlink" title="0x00 自定义 GenericUDAF 开发"></a>0x00 自定义 GenericUDAF 开发</h2><p>编写 GenericUDAF 需要下面两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver</code> ，重写 <code>getEvaluator</code> 函数；</li><li>依据 <code>getEvaluator</code> 函数返回值，编写内部静态类，继承 <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator</code>，并重写其7个方法；</li></ul><p>上述过程涉及两个重要抽象类 Resolver 与 Evaluator，其中 Resolver 官方不建议使用 GenericUDAFResolver2 接口，而使用 AbstractGenericUDAFResolver 接口；Evaluator 使用 GenericUDAFEvaluator 接口，并且必须是 public static 子类，需要重写的方法有 <code>init</code>，<code>getNewAggregationBuffer</code>，<code>iterate</code>，<code>terminatePartial</code>，<code>merge</code>，<code>terminate</code>，<code>reset</code> 共7个；理解 Evaluator 之前，必须先理解 ObjectInspector 接口 与 Model 内部类；另外，UDAF 逻辑处理主要发生在 Evaluator 中。</p><p>ObjectInspector 作用主要是解耦数据使用与数据格式，使得数据流在输入输出端切换不同的输入输出格式，不同的 Operator 上使用不同的格式。</p><p>Model 代表了 UDAF 在 MapReduce 的各个阶段，具体如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public static enum Mode &#123;</span><br><span class="line">  /**</span><br><span class="line">   * PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合</span><br><span class="line">   * 将会调用iterate()和terminatePartial()</span><br><span class="line">   */</span><br><span class="line">  PARTIAL1,</span><br><span class="line">  /**</span><br><span class="line">   * PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合:</span><br><span class="line">   * 将会调用merge() 和 terminatePartial() </span><br><span class="line">   */</span><br><span class="line">  PARTIAL2,</span><br><span class="line">  /**</span><br><span class="line">   * FINAL: mapreduce的reduce阶段:从部分数据的聚合到完全聚合 </span><br><span class="line">   * 将会调用merge()和terminate()</span><br><span class="line">   */</span><br><span class="line">  FINAL,</span><br><span class="line">  /**</span><br><span class="line">   * COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了:从原始数据直接到完全聚合</span><br><span class="line">   * 将会调用 iterate()和terminate()</span><br><span class="line">   */</span><br><span class="line">  COMPLETE</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>一般情况下，完整的 UDAF 逻辑是一个 mapreduce 过程，如果有 mapper 和 reducer，就会经历 PARTIAL1(mapper)，FINAL(reducer)，如果还有 combiner，那就会经历 PARTIAL1(mapper)，PARTIAL2(combiner)，FINAL(reducer)。而有一些情况下的 mapreduce，只有 mapper 没有 reducer，所以就会只有 COMPLETE 阶段，这个阶段直接输入原始数据，出结果。</p><p><img src="https://i.loli.net/2019/11/09/xHGvRTEm5O9jk34.png" alt="Model各阶段对应Evaluator方法调用"></p><h2 id="0x01-散度-DivergenceGenericUDAF-示例代码"><a href="#0x01-散度-DivergenceGenericUDAF-示例代码" class="headerlink" title="0x01 散度 DivergenceGenericUDAF 示例代码"></a>0x01 散度 DivergenceGenericUDAF 示例代码</h2><p><img src="https://i.loli.net/2019/11/09/nGODqcvX7Ywr3xi.png" alt="散度数学公式"></p><p>上述公式的参考代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;</span><br><span class="line">import org.apache.hadoop.hive.serde2.io.DoubleWritable;</span><br><span class="line">import org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.ql.util.JavaDataModel;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">        name = &quot;divergence&quot;,</span><br><span class="line">        value = &quot;_FUNC_(px, qy) - Calculate divergence from px, qy &quot;)</span><br><span class="line">public class DivergenceGenericUDAF extends AbstractGenericUDAFResolver &#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException &#123;</span><br><span class="line">        if (parameters.length != 2) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(parameters.length - 1,</span><br><span class="line">                    &quot;Exactly two arguments are expected.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (parameters[0].getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(0,</span><br><span class="line">                    &quot;Only primitive type arguments are accepted but &quot;</span><br><span class="line">                            + parameters[0].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (parameters[1].getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(1,</span><br><span class="line">                    &quot;Only primitive type arguments are accepted but &quot;</span><br><span class="line">                            + parameters[1].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        switch (((PrimitiveTypeInfo) parameters[0]).getPrimitiveCategory()) &#123;</span><br><span class="line">            case BYTE:</span><br><span class="line">            case SHORT:</span><br><span class="line">            case INT:</span><br><span class="line">            case LONG:</span><br><span class="line">            case FLOAT:</span><br><span class="line">            case DOUBLE:</span><br><span class="line">            case TIMESTAMP:</span><br><span class="line">            case DECIMAL:</span><br><span class="line">                switch (((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()) &#123;</span><br><span class="line">                    case BYTE:</span><br><span class="line">                    case SHORT:</span><br><span class="line">                    case INT:</span><br><span class="line">                    case LONG:</span><br><span class="line">                    case FLOAT:</span><br><span class="line">                    case DOUBLE:</span><br><span class="line">                    case TIMESTAMP:</span><br><span class="line">                    case DECIMAL:</span><br><span class="line">                        return new GenericUDAFDivergenceEvaluator();</span><br><span class="line">                    case STRING:</span><br><span class="line">                    case BOOLEAN:</span><br><span class="line">                    case DATE:</span><br><span class="line">                    default:</span><br><span class="line">                        throw new UDFArgumentTypeException(1,</span><br><span class="line">                                &quot;Only numeric or string type arguments are accepted but &quot;</span><br><span class="line">                                        + parameters[1].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">            case STRING:</span><br><span class="line">            case BOOLEAN:</span><br><span class="line">            case DATE:</span><br><span class="line">            default:</span><br><span class="line">                throw new UDFArgumentTypeException(0,</span><br><span class="line">                        &quot;Only numeric or string type arguments are accepted but &quot;</span><br><span class="line">                                + parameters[0].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class GenericUDAFDivergenceEvaluator extends GenericUDAFEvaluator &#123;</span><br><span class="line"></span><br><span class="line">        private PrimitiveObjectInspector pInputOI;</span><br><span class="line">        private PrimitiveObjectInspector qInputOI;</span><br><span class="line"></span><br><span class="line">        private PrimitiveObjectInspector partialOI;</span><br><span class="line"></span><br><span class="line">        private Object partialResult;</span><br><span class="line"></span><br><span class="line">        private DoubleWritable result;</span><br><span class="line"></span><br><span class="line">        /*</span><br><span class="line">         * PARTIAL1  (input, partial)</span><br><span class="line">         * PARTIAL2  (partial, partial)</span><br><span class="line">         * FINAL     (partial, output)</span><br><span class="line">         * COMPLETE  (input, output)</span><br><span class="line">         */</span><br><span class="line">        @Override</span><br><span class="line">        public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException &#123;</span><br><span class="line">            super.init(m, parameters);</span><br><span class="line">            result = new DoubleWritable(0);</span><br><span class="line">            // initialize input</span><br><span class="line">            if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) &#123;</span><br><span class="line">                assert (parameters.length == 2);</span><br><span class="line">                pInputOI = (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">                qInputOI = (PrimitiveObjectInspector) parameters[1];</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                partialOI = (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">            &#125;</span><br><span class="line">            // initialize output</span><br><span class="line">            if (m == Mode.PARTIAL1 || m == Mode.PARTIAL2) &#123;</span><br><span class="line">                partialOI = (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">                partialResult = new DoubleWritable(0);</span><br><span class="line">                return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                partialResult = new DoubleWritable(0);</span><br><span class="line">                return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        static class SumDoubleAgg extends AbstractAggregationBuffer &#123;</span><br><span class="line">            boolean empty;</span><br><span class="line">            double sum;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public int estimate() &#123; return JavaDataModel.PRIMITIVES1 + JavaDataModel.PRIMITIVES2; &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void reset(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line">            divergenceAgg.empty = true;</span><br><span class="line">            divergenceAgg.sum = 0.0;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = new SumDoubleAgg();</span><br><span class="line">            reset(divergenceAgg);</span><br><span class="line">            return divergenceAgg;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException &#123;</span><br><span class="line">            assert (parameters.length == 2);</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line"></span><br><span class="line">            double vp = PrimitiveObjectInspectorUtils.getDouble(parameters[0], pInputOI);</span><br><span class="line">            double vq = PrimitiveObjectInspectorUtils.getDouble(parameters[1], qInputOI);</span><br><span class="line"></span><br><span class="line">            divergenceAgg.sum += vp * Math.log(vp / vq);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public Object terminatePartial(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line">            partialResult = new DoubleWritable(divergenceAgg.sum);</span><br><span class="line">            return partialResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void merge(AggregationBuffer agg, Object partial) throws HiveException &#123;</span><br><span class="line">            if (partial != null) &#123;</span><br><span class="line">                SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line"></span><br><span class="line">                double subSum = PrimitiveObjectInspectorUtils.getDouble(partial, partialOI);</span><br><span class="line">                divergenceAgg.sum += subSum;</span><br><span class="line"></span><br><span class="line">                divergenceAgg.empty = false;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public Object terminate(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line">            if (divergenceAgg.empty) &#123;</span><br><span class="line">                result = null;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                result.set(divergenceAgg.sum);</span><br><span class="line">            &#125;</span><br><span class="line">            return result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x02-代码走读"><a href="#0x02-代码走读" class="headerlink" title="0x02 代码走读"></a>0x02 代码走读</h2><ol><li><p>getEvaluator() 被调用<br>1.1 检查输入参数的个数；<br>1.2 检查输入参数的类型；<br>1.3 根据不同的输入参数类型，返回对应的 GenericUDAFEvaluator；</p></li><li><p>类 GenericUDAFDivergenceEvaluator 的方法说明<br>2.1 init() 方法，定义 mapreduce 不同阶段的输入与输出；<br>2.2 getNewAggregationBuffer() 方法，获取新的中间结果；<br>2.3 iterate() 方法，读取输入行的 p,q 累加至中间结果；<br>2.4 terminatePartial() 方法，输出中间结果；<br>2.5 merge() 方法，聚合中间结果；<br>2.6 terminate() 方法，输出聚合结果；<br>2.7 reset() 方法，重置中间结果；</p></li><li><p>方法调用过程（参考 Model各阶段对应Evaluator方法调用）</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy" target="_blank" rel="noopener">GenericUDAFCaseStudy</a><br><a href="https://blog.csdn.net/kent7306/article/details/50110067" target="_blank" rel="noopener">Hive UDAF开发详解</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;UDAF 是 Hive 中用户自定义的聚合函数，特点是输入多行输出一行，Hive 内置 UDAF 函数包括有 sum() 与 count() 等。UDAF 实现有简单与通用两种方式，简单 UDAF 因为使用 Java 反射导致性能损失，而且有些特性不能使用，已经被弃用了；在本文中我们将关注 Hive 中自定义聚合函数-GenericUDAF，即通用方式。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数GenericUDTF</title>
    <link href="https://jordenbruce.com/2019/11/07/hive-genericudtf/"/>
    <id>https://jordenbruce.com/2019/11/07/hive-genericudtf/</id>
    <published>2019-11-07T12:18:20.000Z</published>
    <updated>2019-11-07T14:20:12.045Z</updated>
    
    <content type="html"><![CDATA[<p>之前介绍的 UDF 特点是输入一行输出一行；本文将要介绍的是 UDTF，其特点是输入一行输出多行，而使用的接口是 GenericUDTF，比 UDF 更为复杂。<br><a id="more"></a></p><h2 id="0x00-自定义-GenericUDTF-开发"><a href="#0x00-自定义-GenericUDTF-开发" class="headerlink" title="0x00 自定义 GenericUDTF 开发"></a>0x00 自定义 GenericUDTF 开发</h2><p>编写 GenericUDTF 需要两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF</code> 类；</li><li>重写 <code>initialize</code> ，<code>process</code> ，<code>close</code> 三个方法；</li></ul><p>每个方法有着不同的作用，参考如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 该方法指定输入输出参数：输入参数的ObjectInspector与输出参数的StructObjectInspector</span><br><span class="line">abstract StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException; </span><br><span class="line"> </span><br><span class="line">// 该方法处理输入记录，然后通过forward()方法返回输出结果</span><br><span class="line">abstract void process(Object[] record) throws HiveException;</span><br><span class="line"> </span><br><span class="line">// 该方法用于通知UDTF没有行可以处理了 </span><br><span class="line">// 另外，可以在该方法中清理代码或者产生额外的输出</span><br><span class="line">abstract void close() throws HiveException;</span><br></pre></td></tr></table></figure><p>其中 process() 方法中有个 forward() 方法需要解释下，对于每一行的输入都会有多行的输出，每一行输出时都要调用 forward() 方法，并定义输出行的格式（一列或多列）。</p><h2 id="0x01-官方-ExplodeGenericUDTF-示例代码"><a href="#0x01-官方-ExplodeGenericUDTF-示例代码" class="headerlink" title="0x01 官方 ExplodeGenericUDTF 示例代码"></a>0x01 官方 ExplodeGenericUDTF 示例代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.Map.Entry;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.TaskExecutionException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">        name = &quot;explode&quot;,</span><br><span class="line">        value = &quot;_FUNC_(a) - separates the elements of array a into multiple rows,&quot;</span><br><span class="line">                + &quot; or the elements of a map into multiple rows and columns &quot;)</span><br><span class="line">public class ExplodeGenericUDTF extends GenericUDTF &#123;</span><br><span class="line"></span><br><span class="line">    private transient ObjectInspector inputOI = null;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close() throws HiveException &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException &#123;</span><br><span class="line">        if (args.length != 1) &#123;</span><br><span class="line">            throw new UDFArgumentException(&quot;explode() takes only one argument&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;();</span><br><span class="line">        ArrayList&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;();</span><br><span class="line"></span><br><span class="line">        switch (args[0].getCategory()) &#123;</span><br><span class="line">            case LIST:</span><br><span class="line">                inputOI = args[0];</span><br><span class="line">                fieldNames.add(&quot;col&quot;);</span><br><span class="line">                fieldOIs.add(((ListObjectInspector)inputOI).getListElementObjectInspector());</span><br><span class="line">                break;</span><br><span class="line">            case MAP:</span><br><span class="line">                inputOI = args[0];</span><br><span class="line">                fieldNames.add(&quot;key&quot;);</span><br><span class="line">                fieldNames.add(&quot;value&quot;);</span><br><span class="line">                fieldOIs.add(((MapObjectInspector)inputOI).getMapKeyObjectInspector());</span><br><span class="line">                fieldOIs.add(((MapObjectInspector)inputOI).getMapValueObjectInspector());</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">                throw new UDFArgumentException(&quot;explode() takes an array or a map as a parameter&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private transient final Object[] forwardListObj = new Object[1];</span><br><span class="line">    private transient final Object[] forwardMapObj = new Object[2];</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void process(Object[] o) throws HiveException &#123;</span><br><span class="line">        switch (inputOI.getCategory()) &#123;</span><br><span class="line">            case LIST:</span><br><span class="line">                ListObjectInspector listOI = (ListObjectInspector)inputOI;</span><br><span class="line">                List&lt;?&gt; list = listOI.getList(o[0]);</span><br><span class="line">                if (list == null) &#123;</span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">                for (Object r : list) &#123;</span><br><span class="line">                    forwardListObj[0] = r;</span><br><span class="line">                    forward(forwardListObj); //输出一行</span><br><span class="line">                &#125;</span><br><span class="line">                break;</span><br><span class="line">            case MAP:</span><br><span class="line">                MapObjectInspector mapOI = (MapObjectInspector)inputOI;</span><br><span class="line">                Map&lt;?,?&gt; map = mapOI.getMap(o[0]);</span><br><span class="line">                if (map == null) &#123;</span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">                for (Entry&lt;?,?&gt; r : map.entrySet()) &#123;</span><br><span class="line">                    forwardMapObj[0] = r.getKey();</span><br><span class="line">                    forwardMapObj[1] = r.getValue();</span><br><span class="line">                    forward(forwardMapObj); //输出一行</span><br><span class="line">                &#125;</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">                throw new TaskExecutionException(&quot;explode() can only operate on an array or a map&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return &quot;explode&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x02-代码走读"><a href="#0x02-代码走读" class="headerlink" title="0x02 代码走读"></a>0x02 代码走读</h2><p>方法的调用关系如下：</p><ol><li><p>initialize() 被调用<br>1.1 检查输入参数的个数<br>1.2 检查参数的类型是否为 LIST 或 MAP，并保存 inputOI 用以供 process() 使用<br>1.3 返回 StructObjectInspector，并定义好输出行的格式</p></li><li><p>process() 被调用<br>2.1. 判断输入参数的数据类型<br>2.2. 对于 LIST 类型，每一个元素输出一行，且只有一列<br>2.3. 对于 MAP 类型，每一对键值输出一行，且有两列<br>2.4. 其他数据类型抛出异常</p></li><li><p>close() 被调用，什么都不做</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide+UDTF" target="_blank" rel="noopener">DeveloperGuide UDTF</a><br><a href="https://jordenbruce.com/2019/09/30/hql-function/">HiveQL的函数概览</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前介绍的 UDF 特点是输入一行输出一行；本文将要介绍的是 UDTF，其特点是输入一行输出多行，而使用的接口是 GenericUDTF，比 UDF 更为复杂。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数GenericUDF</title>
    <link href="https://jordenbruce.com/2019/11/06/hive-genericudf/"/>
    <id>https://jordenbruce.com/2019/11/06/hive-genericudf/</id>
    <published>2019-11-06T13:39:44.000Z</published>
    <updated>2019-11-07T14:02:38.818Z</updated>
    
    <content type="html"><![CDATA[<p>编写 Apache Hive 用户自定义函数（UDF）有两个不同的接口：一个是非常简单的 UDF，上一篇已经介绍过了；还有一个是 GenericUDF，相对复杂点。两个 API 的区别是：如果函数的参数和返回都是基础数据类型，那么简单 API（UDF）可以胜任；但是，如果你想写一个函数用来操作内嵌数据结构，如 Map、List 和 Set，此时就需要去熟悉复杂 API（GenericUDF）。<br><a id="more"></a></p><h2 id="0x00-自定义-GenericUDF-开发"><a href="#0x00-自定义-GenericUDF-开发" class="headerlink" title="0x00 自定义 GenericUDF 开发"></a>0x00 自定义 GenericUDF 开发</h2><p>编写 GenericUDF 需要两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</code> 类；</li><li>重写 <code>initialize</code> ，<code>evaluate</code> ，<code>getDisplayString</code> 三个方法；</li></ul><p>每个方法有着不同的作用，参考如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// 这个方法只调用一次，并且在evaluate()方法之前调用。</span><br><span class="line">// 该方法接受的参数是一个ObjectInspectors数组，表示函数输入参数类型。</span><br><span class="line">// 1.检查接受正确的参数个数；</span><br><span class="line">// 2.检查接受正确的参数类型；</span><br><span class="line">// 3.定义返回值被序列化的一致类型。</span><br><span class="line">abstract ObjectInspector initialize(ObjectInspector[] arguments);</span><br><span class="line"></span><br><span class="line">// 这个方法类似UDF的evaluate()方法。它处理真实的参数，并返回最终结果。</span><br><span class="line">abstract Object evaluate(DeferredObject[] arguments);</span><br><span class="line"></span><br><span class="line">// 这个方法用于当实现的GenericUDF出错的时候，打印出提示信息。</span><br><span class="line">// 而提示信息就是你实现该方法最后返回的字符串。 </span><br><span class="line">abstract String getDisplayString(String[] children);</span><br></pre></td></tr></table></figure><p>其中有个 ObjectInspector 需要解释下，简单来说是帮助使用者访问需要序列化或者反序列化的对象，为数据类型提供一致性的访问接口。有关 ObjectInspector 更深入地理解，请参考 <a href="https://www.jianshu.com/p/5ea006282238" target="_blank" rel="noopener">Hive-ObjectInspector</a></p><h2 id="0x01-官方-ArrayGenericUDF-示例代码"><a href="#0x01-官方-ArrayGenericUDF-示例代码" class="headerlink" title="0x01 官方 ArrayGenericUDF 示例代码"></a>0x01 官方 ArrayGenericUDF 示例代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">        name = &quot;array&quot;,</span><br><span class="line">        value = &quot;_FUNC_(n0, n1...) - Creates an array with the given elements &quot;)</span><br><span class="line">public class ArrayGenericUDF extends GenericUDF &#123;</span><br><span class="line"></span><br><span class="line">    private transient Converter[] converters;</span><br><span class="line">    private transient ArrayList&lt;Object&gt; ret = new ArrayList&lt;Object&gt;();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123;</span><br><span class="line"></span><br><span class="line">        if (arguments.length &lt; 1) &#123;</span><br><span class="line">            throw new UDFArgumentLengthException(&quot;array() takes at least one parameter.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);</span><br><span class="line"></span><br><span class="line">        for (int i = 0; i &lt; arguments.length; i++) &#123;</span><br><span class="line">            if (!returnOIResolver.update(arguments[i])) &#123;</span><br><span class="line">                throw new UDFArgumentTypeException(i, &quot;Argument type \&quot;&quot;</span><br><span class="line">                        + arguments[i].getTypeName()</span><br><span class="line">                        + &quot;\&quot; is different from preceding arguments. &quot;</span><br><span class="line">                        + &quot;Previous type was \&quot;&quot; + arguments[i - 1].getTypeName() + &quot;\&quot;&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        converters = new Converter[arguments.length];</span><br><span class="line"></span><br><span class="line">        ObjectInspector returnOI =</span><br><span class="line">                returnOIResolver.get(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">        for (int i = 0; i &lt; arguments.length; i++) &#123;</span><br><span class="line">            converters[i] = ObjectInspectorConverters.getConverter(arguments[i], returnOI);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return ObjectInspectorFactory.getStandardListObjectInspector(returnOI);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public Object evaluate(DeferredObject[] arguments) throws HiveException &#123;</span><br><span class="line">        ret.clear();</span><br><span class="line">        for (int i = 0; i &lt; arguments.length; i++) &#123;</span><br><span class="line">            ret.add(converters[i].convert(arguments[i].get()));</span><br><span class="line">        &#125;</span><br><span class="line">        return ret;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String getDisplayString(String[] children) &#123;</span><br><span class="line">        return getStandardDisplayString(&quot;array&quot;, children, &quot;,&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x02-代码走读"><a href="#0x02-代码走读" class="headerlink" title="0x02 代码走读"></a>0x02 代码走读</h2><p>方法的调用关系如下：</p><ol><li><p>ArrayGenericUDF 用默认的构造器来初始化；</p></li><li><p>initialize() 被调用，传入函数参数的 ObjectInspector[] 数组；<br>2.1. 检查传入的参数个数与每个参数的数据类型是正确的；<br>2.2. 保存 converters (ObjectInspector) 用以供 evaluate() 使用；<br>2.3. 返回 ListObjectInspector，让 Hive 能够读取该函数的返回结果；</p></li><li><p>对于查询中的每一行，evaluate() 方法都会被调用，并传入该行的指定列；<br>3.1. 利用 initialize() 方法中存储的 converters (ObjectInspector) 来抽取出正确的值；<br>3.2. 执行处理逻辑然后用 initialize() 返回的 ListObjectInspector 来序列化返回值；</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/SerDe" target="_blank" rel="noopener">SerDe</a><br><a href="https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html" target="_blank" rel="noopener">Hadoop Hive UDF Tutorial - Extending Hive with Custom Functions</a><br><a href="https://www.jianshu.com/p/5ea006282238" target="_blank" rel="noopener">Hive-ObjectInspector</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;编写 Apache Hive 用户自定义函数（UDF）有两个不同的接口：一个是非常简单的 UDF，上一篇已经介绍过了；还有一个是 GenericUDF，相对复杂点。两个 API 的区别是：如果函数的参数和返回都是基础数据类型，那么简单 API（UDF）可以胜任；但是，如果你想写一个函数用来操作内嵌数据结构，如 Map、List 和 Set，此时就需要去熟悉复杂 API（GenericUDF）。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数UDF</title>
    <link href="https://jordenbruce.com/2019/11/03/hive-udf/"/>
    <id>https://jordenbruce.com/2019/11/03/hive-udf/</id>
    <published>2019-11-03T13:47:18.000Z</published>
    <updated>2019-11-04T13:52:33.524Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 有丰富的内置函数（Built-in Functions），方便数据处理与数据分析等。即便如此，内置函数有时候还是无法满足需求，这时就需要自定义函数（User-Defined Functions , UDF）来扩展 Hive 函数库，实现用户想要的功能。<br><a id="more"></a></p><h2 id="0x00-自定义-UDF-开发"><a href="#0x00-自定义-UDF-开发" class="headerlink" title="0x00 自定义 UDF 开发"></a>0x00 自定义 UDF 开发</h2><p>编写 UDF 需要下面两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.exec.UDF</code> ；</li><li>实现 <code>evaluate</code> 函数，这个函数必须要有返回值，不能设置为 void。同时建议使用 mapreduce 编程模型中的数据类型( Text, IntWritable 等)，因为 SQL 语句会被转换为 mapreduce 任务；</li></ul><p>示例代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import java.util.Date;</span><br><span class="line">import java.text.ParseException;</span><br><span class="line">import java.text.SimpleDateFormat;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">name = &quot;calc_remain&quot;,</span><br><span class="line">value = &quot;_FUNC_(active_list, active_list_date, base_date, remain) - Calculate remain_N based on a date.&quot;,</span><br><span class="line">extended = &quot;Example:\n&quot; +</span><br><span class="line">   &quot;select calc_remain(&apos;1,0,1,0,1,1,0,1&apos;,&apos;2019-03-15&apos;,&apos;2019-03-10&apos;,3);\n&quot;)</span><br><span class="line">public class CalcRemain extends UDF &#123;</span><br><span class="line"></span><br><span class="line">private interface Status &#123;</span><br><span class="line">String UnActive = &quot;0&quot;;</span><br><span class="line">String Active = &quot;1&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public int evaluate(String strActiveList, String strListDate, String strBaseDate, int intRemain) &#123;</span><br><span class="line">if (IsEmpty(strActiveList) || IsEmpty(strListDate)) &#123;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int intOuput = 0;</span><br><span class="line"></span><br><span class="line">Date dtBase = StringToDate(strBaseDate);</span><br><span class="line">Date dtLast = StringToDate(strListDate);</span><br><span class="line"></span><br><span class="line">long lngDiffDays = (dtLast.getTime() - dtBase.getTime()) / (24*3600*1000);</span><br><span class="line">if (intRemain &lt; 0 || lngDiffDays &lt; 1 || lngDiffDays &lt; intRemain) &#123;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int intDiffDays = (int)lngDiffDays;</span><br><span class="line">int lenActiveList = strActiveList.length();</span><br><span class="line"></span><br><span class="line">String[] arrActiveList = strActiveList</span><br><span class="line">                .substring(lenActiveList-intDiffDays*2-1, lenActiveList)</span><br><span class="line">                .split(&quot;,&quot;);</span><br><span class="line"></span><br><span class="line">if (arrActiveList[0].equals(Status.UnActive)) &#123;</span><br><span class="line">return 0;</span><br><span class="line">&#125; else if (arrActiveList[intRemain].equals(Status.Active)) &#123;</span><br><span class="line">intOuput = 1;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">intOuput = 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return intOuput;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private Date StringToDate(String strTime) &#123;</span><br><span class="line">Date dtOutput = null;</span><br><span class="line">SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);</span><br><span class="line">try &#123;</span><br><span class="line">dtOutput = sdf.parse(strTime);</span><br><span class="line">&#125; catch (ParseException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">return dtOutput;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private boolean IsEmpty (String strArgument) &#123;</span><br><span class="line">if (strArgument == null || strArgument.length() &lt;= 0) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x01-自定义-UDF-部署方式"><a href="#0x01-自定义-UDF-部署方式" class="headerlink" title="0x01 自定义 UDF 部署方式"></a>0x01 自定义 UDF 部署方式</h2><p>官方提供了两种部署 UDF 的方式：</p><ul><li>临时部署（Temporary Functions）</li><li>永久部署（Permanent Functions）</li></ul><p>两者的区别在于：临时部署的方式，只会在当前 Session 下有效并可用；永久部署的方式，在部署成功后任何一个 Hive 客户端（重新启动的 Hive 客户端，已经启动的客户端需要重新加载）都可以使用。</p><p>(1) 临时部署</p><p>这个是最常见的 Hive 使用方式，通过 hive 命令来完成 UDF 的部署；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; add jar /path/to/local.jar; </span><br><span class="line">hive&gt; create temporary function calc_remain as &apos;com.data.hive.CalcRemain&apos;;</span><br></pre></td></tr></table></figure><p>建议函数名使用 <strong>下划线命名法</strong>（全部小写字母）。</p><p>(2) 永久部署</p><p>这种方式是 hive-0.13 版本以后开始支持的注册方法；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create function udf.calc_remain </span><br><span class="line">hive&gt; as &apos;com.data.hive.CalcRemain&apos; </span><br><span class="line">hive&gt; using jar &apos;hdfs:///path/to/hdfs.jar&apos;;</span><br></pre></td></tr></table></figure><p>需要注意的是：函数名称前面一定要带上数据库名称。</p><h2 id="0x02-函数相关的-HQL-语句"><a href="#0x02-函数相关的-HQL-语句" class="headerlink" title="0x02 函数相关的 HQL 语句"></a>0x02 函数相关的 HQL 语句</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">-- 查看所有函数(内置函数+自定义函数)</span><br><span class="line">show functions;</span><br><span class="line">-- 查看某个函数的使用说明</span><br><span class="line">describe function function_name;</span><br><span class="line">-- 创建临时自定义函数</span><br><span class="line">create temporary function function_name as class_name;</span><br><span class="line">-- 删除临时自定义函数</span><br><span class="line">drop temporary function [if exists] function_name;</span><br><span class="line">-- 创建永久自定义函数</span><br><span class="line">create function [db_name.]function_name as class_name</span><br><span class="line">  [using jar|file|archive &apos;file_uri&apos; [, jar|file|archive &apos;file_uri&apos;] ];</span><br><span class="line">-- 删除永久自定义函数</span><br><span class="line">drop function [if exists] function_name;</span><br><span class="line">-- 重载函数</span><br><span class="line">reload function;</span><br></pre></td></tr></table></figure><h2 id="0x03-扩展与延伸"><a href="#0x03-扩展与延伸" class="headerlink" title="0x03 扩展与延伸"></a>0x03 扩展与延伸</h2><p>Hive 自定义函数的扩展，不仅有 UDF（User-Defined Functions），还有 UDAF（User-Defined Aggregate Functions）和 UDTF（User-Defined Table-Generating Functions），详情请参考官方说明。</p><p>另外，如果数据处理在函数级别不能解决，还可以借助 <code>TRANSFORM</code> 自定义 Map 和 Reduce 函数，或者编写 MapReduce 程序。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins" target="_blank" rel="noopener">Creating Custom UDFs</a><br><a href="https://blog.csdn.net/qq_32653877/article/details/87182898" target="_blank" rel="noopener">Java编写Hive的UDF</a><br><a href="http://chaozi204.github.io/blog/hive-udf-deploy/" target="_blank" rel="noopener">Hive UDF 部署方式小结</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy" target="_blank" rel="noopener">Writing GenericUDAFs</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform" target="_blank" rel="noopener">Hive’s Transform functionality</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 有丰富的内置函数（Built-in Functions），方便数据处理与数据分析等。即便如此，内置函数有时候还是无法满足需求，这时就需要自定义函数（User-Defined Functions , UDF）来扩展 Hive 函数库，实现用户想要的功能。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Java通过JDBC连接Hive</title>
    <link href="https://jordenbruce.com/2019/11/03/hive-jdbc-client/"/>
    <id>https://jordenbruce.com/2019/11/03/hive-jdbc-client/</id>
    <published>2019-11-03T12:56:18.000Z</published>
    <updated>2019-11-03T14:21:36.883Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 是大数据技术簇中进行数据仓库应用的基础组件，是其它类似数据仓库应用的对比基准。基础的数据操作可以通过脚本方式以 cli 进行处理；若需要开发应用程序，则需要使用 hive-jdbc 驱动进行连接。<br><a id="more"></a></p><h2 id="0x00-添加-hive-site-xml-配置项"><a href="#0x00-添加-hive-site-xml-配置项" class="headerlink" title="0x00 添加 hive-site.xml 配置项"></a>0x00 添加 hive-site.xml 配置项</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">&lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="0x01-启动-hiveserver2-服务"><a href="#0x01-启动-hiveserver2-服务" class="headerlink" title="0x01 启动 hiveserver2 服务"></a>0x01 启动 hiveserver2 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$&#123;HIVE_HOME&#125;/bin/hive --service metastore &amp;</span><br><span class="line">$&#123;HIVE_HOME&#125;/bin/hive --service hiveserver2 &amp;</span><br><span class="line"></span><br><span class="line">netstat -lnt | grep 10000</span><br></pre></td></tr></table></figure><h2 id="0x02-beeline-方式连接"><a href="#0x02-beeline-方式连接" class="headerlink" title="0x02 beeline 方式连接"></a>0x02 beeline 方式连接</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">shell&gt;&gt; $&#123;HIVE_HOME&#125;/bin/beeline</span><br><span class="line">Beeline version 1.2.2 by Apache Hive</span><br><span class="line">beeline&gt; !connect jdbc:hive2://localhost:10000/default</span><br><span class="line">Connecting to jdbc:hive2://localhost:10000/default</span><br><span class="line">Enter username for jdbc:hive2://localhost:10000/default: hadoop</span><br><span class="line">Enter password for jdbc:hive2://localhost:10000/default: ******</span><br><span class="line">Connected to: Apache Hive (version 1.2.2)</span><br><span class="line">Driver: Hive JDBC (version 1.2.2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">+-------------------+--+</span><br><span class="line">|     tab_name      |</span><br><span class="line">+-------------------+--+</span><br><span class="line">| managed_user      |</span><br><span class="line">| ods_user          |</span><br><span class="line">| partitioned_user  |</span><br><span class="line">+-------------------+--+</span><br><span class="line">3 rows selected (0.358 seconds)</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt; !quit</span><br><span class="line">Closing: 0: jdbc:hive2://localhost:10000/default</span><br></pre></td></tr></table></figure><h2 id="0x03-jdbc-方式连接"><a href="#0x03-jdbc-方式连接" class="headerlink" title="0x03 jdbc 方式连接"></a>0x03 jdbc 方式连接</h2><p>建议使用 IntelliJ IDEA 开发工具，创建 Maven 工程，Java 代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import java.sql.SQLException;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line"></span><br><span class="line">public class HiveJDBC &#123;</span><br><span class="line"></span><br><span class="line">    private static String DriverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;;</span><br><span class="line"></span><br><span class="line">    private static String ServerUrl = &quot;jdbc:hive2://localhost:10000/default&quot;;</span><br><span class="line">    private static String ServerUser = &quot;hadoop&quot;;</span><br><span class="line">    private static String ServerPwd = &quot;hadoop&quot;;</span><br><span class="line"></span><br><span class="line">    public static void main(String args[]) throws SQLException &#123;</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            Class.forName(DriverName);</span><br><span class="line">        &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            System.exit(1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Connection hiveConn = DriverManager.getConnection(ServerUrl, ServerUser, ServerPwd);</span><br><span class="line">        Statement hiveStmt = hiveConn.createStatement();</span><br><span class="line"></span><br><span class="line">        ResultSet hqlOutput = hiveStmt.executeQuery(&quot;show tables&quot;);</span><br><span class="line">        while (hqlOutput.next()) &#123;</span><br><span class="line">            System.out.println(hqlOutput.getString(1));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        hqlOutput.close();</span><br><span class="line">        hiveStmt.close();</span><br><span class="line">        hiveConn.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>添加 Maven 依赖，pom.xml 依赖配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">&lt;project.build.sourceEncoding&gt;UTF8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">&lt;hadoop.version&gt;2.7.7&lt;/hadoop.version&gt;</span><br><span class="line">&lt;hive.version&gt;1.2.2&lt;/hive.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">&lt;id&gt;Apache Hadoop&lt;/id&gt;</span><br><span class="line">&lt;name&gt;Apache Hadoop&lt;/name&gt;</span><br><span class="line">&lt;url&gt;https://repo1.maven.org/maven2/&lt;/url&gt;</span><br><span class="line">&lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-metastore&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p>大功告成！</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/HiveClient" target="_blank" rel="noopener">Hive Client</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients" target="_blank" rel="noopener">HiveServer2 Clients</a><br><a href="https://www.cnblogs.com/gridmix/p/5102725.html" target="_blank" rel="noopener">通过JDBC连接hive</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 是大数据技术簇中进行数据仓库应用的基础组件，是其它类似数据仓库应用的对比基准。基础的数据操作可以通过脚本方式以 cli 进行处理；若需要开发应用程序，则需要使用 hive-jdbc 驱动进行连接。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive体系结构</title>
    <link href="https://jordenbruce.com/2019/11/02/hive-architecture/"/>
    <id>https://jordenbruce.com/2019/11/02/hive-architecture/</id>
    <published>2019-11-02T03:46:32.000Z</published>
    <updated>2019-11-03T12:59:19.852Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 最初是由 FaceBook 公司开发的一个基于 Hadoop 框架并且开源的一个数据仓库工具，后贡献给了 Apache 基金会由 Apache 来进行维护和更新。Hive 可以将结构化的文件映射为一张数据表，但并不提供查询功能，而是将 HiveQL 转化为 MapReduce 任务进行运行。同时，Hive 本身不存储数据，只是存储数据的路径或者操作信息，真的数据是存储在可靠的文件系统当中（如HDFS、Derby等）。<br><a id="more"></a></p><h2 id="0x00-Hive-架构"><a href="#0x00-Hive-架构" class="headerlink" title="0x00 Hive 架构"></a>0x00 Hive 架构</h2><p><img src="https://i.loli.net/2019/11/02/VUOGX45StkYbwMF.jpg" alt="Hive 架构"></p><p>Hive 架构主要包括如下组件：CLI（command line interface）、JDBC/ODBC、Web GUI（Hive WEB Interface）、Thrift Server、Metastore 和 Driver(Complier、Optimizer 和 Executor)，这些组件可以分为两大类：服务端组件和客户端组件。</p><p>(1) 客户端组件</p><ul><li>CLI：最常用的用户接口，cli 启动的时候，会同时启动一个 Hive 副本；</li><li>JDBC/ODBC：Client 是 Hive 的客户端，用户连接至 Hive Server。在启动 Client 模式的时候，需要指出 Hive Server 所在节点，并且在该节点启动 Hive Server ；</li><li>HWI：通过浏览器访问 Hive；</li></ul><p>(2) 服务端组件</p><ul><li>Thrift Server：Thrift 是 facebook 开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，hive 集成了Thrift Server 服务，能让不同的编程语言调用 hive 的接口；</li><li>Metastore：元数据服务组件，这个组件用于存储 hive 的元数据，包括表名、表所属的数据库、表的拥有者、列/分区字段、表的类型、表的数据所在目录等内容。hive 的元数据存储在关系数据库里，支持 derby、mysql 两种关系型数据库；</li><li>Driver：包括 Complier、Optimizer 和 Executor，它们的作用是将我们写的 HiveQL 语句进行解析、编译、优化，生成执行计划，然后调用底层的 MapReduce 计算框架；</li></ul><h2 id="0x01-Hive-工作原理"><a href="#0x01-Hive-工作原理" class="headerlink" title="0x01 Hive 工作原理"></a>0x01 Hive 工作原理</h2><p><img src="https://i.loli.net/2019/11/02/GIvJ2RqS6d3Hf4i.png" alt="Hive 查询的工作流程"></p><ol><li>UI 提交查询（HiveQL）给 Driver；</li><li>Driver 为查询创建会话句柄，并将查询发给 Complier 生成执行计划；</li><li>Complier 从 Metastore 获取必要的元数据信息；</li><li>由 Complier 生成的执行计划是 作业的DAG，包括三种作业：MapReduce 作业、元数据操作、HDFS 操作；</li><li>Execution Engine 将不同作业提交给对应的组件；</li><li>Execution Engine 将匹配上的查询结果经过 Driver 发送给 UI。</li></ol><p>细心的读者会发现，上述执行过程还是 JobTracker 版本，从 Hadoop-2.x 开始升级为 YARN，MapReduce 作为一种应用程序运行在 YARN 框架下。</p><h2 id="0x02-Hive-特点"><a href="#0x02-Hive-特点" class="headerlink" title="0x02 Hive 特点"></a>0x02 Hive 特点</h2><p>(1) 优点</p><ul><li>简单容易上手：提供了类 SQL 查询语言 HiveQL</li><li>可扩展：为超大数据集设计了计算/扩展能力（MR 作为计算引擎，HDFS 作为存储系统）</li><li>提供统一的元数据管理</li><li>延展性：Hive 支持用户自定义函数，还允许 TRANSFORM 嵌入不同语言的 map-reduce 脚本</li><li>容错：良好的容错性，节点出现问题 SQL 仍可完成执行</li></ul><p>(2) 缺点</p><ul><li>HiveQL 表达能力有限（迭代式算法，数据挖掘）</li><li>hive 效率比较低（mapreduce，调优）</li><li>hive 可控性差</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Design" target="_blank" rel="noopener">Hive Architecture Overview</a><br><a href="https://www.cnblogs.com/zimo-jing/p/9028949.html" target="_blank" rel="noopener">深入学习Hive应用场景及架构原理</a><br><a href="https://blog.csdn.net/m0_37914799/article/details/85109143" target="_blank" rel="noopener">Hive的架构及元数据三种存储模式</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 最初是由 FaceBook 公司开发的一个基于 Hadoop 框架并且开源的一个数据仓库工具，后贡献给了 Apache 基金会由 Apache 来进行维护和更新。Hive 可以将结构化的文件映射为一张数据表，但并不提供查询功能，而是将 HiveQL 转化为 MapReduce 任务进行运行。同时，Hive 本身不存储数据，只是存储数据的路径或者操作信息，真的数据是存储在可靠的文件系统当中（如HDFS、Derby等）。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HDFS架构原理</title>
    <link href="https://jordenbruce.com/2019/10/19/hadoop-hdfs/"/>
    <id>https://jordenbruce.com/2019/10/19/hadoop-hdfs/</id>
    <published>2019-10-19T09:29:24.000Z</published>
    <updated>2019-10-22T12:36:09.282Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS 被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统，是 Hadoop 应用程序使用的主要分布式存储。HDFS 与现有的分布式文件系统有许多相似之处；但是，与其他分布式文件系统的区别也是很明显的。HDFS 具有高度的容错能力，旨在部署在低成本硬件上；HDFS 提供对应用程序数据的高吞吐量访问，并且适用于具有大数据集的应用程序；HDFS 放宽了一些POSIX要求，以实现对文件系统数据的流式访问。<br><a id="more"></a></p><h2 id="0x00-HDFS-架构"><a href="#0x00-HDFS-架构" class="headerlink" title="0x00 HDFS 架构"></a>0x00 HDFS 架构</h2><p>HDFS (Hadoop Distributed File System) 采用的是 Master/Slave 架构，一个 HDFS 集群包含一个单独的 NameNode 和多个 DataNode 节点，如下图所示：</p><p><img src="https://i.loli.net/2019/10/21/4JgoDXhFeVaukj9.png" alt="HDFS 架构"></p><p>主要由以下几个组件构成：</p><ul><li>NameNode</li><li>Secondary NameNode</li><li>DataNode</li><li>Client</li></ul><p>需要说明的是：类似于磁盘 Block size 概念，Hadoop 集群中文件的存储都是以块的形式存储在 HDFS 中。通过设置 hdfs-site.xml 文件中 dfs.blocksize 参数来配置块大小，默认是128M。</p><h2 id="0x01-文件写入过程"><a href="#0x01-文件写入过程" class="headerlink" title="0x01 文件写入过程"></a>0x01 文件写入过程</h2><p><img src="https://i.loli.net/2019/10/21/5EOiCB2JVQYNswd.png" alt="HDFS 文件写入过程"></p><p>Client 向 HDFS 写入文件的具体过程如下：</p><ol><li><p>Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象；</p></li><li><p>通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；</p></li><li><p>通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；</p></li><li><p>DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；</p></li><li><p>DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；</p></li><li><p>完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，完成文件写入；</p></li><li><p>调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。</p></li></ol><h2 id="0x02-文件读取过程"><a href="#0x02-文件读取过程" class="headerlink" title="0x02 文件读取过程"></a>0x02 文件读取过程</h2><p><img src="https://i.loli.net/2019/10/21/oYipLz9MEq4TyS7.png" alt="HDFS 文件读取过程"></p><p>Client 向 HDFS 读取文件的具体过程如下：</p><ol><li><p>Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；</p></li><li><p>NameNode 返回存储的每个块的 DataNode 列表；</p></li><li><p>Client 将连接到列表中最近的 DataNode；</p></li><li><p>Client 开始从 DataNode 并行读取数据；</p></li><li><p>一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。</p></li></ol><p>在处理 Client 的读取请求时，HDFS 会利用机架感知选举最接近 Client 位置的副本，这将会减少读取延迟和带宽消耗。</p><h2 id="0x03-HDFS-遗留问题"><a href="#0x03-HDFS-遗留问题" class="headerlink" title="0x03 HDFS 遗留问题"></a>0x03 HDFS 遗留问题</h2><p>在前面的介绍中，关于 HDFS 1.0 架构的设计缺陷主要有以下两点：</p><ul><li><p>NameNode 的单点问题，如果 NameNode 挂掉了，数据读写都会受到影响，HDFS 整体将变得不可用，这在生产环境中是不可接受的；</p></li><li><p>水平扩展问题，随着集群规模的扩大，1.0 时集群规模达到3000时，会导致整个集群管理的文件数目达到上限（因为 NameNode 要管理整个集群 block 元信息、数据目录信息等）。</p></li></ul><p>为了解决上面的两个问题，Hadoop2.0 提供一套统一的解决方案：</p><ul><li><p>HA（High Availability 高可用方案）：这个是为了解决 NameNode 单点问题；</p></li><li><p>NameNode Federation：是用来解决 HDFS 集群的线性扩展能力。</p></li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html" target="_blank" rel="noopener">HDFS Users Guide</a><br><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="noopener">HDFS Architecture</a><br><a href="https://www.cnblogs.com/sheng-sjk/p/10624958.html" target="_blank" rel="noopener">Hdfs block数据块大小的设置规则</a><br><a href="https://www.jianshu.com/p/25bdb4b8051e" target="_blank" rel="noopener">HDFS基础架构与各个组件的功能</a><br><a href="https://matt33.com/2018/07/15/hdfs-architecture-learn/" target="_blank" rel="noopener">HDFS 架构学习总结</a><br><a href="https://www.open-open.com/lib/view/open1376228205209.html" target="_blank" rel="noopener">HDFS 原理、架构与特性介绍</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HDFS 被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统，是 Hadoop 应用程序使用的主要分布式存储。HDFS 与现有的分布式文件系统有许多相似之处；但是，与其他分布式文件系统的区别也是很明显的。HDFS 具有高度的容错能力，旨在部署在低成本硬件上；HDFS 提供对应用程序数据的高吞吐量访问，并且适用于具有大数据集的应用程序；HDFS 放宽了一些POSIX要求，以实现对文件系统数据的流式访问。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="hdfs" scheme="https://jordenbruce.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>YARN技术原理</title>
    <link href="https://jordenbruce.com/2019/10/16/hadoop-yarn/"/>
    <id>https://jordenbruce.com/2019/10/16/hadoop-yarn/</id>
    <published>2019-10-15T23:58:29.000Z</published>
    <updated>2019-10-22T12:36:15.572Z</updated>
    
    <content type="html"><![CDATA[<p>YARN (Yet Another Resource Negotiator) 是从 hadoop-0.23 开始引入的新架构，他的基本设计思想是将 JobTracker 的两个主要功能，资源管理和作业控制（包括调度/监视等），拆分为单独的守护进程，分别是一个全局的 ResourceManager（RM）和每个应用程序的 ApplicationMaster（AM）。应用程序可以是单个作业，也可以是作业的DAG。<br><a id="more"></a></p><h2 id="0x00-YARN-架构"><a href="#0x00-YARN-架构" class="headerlink" title="0x00 YARN 架构"></a>0x00 YARN 架构</h2><p>YARN 的全称是 Yet Another Resource Negotiator，YARN 整体上是 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave，如下图所示：</p><p><img src="https://i.loli.net/2019/02/20/5c6d021102002.png" alt="YARN架构"></p><p>主要由以下几个组件构成：</p><ul><li>ResourceManager (RM) : Scheduler, Applications Manager (ASM)</li><li>NodeManager (NM)</li><li>ApplicationMaster (AM)</li><li>Container</li></ul><p>需要说明的是：Scheduler 是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 FIFO Scheduler，Fair Scheduler 和 Capacity Schedule 等。</p><h2 id="0x01-YARN-工作流程"><a href="#0x01-YARN-工作流程" class="headerlink" title="0x01 YARN 工作流程"></a>0x01 YARN 工作流程</h2><p>如下图所示用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：</p><ul><li>启动AM ，如下步骤1~3；</li><li>由AM创建应用程序为它申请资源并监控它的整个运行过程，直到运行完成，如下步骤4~7。</li></ul><p><img src="https://i.loli.net/2019/02/20/5c6d03e204bfc.png" alt="YARN工作流程"></p><ol><li><p>用户向YARN中提交应用程序，其中包括AM程序、启动AM的命令、命令参数、用户程序等；事实上，需要准确描述运行ApplicationMaster的unix进程的所有信息。提交工作通常由YarnClient来完成。</p></li><li><p>RM为该应用程序分配第一个Container，并与对应的NM通信，要求它在这个Container中启动AM；</p></li><li><p>AM首先向RM注册，这样用户可以直接通过RM査看应用程序的运行状态，运行状态通过 AMRMClientAsync.CallbackHandler的getProgress() 方法来传递给RM。 然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4〜7；</p></li><li><p>AM采用轮询的方式通过RPC协议向RM申请和领取资源；资源的协调通过 AMRMClientAsync异步完成,相应的处理方法封装在AMRMClientAsync.CallbackHandler中。</p></li><li><p>—旦AM申请到资源后，便与对应的NM通信，要求它启动任务；通常需要指定一个ContainerLaunchContext，提供Container启动时需要的信息。</p></li><li><p>NM为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</p></li><li><p>各个任务通过某个RPC协议向AM汇报自己的状态和进度，以让AM随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；ApplicationMaster与NM的通信通过NMClientAsync object来完成，容器的所有事件通过NMClientAsync.CallbackHandler来处理。例如启动、状态更新、停止等。</p></li><li><p>应用程序运行完成后，AM向RM注销并关闭自己。</p></li></ol><h2 id="0x02-YARN-资源管理"><a href="#0x02-YARN-资源管理" class="headerlink" title="0x02 YARN 资源管理"></a>0x02 YARN 资源管理</h2><p>当前 YARN 支持内存和CPU两种资源类型的管理和分配。主要是 scheduler 和 NodeManager 的配置选项设置，参数如下：</p><table><thead><tr><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>yarn.resourcemanager.scheduler.class</td><td>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>32</td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>8192</td></tr><tr><td>yarn.nodemanager.vmem-pmem-ratio</td><td>2.1</td></tr><tr><td>yarn.nodemanager.pmem-check-enabled</td><td>true</td></tr><tr><td>yarn.nodemanager.vmem-check-enabled</td><td>true</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>8</td></tr><tr><td>yarn.nodemanager.resource.percentage-physical-cpu-limit</td><td>100</td></tr></tbody></table><p>另外，YARN 使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">YARN Architecture</a><br><a href="https://matt33.com/2018/09/01/yarn-architecture-learn/" target="_blank" rel="noopener">YARN 架构学习总结</a><br><a href="https://cshihong.github.io/2018/05/11/Yarn技术原理/" target="_blank" rel="noopener">Yarn技术原理</a><br><a href="https://blog.csdn.net/bingduanlbd/article/details/51880019" target="_blank" rel="noopener">理解Hadoop YARN架构</a><br><a href="http://www.cnblogs.com/wcwen1990/p/6737985.html" target="_blank" rel="noopener">YARN架构设计详解</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YARN (Yet Another Resource Negotiator) 是从 hadoop-0.23 开始引入的新架构，他的基本设计思想是将 JobTracker 的两个主要功能，资源管理和作业控制（包括调度/监视等），拆分为单独的守护进程，分别是一个全局的 ResourceManager（RM）和每个应用程序的 ApplicationMaster（AM）。应用程序可以是单个作业，也可以是作业的DAG。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="yarn" scheme="https://jordenbruce.com/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce执行过程详解</title>
    <link href="https://jordenbruce.com/2019/10/13/hadoop-mapreduce/"/>
    <id>https://jordenbruce.com/2019/10/13/hadoop-mapreduce/</id>
    <published>2019-10-13T14:57:51.000Z</published>
    <updated>2019-10-13T12:34:06.589Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。<br><a id="more"></a></p><h2 id="0x00-分析MapReduce执行过程"><a href="#0x00-分析MapReduce执行过程" class="headerlink" title="0x00 分析MapReduce执行过程"></a>0x00 分析MapReduce执行过程</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。整个流程如图：</p><p><img src="https://i.loli.net/2019/10/12/jotATrBqyIzV5cY.png" alt="MapReduce执行过程"></p><h2 id="0x01-Mapper任务的执行过程详解"><a href="#0x01-Mapper任务的执行过程详解" class="headerlink" title="0x01 Mapper任务的执行过程详解"></a>0x01 Mapper任务的执行过程详解</h2><p><strong>每个Mapper任务是一个java进程</strong>，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下几个阶段，如图所示：</p><p><img src="https://i.loli.net/2019/10/12/6jbSxLJQNqls4FP.png" alt="Mapper执行过程"></p><p>在上图中，把Mapper任务的运行过程分为六个阶段。</p><ol><li><p>第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值64MB，输入文件有两个，一个是32MB，一个是72MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p></li><li><p>第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一行的起始位置(单位是字节)，“值”是本行的文本内容。</p></li><li><p>第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p></li><li><p>第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer任务运行的数量。默认只有一个Reducer任务。</p></li><li><p>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入第六阶段；如果没有，直接输出到本地的linux文件中。</p></li><li><p>第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。</p></li></ol><h2 id="0x02-Reducer任务的执行过程详解"><a href="#0x02-Reducer任务的执行过程详解" class="headerlink" title="0x02 Reducer任务的执行过程详解"></a>0x02 Reducer任务的执行过程详解</h2><p><strong>每个Reducer任务是一个java进程</strong>。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为如下图所示的几个阶段。</p><p><img src="https://i.loli.net/2019/10/12/HXhC2wvUQLKdV4e.png" alt="Reducer执行过程"></p><ol><li><p>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。</p></li><li><p>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。</p></li><li><p>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS文件中。</p></li></ol><p>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p><h2 id="0x03-键值对的编号"><a href="#0x03-键值对的编号" class="headerlink" title="0x03 键值对的编号"></a>0x03 键值对的编号</h2><p>在对Mapper任务、Reducer任务的分析过程中，会看到很多阶段都出现了键值对，读者容易混淆，所以这里对键值对进行编号，方便大家理解键值对的变化情况，如下图所示。</p><p><img src="https://i.loli.net/2019/10/12/q6Y9Sl5ofQCW7LJ.png" alt="键值对"></p><p>在上图中，对于Mapper任务输入的键值对，定义为key1和value1。在map方法中处理后，输出的键值对，定义为key2和value2。reduce方法接收key2和value2，处理后，输出key3和value3。在下文讨论键值对时，可能把key1和value1简写为&lt;k1,v1&gt;，key2和value2简写为&lt;k2,v2&gt;，key3和value3简写为&lt;k3,v3&gt;。</p><h2 id="转载说明"><a href="#转载说明" class="headerlink" title="转载说明"></a>转载说明</h2><p><a href="https://my.oschina.net/itblog/blog/275294" target="_blank" rel="noopener">Hadoop MapReduce执行过程详解（带hadoop例子）</a><br><a href="http://matt33.com/2016/03/02/hadoop-shuffle/" target="_blank" rel="noopener">MapReduce之Shuffle过程详述</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="mapreduce" scheme="https://jordenbruce.com/tags/mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库的初级手册</title>
    <link href="https://jordenbruce.com/2019/10/12/junior-manual/"/>
    <id>https://jordenbruce.com/2019/10/12/junior-manual/</id>
    <published>2019-10-12T12:54:02.000Z</published>
    <updated>2019-10-25T13:00:41.433Z</updated>
    
    <content type="html"><![CDATA[<p>这套初级手册是基于 Hadoop+Hive+Sqoop+Airflow 编写的，主要目的是对数据仓库有个宏观的认识，包括数据采集、ETL流程、任务调度和数据模型等。当中的每一个环节都有着丰富的内容，会在后期的中级手册与高级手册进行讨论，欢迎大家来围观。<br><a id="more"></a></p><h2 id="0x00-环境搭建"><a href="#0x00-环境搭建" class="headerlink" title="0x00 环境搭建"></a>0x00 环境搭建</h2><ul><li><a href="https://jordenbruce.com/2019/09/15/hadoop-release/">Hadoop发行版的选取</a></li><li><a href="https://jordenbruce.com/2019/09/15/hadoop-build/">编译Hadoop源码包</a></li><li><a href="https://jordenbruce.com/2019/09/15/hadoop-install/">手动搭建Hadoop分布式运行环境</a></li><li><a href="https://jordenbruce.com/2019/09/15/hive-install/">手动搭建Hive开发环境</a></li><li><a href="https://jordenbruce.com/2019/09/15/sqoop-install/">手动搭建Sqoop开发环境</a></li></ul><h2 id="0x01-工具使用"><a href="#0x01-工具使用" class="headerlink" title="0x01 工具使用"></a>0x01 工具使用</h2><ul><li><a href="https://jordenbruce.com/2019/09/20/hadoop-cli/">hadoop命令行的常用操作</a></li><li><a href="https://jordenbruce.com/2019/09/22/hdfs-cli/">hdfs命令行的常用操作</a></li><li><a href="https://jordenbruce.com/2019/09/22/yarn-cli/">yarn命令行的常用操作</a></li><li><a href="https://jordenbruce.com/2019/09/23/hql-table/">HiveQL的Table常用操作</a></li><li><a href="https://jordenbruce.com/2019/09/24/hql-dml/">HiveQL的导入与导出</a></li><li><a href="https://jordenbruce.com/2019/09/26/hql-select/">HiveQL的Select语句</a></li><li><a href="https://www.cnblogs.com/liupengpengg/p/7908274.html" target="_blank" rel="noopener">HiveQL的Join语句</a></li><li><a href="https://jordenbruce.com/2019/09/30/hql-function/">HiveQL的函数概览</a></li><li><a href="https://jordenbruce.com/2019/10/05/hql-function-math/">HiveQL的数学函数</a></li><li><a href="https://jordenbruce.com/2019/10/05/hql-function-date/">HiveQL的日期函数</a></li><li><a href="https://jordenbruce.com/2019/10/05/hql-function-string/">HiveQL的字符串函数</a></li><li><a href="https://jordenbruce.com/2019/10/08/sqoop-cli/">Sqoop命令行的导入与导出</a></li></ul><h2 id="0x02-数据模型"><a href="#0x02-数据模型" class="headerlink" title="0x02 数据模型"></a>0x02 数据模型</h2><ul><li><a href="https://jordenbruce.com/2019/10/09/growth-model/">简化版的增长模型</a></li></ul><h2 id="0x03-任务流调度"><a href="#0x03-任务流调度" class="headerlink" title="0x03 任务流调度"></a>0x03 任务流调度</h2><ul><li><a href="https://jordenbruce.com/2019/10/10/workflow/">常见的任务调度系统</a></li></ul><h2 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h2><p>除了以上提到的主题之外，还有数据管理（元数据，计算管理，存储和成本管理，数据质量），数据应用（接口服务，报表服务，应用服务），数据挖掘等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这套初级手册是基于 Hadoop+Hive+Sqoop+Airflow 编写的，主要目的是对数据仓库有个宏观的认识，包括数据采集、ETL流程、任务调度和数据模型等。当中的每一个环节都有着丰富的内容，会在后期的中级手册与高级手册进行讨论，欢迎大家来围观。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="modeling" scheme="https://jordenbruce.com/tags/modeling/"/>
    
      <category term="hadoop" scheme="https://jordenbruce.com/tags/hadoop/"/>
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
      <category term="sqoop" scheme="https://jordenbruce.com/tags/sqoop/"/>
    
      <category term="workflow" scheme="https://jordenbruce.com/tags/workflow/"/>
    
  </entry>
  
  <entry>
    <title>常见的任务调度系统</title>
    <link href="https://jordenbruce.com/2019/10/10/workflow/"/>
    <id>https://jordenbruce.com/2019/10/10/workflow/</id>
    <published>2019-10-10T13:32:00.000Z</published>
    <updated>2019-10-25T12:51:21.126Z</updated>
    
    <content type="html"><![CDATA[<p>离线数据仓库的最大特点是离线，也就是说数据会经过ETL处理之后才显示，最常见的形式是T+1，比如隔天出数。而提前设计的ETL任务一般需要在凌晨去运行，数据工程师不可能每天凌晨起来执行任务，这时候就需要有任务调度系统了。它不仅要能够定时地启动任务，还要能够管理任务之间复杂的依赖关系，使得成千上万的任务有条不紊地运行起来，保证数据能够在预定时间（比如上班前）能够出来，方便其他同事进行查阅。<br><a id="more"></a></p><p>由于任务调度系统太重要了，很多有开发能力的公司都选择自主研发。尽管需要一些成本，但是为了稳定性和个性化需求也是值得的。当然，对于一些初创型的互联网公司，市场上也有很多开源的任务调度系统，它们经过反复迭代，稳定性有了很大提升，甚至达到了企业级标准。接下来将介绍几款开源的任务调度系统，它们都分别在各大互联网公司有着成功案例，请放心使用。</p><h2 id="0x00-Luigi"><a href="#0x00-Luigi" class="headerlink" title="0x00 Luigi"></a>0x00 Luigi</h2><p>Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.</p><p>github: <a href="https://github.com/spotify/luigi" target="_blank" rel="noopener">https://github.com/spotify/luigi</a><br>documentation: <a href="https://luigi.readthedocs.io/en/stable/" target="_blank" rel="noopener">https://luigi.readthedocs.io/en/stable/</a></p><h2 id="0x01-Azkaban"><a href="#0x01-Azkaban" class="headerlink" title="0x01 Azkaban"></a>0x01 Azkaban</h2><p>Azkaban is a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.</p><p>github: <a href="https://github.com/azkaban/azkaban" target="_blank" rel="noopener">https://github.com/azkaban/azkaban</a><br>documentation: <a href="https://azkaban.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://azkaban.readthedocs.io/en/latest/</a></p><h2 id="0x02-Apache-Airflow"><a href="#0x02-Apache-Airflow" class="headerlink" title="0x02 Apache Airflow"></a>0x02 Apache Airflow</h2><p>Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.</p><p>When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.</p><p>Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.</p><p>github: <a href="https://github.com/apache/airflow" target="_blank" rel="noopener">https://github.com/apache/airflow</a><br>documentation: <a href="https://airflow.apache.org/" target="_blank" rel="noopener">https://airflow.apache.org/</a></p><h2 id="0x03-Dolphin-Scheduler"><a href="#0x03-Dolphin-Scheduler" class="headerlink" title="0x03 Dolphin Scheduler"></a>0x03 Dolphin Scheduler</h2><p>Dolphin Scheduler is a distributed and easy-to-expand visual DAG workflow scheduling system, dedicated to solving the complex dependencies in data processing, making the scheduling system out of the box for data processing. </p><p>github: <a href="https://github.com/apache/incubator-dolphinscheduler" target="_blank" rel="noopener">https://github.com/apache/incubator-dolphinscheduler</a><br>documentation: <a href="https://dolphinscheduler.apache.org/en-us/docs/user_doc/quick-start.html" target="_blank" rel="noopener">https://dolphinscheduler.apache.org/en-us/docs/user_doc/quick-start.html</a></p><h2 id="0x04-延伸"><a href="#0x04-延伸" class="headerlink" title="0x04 延伸"></a>0x04 延伸</h2><p>上述提到的开源任务调度系统，尽管功能很完善，但是易用性有待提高，更不能满足企业的个性化需求，建议进行二次开发。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://github.com/common-workflow-language/common-workflow-language/wiki/Existing-Workflow-systems" target="_blank" rel="noopener">Existing Workflow systems</a><br><a href="https://www.oschina.net/project/tag/327/task-schedule" target="_blank" rel="noopener">Job/Task Schedule</a><br><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;离线数据仓库的最大特点是离线，也就是说数据会经过ETL处理之后才显示，最常见的形式是T+1，比如隔天出数。而提前设计的ETL任务一般需要在凌晨去运行，数据工程师不可能每天凌晨起来执行任务，这时候就需要有任务调度系统了。它不仅要能够定时地启动任务，还要能够管理任务之间复杂的依赖关系，使得成千上万的任务有条不紊地运行起来，保证数据能够在预定时间（比如上班前）能够出来，方便其他同事进行查阅。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="workflow" scheme="https://jordenbruce.com/tags/workflow/"/>
    
  </entry>
  
  <entry>
    <title>简化版的增长模型</title>
    <link href="https://jordenbruce.com/2019/10/09/growth-model/"/>
    <id>https://jordenbruce.com/2019/10/09/growth-model/</id>
    <published>2019-10-09T13:00:04.000Z</published>
    <updated>2019-10-25T12:49:07.075Z</updated>
    
    <content type="html"><![CDATA[<p>在互联网环境下，存量市场的竞争越来越激烈，许多公司都将用户提到了战略高度。借助用户生命周期管理与用户画像等，实现对用户的精细化运营。而增长模型是支撑这些应用的基础数据模型，包括日活、新增与留存。<br><a id="more"></a></p><h2 id="0x00-表模型概览"><a href="#0x00-表模型概览" class="headerlink" title="0x00 表模型概览"></a>0x00 表模型概览</h2><p>这套增长模型是基于公司客户端日志来设计的，ETL处理流程如下：</p><p><img src="https://i.loli.net/2019/10/09/iyDBbVJae69HNsY.png" alt="增长模型ETL流程"></p><p>涉及的表有5张，请参考表说明：</p><table><thead><tr><th>表名</th><th>说明</th></tr></thead><tbody><tr><td>ods.com_client_log_di</td><td>Kafka上报的客户端日志</td></tr><tr><td>cdm.dwd_bhv_app_startup_di</td><td>APP启动信息表</td></tr><tr><td>cdm.dws_bhv_device_actuser_di</td><td>用户活跃信息表</td></tr><tr><td>cdm.dim_bhv_device_ds</td><td>用户全量信息表 (新增)</td></tr><tr><td>ads.dm_bhv_device_remain_di</td><td>用户留存信息表</td></tr></tbody></table><p>接下来会逐个介绍每个表的ETL逻辑。</p><h2 id="0x01-客户端日志"><a href="#0x01-客户端日志" class="headerlink" title="0x01 客户端日志"></a>0x01 客户端日志</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists ods.com_client_log_di (</span><br><span class="line">     log_id         bigint  comment &apos;全局唯一日志ID&apos;</span><br><span class="line">    ,log_timestamp  bigint  comment &apos;上报日志时间戳&apos;</span><br><span class="line">    ,device         string  comment &apos;唯一设备ID&apos;</span><br><span class="line">    ,os             string  comment &apos;操作系统&apos;</span><br><span class="line">    ,brand          string  comment &apos;品牌&apos;</span><br><span class="line">    ,model          string  comment &apos;机型&apos;</span><br><span class="line">    ,manufacturer   string  comment &apos;制造商&apos;</span><br><span class="line">    ,ip             string  comment &apos;客户端IP&apos;</span><br><span class="line">    ,network        string  comment &apos;网络类型&apos;</span><br><span class="line">    ,app_version    string  comment &apos;应用版本&apos;</span><br><span class="line">    ,package_name   string  comment &apos;包名&apos;</span><br><span class="line">    ,sdk_version    string  comment &apos;SDK版本&apos;</span><br><span class="line">    ,event          string  comment &apos;事件&apos;</span><br><span class="line">    ,page           string  comment &apos;页面&apos;</span><br><span class="line">    ,extend         map&lt;string,string&gt;  comment &apos;扩展属性集&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;客户端日志&apos;</span><br><span class="line">partitioned by (</span><br><span class="line">    dt string comment &apos;日期分区&apos;</span><br><span class="line">)</span><br><span class="line">stored as parquet</span><br><span class="line">tblproperties(&apos;parquet.compression&apos;=&apos;SNAPPY&apos;)</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>一般客户端日志的上报量非常大，采用 Kafka 入库 Hive 的方式，消费时延可以做到小时级别。</p><h2 id="0x02-APP启动信息表"><a href="#0x02-APP启动信息表" class="headerlink" title="0x02 APP启动信息表"></a>0x02 APP启动信息表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table cdm.dwd_bhv_app_startup_di partition (dt = &apos;$&#123;YESTERDAY&#125;&apos;)</span><br><span class="line">select </span><br><span class="line">     cast(log_id as string) as log_id</span><br><span class="line">    ,from_unixtime(log_timestamp div 1000, &apos;yyyy-MM-dd HH:mm:ss&apos;) as log_time </span><br><span class="line">    ,device</span><br><span class="line">    ,os</span><br><span class="line">    ,brand</span><br><span class="line">    ,model</span><br><span class="line">    ,manufacturer</span><br><span class="line">    ,ip</span><br><span class="line">    ,network</span><br><span class="line">    ,app_version</span><br><span class="line">    ,package_name</span><br><span class="line">    ,sdk_version</span><br><span class="line">    ,from_unixtime(unix_timestamp(), &apos;yyyy-MM-dd HH:mm:ss&apos;) as etl_time </span><br><span class="line">from ods.com_client_log_di</span><br><span class="line">where dt = &apos;$&#123;YESTERDAY&#125;&apos;</span><br><span class="line">    and event = &apos;app_startup&apos;</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x03-用户活跃信息表"><a href="#0x03-用户活跃信息表" class="headerlink" title="0x03 用户活跃信息表"></a>0x03 用户活跃信息表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table cdm.dws_bhv_device_actuser_di partition (dt = &apos;$&#123;YESTERDAY&#125;&apos;)</span><br><span class="line">select </span><br><span class="line">     device</span><br><span class="line">    ,os</span><br><span class="line">    ,ip</span><br><span class="line">    ,from_unixtime(unix_timestamp(), &apos;yyyy-MM-dd HH:mm:ss&apos;) as etl_time </span><br><span class="line">from (</span><br><span class="line">    select</span><br><span class="line">         device</span><br><span class="line">        ,os</span><br><span class="line">        ,ip</span><br><span class="line">        ,row_number() over(partition by device order by log_id asc) as rank</span><br><span class="line">    from cdm.dwd_bhv_app_startup_di</span><br><span class="line">    where dt = &apos;$&#123;YESTERDAY&#125;&apos;</span><br><span class="line">        and device is not null</span><br><span class="line">        and device != &apos;&apos;</span><br><span class="line">) t</span><br><span class="line">where rank = 1</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x04-用户全量信息表"><a href="#0x04-用户全量信息表" class="headerlink" title="0x04 用户全量信息表"></a>0x04 用户全量信息表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table cdm.dim_bhv_device_ds partition(dt = &apos;$&#123;YESTERDAY&#125;&apos;)</span><br><span class="line">select </span><br><span class="line">     coalesce(a.device,b.device) as device</span><br><span class="line">    ,coalesce(b.activate_date,a.dt) as activate_date</span><br><span class="line">    ,if(b.device is null,1,0) as is_new</span><br><span class="line">    ,case when b.device is not null and a.device is not null</span><br><span class="line">              then concat(b.actlist,&apos;,1&apos;)</span><br><span class="line">          when b.device is null and a.device is not null</span><br><span class="line">              then concat(repeat(&apos;0,&apos;,datediff(&apos;$&#123;YESTERDAY&#125;&apos;,&apos;2019-01-01&apos;)),&apos;1&apos;)</span><br><span class="line">          when b.device is not null and a.device is null</span><br><span class="line">              then concat(b.actlist,&apos;,0&apos;)</span><br><span class="line">          else &apos;other&apos; </span><br><span class="line">     end as actlist</span><br><span class="line">    ,from_unixtime(unix_timestamp(), &apos;yyyy-MM-dd HH:mm:ss&apos;) as etl_time </span><br><span class="line">from (</span><br><span class="line">    select dt, device</span><br><span class="line">    from cdm.dws_bhv_device_actuser_di</span><br><span class="line">    where dt = &apos;$&#123;YESTERDAY&#125;&apos;</span><br><span class="line">) a</span><br><span class="line">full join (</span><br><span class="line">    select dt, device, activate_date, actlist</span><br><span class="line">    from cdm.dim_bhv_device_ds</span><br><span class="line">    where dt = date_sub(&apos;$&#123;YESTERDAY&#125;&apos;,1)</span><br><span class="line">) b</span><br><span class="line">on a.device = b.device </span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x05-用户留存信息表"><a href="#0x05-用户留存信息表" class="headerlink" title="0x05 用户留存信息表"></a>0x05 用户留存信息表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table ads.dm_bhv_device_remain_di partition(dt) </span><br><span class="line">select </span><br><span class="line">     a.device</span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 1)  as remain_s1d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 2)  as remain_s2d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 3)  as remain_s3d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 4)  as remain_s4d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 5)  as remain_s5d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 6)  as remain_s6d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 7)  as remain_s7d </span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 14) as remain_s14d</span><br><span class="line">    ,udf.calc_remain(b.actlist, b.dt, a.dt, 30) as remain_s30d</span><br><span class="line">    ,from_unixtime(unix_timestamp(), &apos;yyyy-MM-dd HH:mm:ss&apos;) as etl_time </span><br><span class="line">    ,a.dt</span><br><span class="line">from (</span><br><span class="line">    select device, dt</span><br><span class="line">    from cdm.dws_bhv_device_actuser_di</span><br><span class="line">    where dt in (date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-1),date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-2)</span><br><span class="line">                ,date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-3),date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-4)</span><br><span class="line">                ,date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-5),date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-6)</span><br><span class="line">                ,date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-7),date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-14)</span><br><span class="line">                ,date_add(&apos;$&#123;YESTERDAY&#125;&apos;,-30))</span><br><span class="line">) a</span><br><span class="line">left join (</span><br><span class="line">    select dt, device, actlist </span><br><span class="line">    from cdm.dim_bhv_device_ds</span><br><span class="line">    where dt = &apos;$&#123;YESTERDAY&#125;&apos; </span><br><span class="line">) b</span><br><span class="line">on a.device = b.device </span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x06-延伸"><a href="#0x06-延伸" class="headerlink" title="0x06 延伸"></a>0x06 延伸</h2><p>正如开篇所说的，这套增长模式是简化版的，还可以衍生出很多的数据模型，比如新增留存信息表等。另外，每家公司对于日活、新增和留存的业务逻辑定义不一样，ETL处理流程会比本文复杂得多，比如剔除作弊用户等。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在互联网环境下，存量市场的竞争越来越激烈，许多公司都将用户提到了战略高度。借助用户生命周期管理与用户画像等，实现对用户的精细化运营。而增长模型是支撑这些应用的基础数据模型，包括日活、新增与留存。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="modeling" scheme="https://jordenbruce.com/tags/modeling/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop命令行的导入与导出</title>
    <link href="https://jordenbruce.com/2019/10/08/sqoop-cli/"/>
    <id>https://jordenbruce.com/2019/10/08/sqoop-cli/</id>
    <published>2019-10-08T13:18:01.000Z</published>
    <updated>2019-10-25T12:51:42.290Z</updated>
    
    <content type="html"><![CDATA[<p>Sqoop是一种被设计为在Hadoop与关系数据库之间传输数据的工具。您可以使用Sqoop将数据从MySQL或Oracle等关系数据库管理系统（RDBMS）导入Hadoop分布式文件系统（HDFS），在Hadoop MapReduce中转换数据，然后将数据导出回RDBMS 。Sqoop使用MapReduce导入和导出数据，还提供了并行操作以及容错能力。<br><a id="more"></a></p><h2 id="0x00-sqoop命令行的语法"><a href="#0x00-sqoop命令行的语法" class="headerlink" title="0x00 sqoop命令行的语法"></a>0x00 sqoop命令行的语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">usage: sqoop COMMAND [ARGS]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">  codegen            Generate code to interact with database records</span><br><span class="line">  create-hive-table  Import a table definition into Hive</span><br><span class="line">  eval               Evaluate a SQL statement and display the results</span><br><span class="line">  export             Export an HDFS directory to a database table</span><br><span class="line">  help               List available commands</span><br><span class="line">  import             Import a table from a database to HDFS</span><br><span class="line">  import-all-tables  Import tables from a database to HDFS</span><br><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span><br><span class="line">  job                Work with saved jobs</span><br><span class="line">  list-databases     List available databases on a server</span><br><span class="line">  list-tables        List available tables in a database</span><br><span class="line">  merge              Merge results of incremental imports</span><br><span class="line">  metastore          Run a standalone Sqoop metastore</span><br><span class="line">  version            Display version information</span><br><span class="line"></span><br><span class="line">See &apos;sqoop help COMMAND&apos; for information on a specific command.</span><br></pre></td></tr></table></figure><p>每个命令的具体含义如下：</p><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>list-databases</td><td>列出所有数据库名</td></tr><tr><td>list-tables</td><td>列出某个数据库下所有表</td></tr><tr><td>create-hive-table</td><td>生成与关系数据库表结构对应的hive表结构</td></tr><tr><td>eval</td><td>执行一个SQL语句并显示结果</td></tr><tr><td>import</td><td>将关系型数据库中的数据导入到HDFS（包括Hive，HBase）中</td></tr><tr><td>import-all-tables</td><td>导入某个数据库下所有表到HDFS中</td></tr><tr><td>import-mainframe</td><td>从一台主机中导入数据集至HDFS</td></tr><tr><td>export</td><td>从HDFS（包括Hive和HBase）中将数据导出到关系型数据库中</td></tr><tr><td>job</td><td>用来生成一个sqoop任务，生成后不会立即执行，需要手动执行</td></tr><tr><td>merge</td><td>将HDFS中不同目录下面的数据合并在一起并放入指定目录中</td></tr><tr><td>codegen</td><td>将关系型数据库中的表映射为一个Java类，在该类中有各列对应的各个字段</td></tr><tr><td>metastore</td><td>启动元数据服务</td></tr><tr><td>version</td><td>查看版本</td></tr><tr><td>help</td><td>查看帮助</td></tr></tbody></table><p>常用命令有：list-tables, import, export 。</p><h2 id="0x01-sqoop-import"><a href="#0x01-sqoop-import" class="headerlink" title="0x01 sqoop import"></a>0x01 sqoop import</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">-D mapred.job.queue.name=root.data \</span><br><span class="line">--connect jdbc:mysql://mdw01:3306/common \</span><br><span class="line">--username sqoop -P \</span><br><span class="line">--table employee \</span><br><span class="line">--mapreduce-job-name sqoop_import_table_full \</span><br><span class="line">--create-hive-table --hive-import \</span><br><span class="line">--hive-database default --hive-table ods_employee_ds</span><br></pre></td></tr></table></figure><p>上述命令是全量导入，sqoop 还支持增量导入。</p><h2 id="0x02-sqoop-export"><a href="#0x02-sqoop-export" class="headerlink" title="0x02 sqoop export"></a>0x02 sqoop export</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">-D mapred.job.queue.name=root.data \</span><br><span class="line">--connect jdbc:mysql://mdw01:3306/common \</span><br><span class="line">--username sqoop -P \</span><br><span class="line">--table employee \</span><br><span class="line">--mapreduce-job-name sqoop_export_table \</span><br><span class="line">--export-dir /user/hive/warehouse/ods_employee_ds</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html" target="_blank" rel="noopener">Sqoop User Guide</a><br><a href="https://www.jianshu.com/p/b1fa9d853c89" target="_blank" rel="noopener">Sqoop 命令与参数详解</a><br><a href="https://www.cnblogs.com/xiaodf/p/6030102.html" target="_blank" rel="noopener">Sqoop 使用手册</a><br><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sqoop是一种被设计为在Hadoop与关系数据库之间传输数据的工具。您可以使用Sqoop将数据从MySQL或Oracle等关系数据库管理系统（RDBMS）导入Hadoop分布式文件系统（HDFS），在Hadoop MapReduce中转换数据，然后将数据导出回RDBMS 。Sqoop使用MapReduce导入和导出数据，还提供了并行操作以及容错能力。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="sqoop" scheme="https://jordenbruce.com/tags/sqoop/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的字符串函数</title>
    <link href="https://jordenbruce.com/2019/10/05/hql-function-string/"/>
    <id>https://jordenbruce.com/2019/10/05/hql-function-string/</id>
    <published>2019-10-05T04:39:39.000Z</published>
    <updated>2019-10-25T12:52:08.806Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 进行数据清洗时，使用最多的函数应该属字符串函数了。主要有：字符编码，字符拼接，字符查找，字符格式化，字符处理，字符截取，字符构造，字符切分，字符替换，编辑距离等。<br><a id="more"></a></p><h2 id="0x00-字符编码"><a href="#0x00-字符编码" class="headerlink" title="0x00 字符编码"></a>0x00 字符编码</h2><ul><li>ascii - 第一个字符的ASCII码</li><li>base64 - 从二进制转换为基本64字符串</li><li>unbase64 - 从基本64字符串转换为二进制</li><li>encode - 从字符串加密为指定字符集的二进制</li><li>decode - 从二进制解码为指定字符集的字符串</li></ul><h2 id="0x01-字符构造"><a href="#0x01-字符构造" class="headerlink" title="0x01 字符构造"></a>0x01 字符构造</h2><ul><li>repeat - 字符组重复n次的字符串</li><li>space - n个空格的字符串</li></ul><h2 id="0x02-字符拼接"><a href="#0x02-字符拼接" class="headerlink" title="0x02 字符拼接"></a>0x02 字符拼接</h2><ul><li>concat - 按顺序串联字符组所得到的字符串</li><li>concat_ws - 以指定分隔符按顺序串联字符组所得到的字符串</li></ul><h2 id="0x03-字符格式化"><a href="#0x03-字符格式化" class="headerlink" title="0x03 字符格式化"></a>0x03 字符格式化</h2><ul><li>lower - 将所有字符都转换为小写形式的字符串</li><li>upper - 将所有字符都转换为大写形式的字符串</li><li>initcap - 首字母大写而其他字母小写的字符串</li><li>format_number</li><li>printf</li></ul><h2 id="0x04-字符查找"><a href="#0x04-字符查找" class="headerlink" title="0x04 字符查找"></a>0x04 字符查找</h2><ul><li>elt - 取第几个字符串</li><li>field - 第一个匹配上字符串的位置</li><li>find_in_set - 查找以逗号分隔字符串的第一个匹配位置</li><li>in_file - 判断字符串是否在文件中占一行</li><li>instr - 查找字符串中第一个字符组匹配上的位置</li><li>locate - 查找字符串中(某个位置之后)第一个字符组匹配上的位置</li><li>ngrams</li><li>context_ngrams</li></ul><h2 id="0x05-字符截取"><a href="#0x05-字符截取" class="headerlink" title="0x05 字符截取"></a>0x05 字符截取</h2><ul><li>substring (substr) - 截取从某个位置开始指定长度的字符串</li><li>substring_index</li><li>regexp_extract - 使用正则表达式提取指定位置的字符串</li><li>get_json_object - 以指定json路径截取json字符串的元素</li><li>parse_url - 截取URL中指定部分</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select regexp_extract(&apos;foothebar&apos;, &apos;foo(.*?)(bar)&apos;, 2);</span><br><span class="line">select get_json_object(src_json.json, &apos;$.owner&apos;) from src_json;</span><br><span class="line">select parse_url(&apos;http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&apos;, &apos;HOST&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x06-字符切分"><a href="#0x06-字符切分" class="headerlink" title="0x06 字符切分"></a>0x06 字符切分</h2><ul><li>split - 拆分指定字符(正则表达式)前后的字符串</li><li>str_to_map - 按照指定格式拆分字符串为Map结构</li><li>sentences</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select split(&apos;abc,dsf,wes&apos;,&apos;,&apos;);</span><br><span class="line">select str_to_map(&apos;abc:abc,dsf:dsf&apos;,&apos;,&apos;,&apos;:&apos;);</span><br><span class="line">select sentences(&apos;Hello there! How are you?&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x07-字符替换"><a href="#0x07-字符替换" class="headerlink" title="0x07 字符替换"></a>0x07 字符替换</h2><ul><li>replace</li><li>regexp_replace - 将按模式(正则表达式)匹配上的所有旧字符串替换为新字符串并返回</li><li>translate</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select regexp_replace(&quot;foobar&quot;, &quot;oo|ar&quot;, &quot;&quot;);</span><br></pre></td></tr></table></figure><h2 id="0x08-字符处理"><a href="#0x08-字符处理" class="headerlink" title="0x08 字符处理"></a>0x08 字符处理</h2><ul><li>length - 获取字符串的长度</li><li>trim - 去掉字符串左右两边的空格</li><li>ltrim - 去掉字符串左边的空格</li><li>rtrim - 去掉字符串右边的空格</li><li>reverse - 将字符串的所有字符反转</li><li>lpad - 在字符串左侧添加n个指定字符组</li><li>rpad - 在字符串右侧添加n个指定字符组</li><li>soundex</li></ul><h2 id="0x09-编辑距离"><a href="#0x09-编辑距离" class="headerlink" title="0x09 编辑距离"></a>0x09 编辑距离</h2><ul><li>levenshtein - 由一个字符串转成另一个字符串所需的最少编辑操作次数</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-StringFunctions" target="_blank" rel="noopener">String Functions</a><br><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 进行数据清洗时，使用最多的函数应该属字符串函数了。主要有：字符编码，字符拼接，字符查找，字符格式化，字符处理，字符截取，字符构造，字符切分，字符替换，编辑距离等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的日期函数</title>
    <link href="https://jordenbruce.com/2019/10/05/hql-function-date/"/>
    <id>https://jordenbruce.com/2019/10/05/hql-function-date/</id>
    <published>2019-10-05T01:03:57.000Z</published>
    <updated>2019-10-25T12:51:02.693Z</updated>
    
    <content type="html"><![CDATA[<p>不管是数仓模型的日期维度，还是日常任务的调度时间，都会用到日期函数。主要有：当前时间，时间分段，时间加减，时间转换等。<br><a id="more"></a></p><h2 id="0x00-当前时间"><a href="#0x00-当前时间" class="headerlink" title="0x00 当前时间"></a>0x00 当前时间</h2><ul><li>current_timestamp - 当前时间戳</li><li>current_date - 当前日期</li><li>unix_timestamp</li></ul><h2 id="0x01-时间分段"><a href="#0x01-时间分段" class="headerlink" title="0x01 时间分段"></a>0x01 时间分段</h2><ul><li>year - 年</li><li>quarter - 季度</li><li>month - 月</li><li>day - 天</li><li>hour - 时</li><li>minute - 分</li><li>second - 秒</li><li>extract</li><li>weekofyear - 所属年的第几周</li><li>last_day - 日期所属月份的最后一天</li><li>next_day - 晚于start_date的下一个day_of_week</li><li>trunc - 截断为格式指定单位的日期 (月，年)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select hour(&apos;2019-10-05 09:38:37&apos;);</span><br><span class="line">select weekofyear(&apos;2019-10-05&apos;);</span><br><span class="line">select trunc(&apos;2019-10-05&apos;,&apos;YYYY&apos;);</span><br><span class="line">select trunc(&apos;2019-10-05&apos;,&apos;MM&apos;);</span><br><span class="line">select last_day(&apos;2019-10-05&apos;);</span><br><span class="line">select next_day(&apos;2019-10-05&apos;,&apos;FRIDAY&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x02-时间加减"><a href="#0x02-时间加减" class="headerlink" title="0x02 时间加减"></a>0x02 时间加减</h2><ul><li>date_add - 添加开始日期的天数</li><li>date_sub - 减去开始日期的天数</li><li>add_months - 起始日期之后num_months的日期</li><li>datediff - 从开始日期到结束日期的天数</li><li>months_between - 两个日期之间的月份数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select date_add(&apos;2019-10-05&apos;,3);</span><br><span class="line">select date_sub(&apos;2019-10-05&apos;,4);</span><br><span class="line">select add_months(&apos;2019-10-05&apos;,1,&apos;yyyy-MM-dd&apos;);</span><br><span class="line">select datediff(&apos;2019-12-31&apos;,&apos;2019-10-05&apos;);</span><br><span class="line">select months_between(&apos;2019-12-31&apos;,&apos;2019-10-05&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x03-时间转换"><a href="#0x03-时间转换" class="headerlink" title="0x03 时间转换"></a>0x03 时间转换</h2><ul><li>from_unixtime - 从时间戳转换为标准日期格式</li><li>unix_timestamp - 从标准日期格式转换为时间戳</li><li>to_date - 取时间戳字符串的日期部分</li><li>from_utc_timestamp</li><li>to_utc_timestamp</li><li>date_format</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select from_unixtime(1570240800, &apos;yyyy-MM-dd&apos;);</span><br><span class="line">select unix_timestamp(&apos;2019-10-05&apos;,&apos;yyyy-MM-dd&apos;);</span><br><span class="line">select to_date(&apos;2019-10-05 09:38:37&apos;);</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-DateFunctions" target="_blank" rel="noopener">Date Functions</a><br><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不管是数仓模型的日期维度，还是日常任务的调度时间，都会用到日期函数。主要有：当前时间，时间分段，时间加减，时间转换等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的数学函数</title>
    <link href="https://jordenbruce.com/2019/10/05/hql-function-math/"/>
    <id>https://jordenbruce.com/2019/10/05/hql-function-math/</id>
    <published>2019-10-04T23:26:30.000Z</published>
    <updated>2019-10-25T12:52:22.887Z</updated>
    
    <content type="html"><![CDATA[<p>使用Hive进行数据分析时，经常会用到数学函数和聚合函数。Hive 支持的内置数学函数有很多，主要有随机函数，取整函数，数学函数，三角函数，进制函数，符号函数，位函数，多列最值函数，分桶函数等。<br><a id="more"></a></p><h2 id="0x00-随机函数"><a href="#0x00-随机函数" class="headerlink" title="0x00 随机函数"></a>0x00 随机函数</h2><ul><li>rand - 随机数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select cast(rand()*1000 as int);</span><br></pre></td></tr></table></figure><h2 id="0x01-取整函数"><a href="#0x01-取整函数" class="headerlink" title="0x01 取整函数"></a>0x01 取整函数</h2><ul><li>round - 保留几位小数</li><li>bround</li><li>floor - 向下取整</li><li>ceil - 向上取整</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select round(rand()*100, 2);</span><br><span class="line">select floor(rand()*100);</span><br><span class="line">select ceil(rand()*100);</span><br></pre></td></tr></table></figure><h2 id="0x02-数学函数"><a href="#0x02-数学函数" class="headerlink" title="0x02 数学函数"></a>0x02 数学函数</h2><ul><li>log - 对数</li><li>log2 - 以2为底的对数</li><li>log10 - 以10为底的对数</li><li>ln - 以e为底的对数</li><li>pow - 指数</li><li>exp - 以e为底的指数</li><li>sqrt - 平方根</li><li>cbrt - 立方根</li><li>e - 自然常数e</li><li>pi - 自然常数π</li><li>factorial - 阶乘</li></ul><h2 id="0x03-三角函数"><a href="#0x03-三角函数" class="headerlink" title="0x03 三角函数"></a>0x03 三角函数</h2><ul><li>sin - 正弦</li><li>cos - 余弦</li><li>tan - 正切</li><li>asin - 反正弦</li><li>acos - 反余弦</li><li>atan - 反正切</li><li>degrees - 从弧度转换为度</li><li>radians - 从度转换为弧度</li></ul><h2 id="0x04-进制函数"><a href="#0x04-进制函数" class="headerlink" title="0x04 进制函数"></a>0x04 进制函数</h2><ul><li>bin - 二进制</li><li>hex - 十六进制</li><li>unhex</li><li>conv - 进制转换</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select conv(cast(rand()*1000 as bigint), 10, 2);</span><br></pre></td></tr></table></figure><h2 id="0x05-符号函数"><a href="#0x05-符号函数" class="headerlink" title="0x05 符号函数"></a>0x05 符号函数</h2><ul><li>positive - 正数</li><li>negative - 负数</li><li>sign - 正负判断</li><li>abs - 绝对值</li></ul><h2 id="0x06-位函数"><a href="#0x06-位函数" class="headerlink" title="0x06 位函数"></a>0x06 位函数</h2><ul><li>shiftleft - 按位左移</li><li>shiftright - 按位右移</li><li>shiftrightunsigned - 按位无符号右移</li></ul><h2 id="0x07-多列最值函数"><a href="#0x07-多列最值函数" class="headerlink" title="0x07 多列最值函数"></a>0x07 多列最值函数</h2><ul><li>greatest - 一行多列取最大值</li><li>least - 一行多列取最小值</li></ul><h2 id="0x07-分桶函数"><a href="#0x07-分桶函数" class="headerlink" title="0x07 分桶函数"></a>0x07 分桶函数</h2><ul><li>pmod - 取模的正数</li><li>width_bucket</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-MathematicalFunctions" target="_blank" rel="noopener">Mathematical Functions</a><br><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Hive进行数据分析时，经常会用到数学函数和聚合函数。Hive 支持的内置数学函数有很多，主要有随机函数，取整函数，数学函数，三角函数，进制函数，符号函数，位函数，多列最值函数，分桶函数等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的函数概览</title>
    <link href="https://jordenbruce.com/2019/09/30/hql-function/"/>
    <id>https://jordenbruce.com/2019/09/30/hql-function/</id>
    <published>2019-09-30T11:24:49.000Z</published>
    <updated>2019-10-25T12:50:11.848Z</updated>
    
    <content type="html"><![CDATA[<p>Select语句主要有三部分：Select子句 (WITH,SELECT,FROM,WHERE,GROUP BY,HAVING,ORDER BY,LIMIT)，Join语句，Function函数。其中，Function函数 是最精彩也是最丰富的部分，不仅官方内置了大量函数，而且用户还可以自定义函数。本文以内置函数为主。<br><a id="more"></a></p><h2 id="0x00-函数语法"><a href="#0x00-函数语法" class="headerlink" title="0x00 函数语法"></a>0x00 函数语法</h2><p>在命令行环境下，使用以下命令查看当前Hive版本的所有函数及其语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SHOW FUNCTIONS;</span><br><span class="line">DESCRIBE FUNCTION &lt;function_name&gt;;</span><br><span class="line">DESCRIBE FUNCTION EXTENDED &lt;function_name&gt;;</span><br></pre></td></tr></table></figure><h2 id="0x01-运算符-Operators"><a href="#0x01-运算符-Operators" class="headerlink" title="0x01 运算符 (Operators)"></a>0x01 运算符 (Operators)</h2><p>内置的运算符有5大类：</p><ul><li>关系运算符 (Relational Operators)</li><li>算术运算符 (Arithmetic Operators)</li><li>逻辑运算符 (Logical Operators)</li><li>字符串运算符 (String Operators)</li><li>复杂类型 (Complex Types)</li></ul><p>大部分都是经常使用的，比如 =,is not null,+,and 等，以下几个运算符不常用却很重要的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select * from default.managed_user where user_id rlike &apos;^(A|B).*&apos;;</span><br><span class="line"></span><br><span class="line">select from_unixtime(log_timestamp div 1000, &apos;yyyy-MM-dd HH:mm:ss&apos;) as log_time;</span><br><span class="line"></span><br><span class="line">select array(&apos;hadoop&apos;, &apos;hive&apos;, &apos;hql&apos;);</span><br><span class="line">select named_struct(&apos;kid&apos;,3001, &apos;user_id&apos;,&apos;C001&apos;, &apos;user_name&apos;,&apos;sqoop&apos;);</span><br><span class="line">select map(&apos;kid&apos;,3001, &apos;user_id&apos;,&apos;C001&apos;, &apos;user_name&apos;,&apos;sqoop&apos;);</span><br></pre></td></tr></table></figure><p>对于 array,struct,map 三种复杂数据类型，都有对应的运算符取其内的数值。</p><h2 id="0x02-标准函数-UDF-Functions"><a href="#0x02-标准函数-UDF-Functions" class="headerlink" title="0x02 标准函数 (UDF, Functions)"></a>0x02 标准函数 (UDF, Functions)</h2><p>内置的标准函数有8大类：</p><ul><li>数学函数 (Mathematical Functions)</li><li>日期函数 (Date Functions)</li><li>字符串函数 (String Functions)</li><li>字符串掩码函数 (Data Masking Functions)</li><li>条件函数 (Conditional Functions)</li><li>类型转换函数 (Type Conversion Functions)</li><li>集合函数 (Collection Functions)</li><li>其他函数 (Misc. Functions)</li></ul><p>常用三大类函数是：数学函数，字符串函数，日期函数；后面会对每一大类函数单独写一篇手册，因为函数是在太丰富了。另外5类函数常用的有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">select mask_last_n(&apos;13912345678&apos;, 4);</span><br><span class="line"></span><br><span class="line">select if(2019&gt;2014, true, false);</span><br><span class="line">select coalesce(user_name, &apos;unknown&apos;);</span><br><span class="line">select case when user_name = &apos;hql&apos; then &apos;hive&apos; else &apos;other&apos; end;</span><br><span class="line"></span><br><span class="line">select cast(user_id as string);</span><br><span class="line"></span><br><span class="line">select size(map_column_name);</span><br><span class="line"></span><br><span class="line">select md5(&apos;HiveQL&apos;);</span><br></pre></td></tr></table></figure><h2 id="0x03-聚合函数-UDAF-Aggregate-Functions"><a href="#0x03-聚合函数-UDAF-Aggregate-Functions" class="headerlink" title="0x03 聚合函数 (UDAF, Aggregate Functions)"></a>0x03 聚合函数 (UDAF, Aggregate Functions)</h2><p>内置的聚合函数主要有：</p><ul><li>常见聚合函数 (count,sum,avg,max,min)</li><li>统计聚合函数 (方差，标准差，协方差，相关系数，分位数，直方图)</li><li>字符串聚合函数 (collect_list,collect_set)</li></ul><h2 id="0x04-表生成函数-UDTF-Table-Generating-Functions"><a href="#0x04-表生成函数-UDTF-Table-Generating-Functions" class="headerlink" title="0x04 表生成函数 (UDTF, Table-Generating Functions)"></a>0x04 表生成函数 (UDTF, Table-Generating Functions)</h2><p>内置的表生成函数有：</p><ul><li>explode</li><li>posexplode</li><li>inline</li><li>stack</li><li>json_tuple</li><li>parse_url_tuple</li></ul><p>最常用的函数是 explode，使用方式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select explode(array(&apos;A&apos;,&apos;B&apos;,&apos;C&apos;)) as col;</span><br><span class="line">select tf.* from (select 0) t lateral view explode(array(&apos;A&apos;,&apos;B&apos;,&apos;C&apos;)) tf as col;</span><br><span class="line"></span><br><span class="line">select explode(map(&apos;A&apos;,10,&apos;B&apos;,20,&apos;C&apos;,30)) as (key,value);</span><br><span class="line">select tf.* from (select 0) t lateral view explode(map(&apos;A&apos;,10,&apos;B&apos;,20,&apos;C&apos;,30)) tf as key,value;</span><br></pre></td></tr></table></figure><h2 id="0x05-XPath特定函数-XPath-specific-Functions"><a href="#0x05-XPath特定函数-XPath-specific-Functions" class="headerlink" title="0x05 XPath特定函数 (XPath-specific Functions)"></a>0x05 XPath特定函数 (XPath-specific Functions)</h2><p>内置的XPath特定函数有：</p><ul><li>xpath</li><li>xpath_string</li><li>xpath_boolean</li><li>xpath_short</li><li>xpath_int</li><li>xpath_long</li><li>xpath_float</li><li>xpath_double</li><li>xpath_number</li></ul><h2 id="0x06-窗口和分析函数-Windowing-and-Analytics-Functions"><a href="#0x06-窗口和分析函数-Windowing-and-Analytics-Functions" class="headerlink" title="0x06 窗口和分析函数 (Windowing and Analytics Functions)"></a>0x06 窗口和分析函数 (Windowing and Analytics Functions)</h2><p>内置的窗口和分析函数主要有：</p><ul><li>窗口函数 (Windowing functions)</li><li>OVER子句 (The OVER clause)</li><li>分析函数 (Analytics functions)</li></ul><p>以下是两个常用的使用方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select a, sum(b) over (partition by c) from t;</span><br><span class="line"></span><br><span class="line">select a, row_number() over (partition by b order by d desc) from t;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">LanguageManual UDF</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+XPathUDF" target="_blank" rel="noopener">LanguageManual XPathUDF</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">LanguageManual WindowingAndAnalytics</a><br><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Select语句主要有三部分：Select子句 (WITH,SELECT,FROM,WHERE,GROUP BY,HAVING,ORDER BY,LIMIT)，Join语句，Function函数。其中，Function函数 是最精彩也是最丰富的部分，不仅官方内置了大量函数，而且用户还可以自定义函数。本文以内置函数为主。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的Select语句</title>
    <link href="https://jordenbruce.com/2019/09/26/hql-select/"/>
    <id>https://jordenbruce.com/2019/09/26/hql-select/</id>
    <published>2019-09-26T13:30:42.000Z</published>
    <updated>2019-10-25T12:52:03.266Z</updated>
    
    <content type="html"><![CDATA[<p>前面文章已经解决了数据存储的问题，这篇将介绍查询数据的Select语句。当表中的数据越来越多时，如何查询想要的数据，或者进行数据分析呢？<br><a id="more"></a></p><h2 id="0x00-语法"><a href="#0x00-语法" class="headerlink" title="0x00 语法"></a>0x00 语法</h2><p>官方的语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[WITH CommonTableExpression (, CommonTableExpression)*]</span><br><span class="line">SELECT [ALL | DISTINCT] select_expr, select_expr, ...</span><br><span class="line">  FROM table_reference</span><br><span class="line">  [WHERE where_condition]</span><br><span class="line">  [GROUP BY col_list]</span><br><span class="line">    [HAVING having_condition]</span><br><span class="line">  [ORDER BY col_list]</span><br><span class="line">  [CLUSTER BY col_list</span><br><span class="line">    | [DISTRIBUTE BY col_list] [SORT BY col_list]</span><br><span class="line">  ]</span><br><span class="line"> [LIMIT [offset,] rows]</span><br></pre></td></tr></table></figure><p>主要包括：WITH子句 (公共临时表)，SELECT子句 (查询结果的输出列)，FROM子句 (查询的数据源)，WHERE子句 (过滤条件)，GROUP BY子句 (分组列表)，HAVING子句 (分组的过滤条件)，ORDER BY子句 (排序列表)，LIMIT子句 (限制输出行数) 等。需要说明的是：</p><ul><li>select语句可以是union查询的一部分或者是另一个查询的子查询。</li><li>table_reference指示查询的输入。它可以是普通的表，视图，join构造或者是子查询。</li></ul><h2 id="0x01-单表查询"><a href="#0x01-单表查询" class="headerlink" title="0x01 单表查询"></a>0x01 单表查询</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select reg_date, count(kid) as cnt</span><br><span class="line">from default.ods_user</span><br><span class="line">where reg_date &gt;= &apos;2019-09-20&apos;</span><br><span class="line">group by reg_date</span><br><span class="line">  having count(kid) &gt; 1</span><br><span class="line">order by reg_date desc</span><br><span class="line">limit 3 </span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>请问，这条SQL语句表达的是什么业务场景？</p><h2 id="0x02-多表连接查询"><a href="#0x02-多表连接查询" class="headerlink" title="0x02 多表连接查询"></a>0x02 多表连接查询</h2><p>HiveQL支持的JOIN方式有：</p><ul><li>内连接 ([INNER] JOIN)</li><li>外连接 ({LEFT|RIGHT|FULL} [OUTER] JOIN)</li><li>左半连接 (LEFT SEMI JOIN)</li><li>笛卡尔积关联 (CROSS JOIN)</li></ul><p>每一种连接的使用说明，请参考 <a href="https://www.cnblogs.com/liupengpengg/p/7908274.html" target="_blank" rel="noopener">Hive中Join的类型和用法</a></p><h2 id="0x03-函数"><a href="#0x03-函数" class="headerlink" title="0x03 函数"></a>0x03 函数</h2><p>HiveQL内置函数主要有6大类：</p><ul><li>运算符 (Operators)</li><li>标准函数 (UDF, Functions)</li><li>聚合函数 (UDAF, Aggregate Functions)</li><li>表生成函数 (UDTF, Table-Generating Functions)</li><li>XPath特定函数 (XPath-specific Functions)</li><li>窗口和分析函数 (Windowing and Analytics Functions)</li></ul><p>有关常用函数的使用说明，下一篇再继续写。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select" target="_blank" rel="noopener">LanguageManual Select</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins" target="_blank" rel="noopener">LanguageManual Joins</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">LanguageManual UDF</a><br><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面文章已经解决了数据存储的问题，这篇将介绍查询数据的Select语句。当表中的数据越来越多时，如何查询想要的数据，或者进行数据分析呢？&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的导入与导出</title>
    <link href="https://jordenbruce.com/2019/09/24/hql-dml/"/>
    <id>https://jordenbruce.com/2019/09/24/hql-dml/</id>
    <published>2019-09-24T13:21:29.000Z</published>
    <updated>2019-10-25T12:50:18.189Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇说完了Table常用操作，创建了几种表，可是表中还没有数据，这就需要数据导入；数据按照业务逻辑经过Hive各种处理之后，还需要数据导出，方便进一步的分析处理。<br><a id="more"></a></p><h2 id="0x00-综述"><a href="#0x00-综述" class="headerlink" title="0x00 综述"></a>0x00 综述</h2><p>常见的数据导入方式有：</p><ul><li>本地文件导入到Hive表</li><li>HDFS文件导入到Hive表</li><li>Hive表导入到一个Hive表</li><li>创建表的过程中从其他表导入</li><li>Hive表导入到多个Hive表</li></ul><p>常见的数据导出方式有：</p><ul><li>Hive表导出到本地文件系统</li><li>Hive表导出到HDFS</li><li>命令行导出到本地文件系统</li></ul><h2 id="0x01-导入"><a href="#0x01-导入" class="headerlink" title="0x01 导入"></a>0x01 导入</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/home/hadoop/user1.txt&apos; into table default.ods_user;</span><br><span class="line">load data inpath &apos;/ods/user2.txt&apos; into table default.ods_user;</span><br><span class="line"></span><br><span class="line">insert overwrite table default.managed_user </span><br><span class="line">select kid,user_id,user_name,reg_date from default.ods_user;</span><br><span class="line"></span><br><span class="line">insert overwrite table default.partitioned_user </span><br><span class="line">partition( dt = &apos;2019-09-20&apos;) </span><br><span class="line">select kid,user_id,user_name,reg_date </span><br><span class="line">from default.managed_user</span><br><span class="line">where reg_date = &apos;2019-09-20&apos;;</span><br><span class="line"></span><br><span class="line">create table default.user_20190921 as </span><br><span class="line">select kid,user_id,user_name,reg_date </span><br><span class="line">from default.ods_user </span><br><span class="line">where reg_date = &apos;2019-09-21&apos;;</span><br><span class="line"></span><br><span class="line">from default.managed_user </span><br><span class="line">insert overwrite table default.partitioned_user partition( dt = &apos;2019-09-21&apos;) </span><br><span class="line">select kid,user_id,user_name,reg_date where reg_date = &apos;2019-09-21&apos; </span><br><span class="line">insert overwrite table default.partitioned_user partition( dt = &apos;2019-09-22&apos;) </span><br><span class="line">select kid,user_id,user_name,reg_date where reg_date = &apos;2019-09-22&apos; ;</span><br></pre></td></tr></table></figure><h2 id="0x02-导出"><a href="#0x02-导出" class="headerlink" title="0x02 导出"></a>0x02 导出</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory &apos;/home/hadoop/user&apos; </span><br><span class="line">row format delimited fields terminated by &apos;|&apos; </span><br><span class="line">stored as textfile </span><br><span class="line">select kid,user_id,user_name,reg_date from default.managed_user;</span><br><span class="line"></span><br><span class="line">insert overwrite directory &apos;/external/user&apos; </span><br><span class="line">stored as parquet </span><br><span class="line">select kid,user_id,user_name,reg_date from default.managed_user;</span><br><span class="line"></span><br><span class="line">hive -e &quot;select * from default.managed_user&quot; &gt;&gt; /home/hadoop/user/source.txt</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML" target="_blank" rel="noopener">LanguageManual DML</a><br><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇说完了Table常用操作，创建了几种表，可是表中还没有数据，这就需要数据导入；数据按照业务逻辑经过Hive各种处理之后，还需要数据导出，方便进一步的分析处理。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的Table常用操作</title>
    <link href="https://jordenbruce.com/2019/09/23/hql-table/"/>
    <id>https://jordenbruce.com/2019/09/23/hql-table/</id>
    <published>2019-09-23T00:28:31.000Z</published>
    <updated>2019-10-25T12:51:58.162Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive数据仓库软件有助于读取，写入和管理驻留在分布式存储中并使用SQL语法查询的大型数据集。而Table是Hive组织数据存储的主要数据单元，是一种结构化存储，用二维表结构来表示。<br><a id="more"></a></p><h2 id="0x00-create-table"><a href="#0x00-create-table" class="headerlink" title="0x00 create table"></a>0x00 create table</h2><p>经常使用的表有：内部表(managed table)，外部表(external table)，分区表(partitioned table)，临时表(temporary table)等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">--1. managed table</span><br><span class="line">create table if not exists default.managed_user (</span><br><span class="line">     kid       bigint  comment &apos;主键&apos;</span><br><span class="line">    ,user_id   string  comment &apos;用户编号&apos;</span><br><span class="line">    ,user_name string  comment &apos;用户名称&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;内部用户表&apos;</span><br><span class="line">stored as parquet</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">--2. external table</span><br><span class="line">create external table if not exists default.external_user (</span><br><span class="line">     kid       bigint  comment &apos;主键&apos;</span><br><span class="line">    ,user_id   string  comment &apos;用户编号&apos;</span><br><span class="line">    ,user_name string  comment &apos;用户名称&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;外部用户表&apos;</span><br><span class="line">stored as textfile</span><br><span class="line">location &apos;/external/user&apos;</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">--3. partitioned table</span><br><span class="line">create table if not exists default.partitioned_user (</span><br><span class="line">     kid       bigint  comment &apos;主键&apos;</span><br><span class="line">    ,user_id   string  comment &apos;用户编号&apos;</span><br><span class="line">    ,user_name string  comment &apos;用户名称&apos;</span><br><span class="line">)</span><br><span class="line">comment &apos;分区用户表&apos;</span><br><span class="line">partitioned by (</span><br><span class="line">    dt string comment &apos;日期分区&apos;</span><br><span class="line">)</span><br><span class="line">stored as parquet</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">--4. temporary table</span><br><span class="line">create temporary table if not exists default.temporary_user </span><br><span class="line">like default.managed_user</span><br><span class="line">;</span><br><span class="line">create temporary table if not exists default.temporary_user as </span><br><span class="line">select kid, user_id from default.partitioned_user where dt = &apos;2019-09-22&apos;</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x01-alter-table"><a href="#0x01-alter-table" class="headerlink" title="0x01 alter table"></a>0x01 alter table</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">alter table default.external_user rename to default.external_user_ds;</span><br><span class="line"></span><br><span class="line">alter table default.managed_user set tblproperties(&quot;skip.header.line.count&quot;=&quot;1&quot;);</span><br><span class="line"></span><br><span class="line">alter table default.managed_user add columns (reg_date string comment &apos;注册日期&apos;);</span><br><span class="line">alter table default.partitioned_user add columns (reg_date string comment &apos;注册日期&apos;) cascade;</span><br></pre></td></tr></table></figure><h2 id="0x02-describe-table-amp-show-table"><a href="#0x02-describe-table-amp-show-table" class="headerlink" title="0x02 describe table &amp; show table"></a>0x02 describe table &amp; show table</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">desc default.managed_user;</span><br><span class="line">desc formatted default.managed_user;</span><br><span class="line"></span><br><span class="line">show tables;</span><br><span class="line">show create table default.managed_user;</span><br><span class="line">show partitions default.partitioned_user;</span><br></pre></td></tr></table></figure><h2 id="0x03-truncate-table-amp-drop-table"><a href="#0x03-truncate-table-amp-drop-table" class="headerlink" title="0x03 truncate table &amp; drop table"></a>0x03 truncate table &amp; drop table</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">truncate table default.managed_user;</span><br><span class="line"></span><br><span class="line">drop table default.partitioned_user;</span><br><span class="line">alter table default.partitioned_user drop partition(dt = &apos;2019-09-22&apos;);</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">LanguageManual DDL</a><br><a href="https://jordenbruce.com/2019/10/12/junior-manual/">数据仓库的初级手册</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Hive数据仓库软件有助于读取，写入和管理驻留在分布式存储中并使用SQL语法查询的大型数据集。而Table是Hive组织数据存储的主要数据单元，是一种结构化存储，用二维表结构来表示。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
</feed>
