<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JordenBruce</title>
  
  <subtitle>A thousand miles begins with a single step.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jordenbruce.com/"/>
  <updated>2020-02-29T09:51:38.410Z</updated>
  <id>https://jordenbruce.com/</id>
  
  <author>
    <name>JordenBruce</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【翻译】Hive中的分区和桶</title>
    <link href="https://jordenbruce.com/2020/02/29/hive-partition-bucket/"/>
    <id>https://jordenbruce.com/2020/02/29/hive-partition-bucket/</id>
    <published>2020-02-29T08:55:30.000Z</published>
    <updated>2020-02-29T09:51:38.410Z</updated>
    
    <content type="html"><![CDATA[<p>在这篇文章中，我们将讨论Hive中的两个重要概念“分区和桶”。这些用于提高查询性能，理解它们很重要，以便您可以有效地应用它们。因此，让我们从分区开始。<br><a id="more"></a></p><h2 id="0x00-分区"><a href="#0x00-分区" class="headerlink" title="0x00 分区"></a>0x00 分区</h2><p>分区是Hive中一种用于增强查询性能的技术。通过将数据重组到子目录中来完成此操作。让我们通过一个例子来理解这个概念。</p><p>假设我们有一个10 GB的大文件，其中包含客户的地理数据。现在，我们要提取特定国家和特定受雇者身份的记录。为此，它将执行表扫描以读取所有行，然后仅选择满足给定谓词的那些记录。</p><p>现在，如果我们按国家对表进行分区并运行查询，它将不会扫描整个表，而只会查看该特定国家/地区的子目录。我们可以看到查询的执行计划，以验证它只查找一个过滤谓词即employeeId。它将直接查看子目录/Country=”印度”，并在该子目录中搜索员工。此技术称为分区修剪。该查询将过滤掉不需要扫描的分区，因此非常有效。同样，用于分区的列也不是create语句的一部分，但可以在查询中使用。它们称为虚拟分区列。每当我们将这些虚拟分区列用作过滤器时，查询服务的速度都比未分区表快得多，尤其是在数据量很大的情况下。</p><p>让我们看看如何创建Hive内部（管理表）和外部分区表并将数据加载到这些表中。</p><h3 id="Hive内部表"><a href="#Hive内部表" class="headerlink" title="Hive内部表"></a>Hive内部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--(1)创建分区表</span><br><span class="line">Create table tbl_managed</span><br><span class="line">(</span><br><span class="line">    employeeId STRING,</span><br><span class="line">    name STRING</span><br><span class="line">)</span><br><span class="line">Partitioned By(country STRING)</span><br><span class="line">Stored as TextFile;</span><br><span class="line">--(2)加载数据</span><br><span class="line">Load Data Inpath &apos;/mydata/Employee/India&apos;</span><br><span class="line">into Table tbl_managed</span><br><span class="line">Partition(country=&apos;India&apos;)</span><br></pre></td></tr></table></figure><h3 id="Hive外部表"><a href="#Hive外部表" class="headerlink" title="Hive外部表"></a>Hive外部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">--(1)创建分区表</span><br><span class="line">Create External table tbl_external</span><br><span class="line">(</span><br><span class="line">    employeeid STRING,</span><br><span class="line">    name STRING</span><br><span class="line">)</span><br><span class="line">Partitioned By(country STRING)</span><br><span class="line">Stored as TextFile;</span><br><span class="line">--(2)添加数据</span><br><span class="line">Alter Table tbl_external Add Partition(country=&apos;India&apos;)</span><br><span class="line">Location &apos;/mydata/Employee/India&apos;;</span><br></pre></td></tr></table></figure><h3 id="分区时的注意事项"><a href="#分区时的注意事项" class="headerlink" title="分区时的注意事项"></a>分区时的注意事项</h3><ul><li>您用于分区的列的基数应该较低，即该列的不同值较低。原因是由于基数高，我们最终会有许多子目录或文件。由于mapper数量取决于输入大小和块大小，因此创建许多分区最终将使用许多mapper，并且在大多数情况下，这将导致资源浪费。</li><li>许多分区还会发生的另一件事是，在内存中跟踪文件系统元数据的NameNode也将具有不必要的开销，因为它现在必须跟踪许多分区。</li><li>当我们在外部表上创建分区时，位置是可选的。但是我们应该始终提供位置（例如root/a/b），因为以后可以将其用于与Hive metastore同步。因此，如果您提供了位置，然后添加了诸如root/a/b/country=’India’之类的子目录，那么当我们运行命令MSCK Repair Table Tablename时。它将自动添加该分区。</li></ul><h3 id="分区在以下情况下很有用"><a href="#分区在以下情况下很有用" class="headerlink" title="分区在以下情况下很有用"></a>分区在以下情况下很有用</h3><ul><li>分区数量有限</li><li>所有分区均匀分布</li></ul><h2 id="0x02-桶"><a href="#0x02-桶" class="headerlink" title="0x02 桶"></a>0x02 桶</h2><p>当由于分区不相等或分区数量太多而无法通过分区提高查询效率时，我们可以尝试进行分桶。桶概念基于桶列上的哈希函数。产生相同哈希的记录将始终位于同一存储桶中。</p><p>要将表划分为桶，我们使用Clustered by子句。每个桶就像目录中的文件，并且所有文件均等分布。</p><h3 id="桶的优点"><a href="#桶的优点" class="headerlink" title="桶的优点"></a>桶的优点</h3><ul><li>在分桶表上进行Map-Side联接的速度更快，因为它们的大小相似。</li><li>可以保持记录在每个存储桶中排序</li><li>当数据按桶中的列排序并用于联接时，Map端联接甚至更快</li><li>提供对非分桶表的有效采样</li><li>使用分桶，我们始终可以定义要形成的桶数量，而在分区中却并非如此</li><li>分区或不分区均可使用桶</li></ul><p>如何创建分桶表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Create Table t1(a INT,b STRING,c STRING)</span><br><span class="line">CLUSTERED BY (b) BUCKETS 128 BUCKETS</span><br></pre></td></tr></table></figure><p>Hive不会对加载到表中的数据强制执行分桶。创建表后，我们必须对其进行管理。有两种方法可以实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--(1)set mapred.reduce.tasks=64(桶数)</span><br><span class="line">Insert Overwrite Table t1</span><br><span class="line">Select a,b,c from table2 cluster by b</span><br><span class="line">--(2) set hive.enforce.bucketing=true;</span><br><span class="line">Insert Overwrite Table t1</span><br><span class="line">Select a,b,c from table2</span><br></pre></td></tr></table></figure><p>这将自动从Create Table语句中找出分桶列和桶数量。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.sqlrelease.com/partitioning-and-bucketing-in-hive" target="_blank" rel="noopener">Partitioning and Bucketing in Hive</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在这篇文章中，我们将讨论Hive中的两个重要概念“分区和桶”。这些用于提高查询性能，理解它们很重要，以便您可以有效地应用它们。因此，让我们从分区开始。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>常用的Hive用户配置属性</title>
    <link href="https://jordenbruce.com/2019/12/21/hive-config-property/"/>
    <id>https://jordenbruce.com/2019/12/21/hive-config-property/</id>
    <published>2019-12-21T09:23:11.000Z</published>
    <updated>2020-02-29T08:11:59.468Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 用户配置属性有很多，包括查询引擎的、文件格式的、元数据的、HiveServer2的等等，本文主要介绍查询引擎的一些常用配置属性。另外，Hive 采用 MapReduce 和 Yarn 作为计算引擎和资源调度时，也有各自的用户配置属性，不在本文的介绍范围之内。<br><a id="more"></a></p><h2 id="0x00-常用的用户配置属性"><a href="#0x00-常用的用户配置属性" class="headerlink" title="0x00 常用的用户配置属性"></a>0x00 常用的用户配置属性</h2><p>尽管Hive完整的用户配置属性有800多个，但是开发者们经过不断地迭代和使用场景的收集，对所有的配置属性都给定了默认值，来满足大部分的应用场景。在某些特定应用场景下，我们还需要手动更改配置属性，来优化查询引擎，提高执行效率。以下是一些常用的配置属性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">set hive.execution.engine=mr;</span><br><span class="line">---并行地执行作业</span><br><span class="line">set hive.exec.parallel=false;</span><br><span class="line">---合并小文件</span><br><span class="line">set hive.merge.mapfiles=true;</span><br><span class="line">set hive.merge.mapredfiles=false;</span><br><span class="line">set hive.hadoop.supports.splittable.combineinputformat=false;</span><br><span class="line">---分组查询时map端聚合</span><br><span class="line">set hive.map.aggr=true;</span><br><span class="line">---优化分组查询的数据倾斜</span><br><span class="line">set hive.groupby.skewindata=false;</span><br><span class="line">---优化谓词下推</span><br><span class="line">set hive.optimize.ppd=true;</span><br><span class="line">---启用mapjoin优化</span><br><span class="line">set hive.ignore.mapjoin.hint=true;</span><br><span class="line">set hive.auto.convert.join=true;</span><br><span class="line">---启用倾斜连接优化</span><br><span class="line">set hive.optimize.skewjoin=false;</span><br><span class="line">---设置reduce任务数</span><br><span class="line">set mapred.reduce.tasks=-1;</span><br><span class="line">set hive.exec.reducers.bytes.per.reducer=256000000;</span><br><span class="line">set hive.exec.reducers.max=1009;</span><br><span class="line">---启用reduce任务的推测执行</span><br><span class="line">set hive.mapred.reduce.tasks.speculative.execution=true;</span><br><span class="line">---启用动态分区</span><br><span class="line">set hive.exec.dynamic.partition=true;</span><br><span class="line">set hive.exec.dynamic.partition.mode=strict;</span><br><span class="line">---查询的最终输出是否压缩</span><br><span class="line">set hive.exec.compress.output=false;</span><br><span class="line">---矢量化查询</span><br><span class="line">set hive.vectorized.execution.enabled=true;</span><br><span class="line">set hive.vectorized.execution.reduce.enabled=true;</span><br><span class="line">---基于成本的优化CBO</span><br><span class="line">set hive.cbo.enable=true;</span><br><span class="line">set hive.compute.query.using.stats=true;</span><br><span class="line">set hive.stats.fetch.column.stats=true;</span><br><span class="line">set hive.stats.fetch.partition.stats=true;</span><br></pre></td></tr></table></figure><p>当然，以上的每一种业务场景都会有一套配置属性，这里就不一一列举了，后期碰到了具体业务场景再进行补充。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties" target="_blank" rel="noopener">Hive Configuration Properties</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 用户配置属性有很多，包括查询引擎的、文件格式的、元数据的、HiveServer2的等等，本文主要介绍查询引擎的一些常用配置属性。另外，Hive 采用 MapReduce 和 Yarn 作为计算引擎和资源调度时，也有各自的用户配置属性，不在本文的介绍范围之内。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>IDEA用Maven开发UDF的最佳实践</title>
    <link href="https://jordenbruce.com/2019/12/20/idea-maven-udf/"/>
    <id>https://jordenbruce.com/2019/12/20/idea-maven-udf/</id>
    <published>2019-12-20T15:10:04.000Z</published>
    <updated>2019-12-21T02:02:14.970Z</updated>
    
    <content type="html"><![CDATA[<p>Intellij IDEA作为一种IDE开发工具，能有效提升编码效率。Maven的核心功能是合理叙述项目间的依赖关系，通过pom.xml文件的配置获取jar依赖包，而不用手动添加。那么，使用Intellij IDEA与Maven如何帮助我们快速地开发Hive UDF函数呢？<br><a id="more"></a></p><h2 id="0x00-搭建开发环境"><a href="#0x00-搭建开发环境" class="headerlink" title="0x00 搭建开发环境"></a>0x00 搭建开发环境</h2><p>Windows操作系统下，下载并安装三个软件：</p><ul><li>JDK 8u112</li><li>Apache Maven 3.3.9</li><li>IntelliJ IDEA Community 2019.2</li></ul><p>安装Maven后，建议修改 {maven_home}/conf/settings.xml 的 localRepository 属性；安装IntelliJ IDEA后，File &gt; Settings &gt; Build Execution Deployment &gt; Build Tools &gt; Maven，必须修改 Maven home directory 和 User settings file 两个配置项，映射到之前安装的Maven版本，使得新建Maven项目都生效。</p><h2 id="0x01-Maven工程的构建"><a href="#0x01-Maven工程的构建" class="headerlink" title="0x01 Maven工程的构建"></a>0x01 Maven工程的构建</h2><p>新建Maven项目，File &gt; New &gt; Project &gt; Maven，点击 Enable Auto-Import，修改pom.xml文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;</span><br><span class="line">         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;com.data.hive&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hive_udf&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">    &lt;properties&gt;</span><br><span class="line">        &lt;project.build.sourceEncoding&gt;UTF8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">        &lt;hadoop.version&gt;2.7.2&lt;/hadoop.version&gt;</span><br><span class="line">        &lt;hive.version&gt;2.0.1&lt;/hive.version&gt;</span><br><span class="line">    &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">    &lt;repositories&gt;</span><br><span class="line">        &lt;!--加入Hadoop原生态的maven仓库的地址--&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;Apache Hadoop&lt;/id&gt;</span><br><span class="line">            &lt;name&gt;Apache Hadoop&lt;/name&gt;</span><br><span class="line">            &lt;url&gt;https://repo1.maven.org/maven2/&lt;/url&gt;</span><br><span class="line">        &lt;/repository&gt;</span><br><span class="line">    &lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;!--添加hadoop依赖--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;!--添加hive依赖--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;/version&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">                        &lt;/goals&gt;</span><br><span class="line">                        &lt;configuration&gt;</span><br><span class="line">                            &lt;filters&gt;</span><br><span class="line">                                &lt;filter&gt;</span><br><span class="line">                                    &lt;artifact&gt;*:*&lt;/artifact&gt;</span><br><span class="line">                                    &lt;excludes&gt;</span><br><span class="line">                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</span><br><span class="line">                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</span><br><span class="line">                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</span><br><span class="line">                                    &lt;/excludes&gt;</span><br><span class="line">                                &lt;/filter&gt;</span><br><span class="line">                            &lt;/filters&gt;</span><br><span class="line">                            &lt;transformers&gt;</span><br><span class="line">                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;</span><br><span class="line">                                    &lt;mainClass&gt;&lt;/mainClass&gt;</span><br><span class="line">                                &lt;/transformer&gt;</span><br><span class="line">                            &lt;/transformers&gt;</span><br><span class="line">                        &lt;/configuration&gt;</span><br><span class="line">                    &lt;/execution&gt;</span><br><span class="line">                &lt;/executions&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure><p>保存后自动添加jar依赖包，非常方便。</p><h2 id="0x02-编译UDF的jar包"><a href="#0x02-编译UDF的jar包" class="headerlink" title="0x02 编译UDF的jar包"></a>0x02 编译UDF的jar包</h2><p>在Maven项目下，src &gt; main &gt; java，新建 Package，再新建 UDF 函数类，编写逻辑代码；接下来有两种方式编译jar包，</p><ul><li>Maven工具箱，Lifecycle &gt; clean &gt; compile &gt; package</li><li>Terminal命令行，mvn clean &gt; mvn compile &gt; mvn package</li></ul><p>操作完之后，会在Maven工程下生成target目录，里面包含两个jar包：一个是集成了所有jar依赖包，比较大；另一个是没有集成jar依赖包，只有源代码，比较小。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/11/03/hive-udf/">Hive自定义函数UDF</a><br><a href="https://jordenbruce.com/2019/11/06/hive-genericudf/">Hive自定义函数GenericUDF</a><br><a href="https://jordenbruce.com/2019/11/07/hive-genericudtf/">Hive自定义函数GenericUDTF</a><br><a href="https://jordenbruce.com/2019/11/09/hive-genericudaf/">Hive自定义函数GenericUDAF</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Intellij IDEA作为一种IDE开发工具，能有效提升编码效率。Maven的核心功能是合理叙述项目间的依赖关系，通过pom.xml文件的配置获取jar依赖包，而不用手动添加。那么，使用Intellij IDEA与Maven如何帮助我们快速地开发Hive UDF函数呢？&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive存储格式与压缩</title>
    <link href="https://jordenbruce.com/2019/12/20/hive-fileformat-compress/"/>
    <id>https://jordenbruce.com/2019/12/20/hive-fileformat-compress/</id>
    <published>2019-12-20T08:03:34.000Z</published>
    <updated>2019-12-20T10:13:50.119Z</updated>
    
    <content type="html"><![CDATA[<p>采用Hive作为数据仓库工具，由于数仓既要存储来自不同系统的数据源，还要执行ETL数据任务，所以Hive不仅提供了多种存储格式（TEXTFILE，ORC，PARQUET），降低了存储成本，而且还支持多种压缩方式（zlib，snappy，bzip2），提高了计算效率。那么，不同的业务场景该选择哪一种存储格式或压缩方式呢？<br><a id="more"></a></p><h2 id="0x00-存储格式"><a href="#0x00-存储格式" class="headerlink" title="0x00 存储格式"></a>0x00 存储格式</h2><p>TEXTFILE是Hive默认的存储格式，存储空间消耗比较大，并且压缩的text无法分割和合并，查询的效率最低，可以直接存储，加载数据的速度最高；</p><p>ORC存储格式是一种Hadoop生态圈中的列式存储格式，它的产生早在2013年初，最初产生自Apache Hive，用于降低Hadoop数据存储空间和加速Hive查询速度。</p><p>Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定，目前能够和Parquet适配的组件很多，基本上通常使用的查询引擎和计算框架都已适配，并且可以很方便的将其它序列化工具生成的数据转换成Parquet格式。</p><table><thead><tr><th>文件格式</th><th>存储方式</th><th>压缩比</th><th>查询速度</th><th>文件拆分</th><th>支持压缩</th></tr></thead><tbody><tr><td>TEXTFILE</td><td>行式</td><td>小</td><td>慢</td><td>不可切分</td><td>NONE, GZIP</td></tr><tr><td>ORC</td><td>列式</td><td>大</td><td>快</td><td>可切分</td><td>NONE, ZLIB, SNAPPY</td></tr><tr><td>PARQUET</td><td>列式</td><td>中</td><td>快</td><td>可切分</td><td>NONE, SNAPPY，BZIP2</td></tr></tbody></table><p>Hive创建表时，使用 stored as [file_format] 关键字指定存储格式。</p><h2 id="0x01-压缩方式"><a href="#0x01-压缩方式" class="headerlink" title="0x01 压缩方式"></a>0x01 压缩方式</h2><p>1403M的原文件，不同压缩方式的特点如下：</p><table><thead><tr><th>压缩方式</th><th>压缩大小</th><th>压缩时间</th><th>解压时间</th><th>文件切分</th></tr></thead><tbody><tr><td>Snappy</td><td>701M</td><td>6.4s</td><td>19.8s</td><td>不可切分</td></tr><tr><td>LZ4</td><td>693M</td><td>6.4s</td><td>2.36s</td><td>不可切分</td></tr><tr><td>LZO</td><td>684M</td><td>7.6s</td><td>11.1s</td><td>带序号可切分</td></tr><tr><td>GZIP</td><td>447M</td><td>85.6s</td><td>21.8s</td><td>不可切分</td></tr><tr><td>BZIP2</td><td>390M</td><td>142.3s</td><td>62.5s</td><td>可切分</td></tr></tbody></table><p>Hive选择MapReduce作为执行引擎时，不同阶段适合不同的压缩方式，建议如下：</p><p><img src="https://i.loli.net/2019/12/20/D1UKXEWt5rGRNqu.png" alt="MapReduce与压缩"></p><p>通过上图可以看出，有三次的压缩，对应有不同的参数如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">---1)Map端的输出压缩</span><br><span class="line">set mapreduce.map.output.compress=true;</span><br><span class="line">set mapreduce.map.output.compress.codec=org.apache.Hadoop.io.compress.SnappyCodec;</span><br><span class="line">---2)Reduce端的输出压缩</span><br><span class="line">set hive.exec.compress.output=true;</span><br><span class="line">set mapreduce.output.fileoutputformat.compress=org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br><span class="line">---3)Job之间的输出压缩</span><br><span class="line">set hive.exec.compress.intermediate=true;</span><br><span class="line">set mapreduce.output.fileoutputformat.compress=org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line">---4)其他</span><br><span class="line">set hive.exec.orc.default.compress=ZLIB;</span><br><span class="line">set hive.exec.orc.compression.strategy=SPEED;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/FileFormats" target="_blank" rel="noopener">File Formats and Compression</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties" target="_blank" rel="noopener">Hive Configuration Properties</a><br><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml" target="_blank" rel="noopener">mapred-default.xml</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;采用Hive作为数据仓库工具，由于数仓既要存储来自不同系统的数据源，还要执行ETL数据任务，所以Hive不仅提供了多种存储格式（TEXTFILE，ORC，PARQUET），降低了存储成本，而且还支持多种压缩方式（zlib，snappy，bzip2），提高了计算效率。那么，不同的业务场景该选择哪一种存储格式或压缩方式呢？&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive元数据的解析</title>
    <link href="https://jordenbruce.com/2019/12/12/hive-metastore/"/>
    <id>https://jordenbruce.com/2019/12/12/hive-metastore/</id>
    <published>2019-12-12T10:21:51.000Z</published>
    <updated>2019-12-12T11:31:46.159Z</updated>
    
    <content type="html"><![CDATA[<p>Hive体系结构的元数据（Metastore）是一个重要的组件，保存了Hive有关库、表、存储、分区等信息。元数据主要包括两个方面：一方面是元数据库，最常见的是采用MySQL；另一方面是元数据服务，与其他查询引擎共享，比如Presto或Impala等。<br><a id="more"></a></p><h2 id="0x00-Hive元数据库"><a href="#0x00-Hive元数据库" class="headerlink" title="0x00 Hive元数据库"></a>0x00 Hive元数据库</h2><p>Hive支持两种类型的元数据库：</p><ul><li>本地或嵌入的元数据库：Derby</li><li>远程的元数据库：MySQL</li></ul><p>说明：嵌入的元数据库主要用于单元测试，并且一次只能有一个进程来连接，所以生产环境不推荐使用。实际上，线上使用最多的是采用MySQL作为远程的元数据库。</p><p>（1）配置Hive元数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://master:3306/hive?characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;mysql&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>（2）元数据库E-R图</p><p><img src="https://i.loli.net/2019/12/12/I24QhomPgeUJVNb.jpg" alt="Hive Metastore"></p><p>（3）元数据库的表说明</p><p><img src="https://i.loli.net/2019/12/12/mJMhpey5fqgbAkl.png" alt="Hive Metastore Tables"></p><h2 id="0x01-Hive元数据服务"><a href="#0x01-Hive元数据服务" class="headerlink" title="0x01 Hive元数据服务"></a>0x01 Hive元数据服务</h2><p>尽管Hive元数据服务也支持两种方式，可是生产环境采用的是MySQL作为元数据库，所以这里只介绍远程服务的配置与启动。</p><p>（1）配置Hive元数据服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">&lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>（2）启动Hive元数据服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service hivestore &amp;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration" target="_blank" rel="noopener">Metastore Administration</a><br><a href="https://jordenbruce.com/2019/11/02/hive-architecture/">Hive体系结构</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive体系结构的元数据（Metastore）是一个重要的组件，保存了Hive有关库、表、存储、分区等信息。元数据主要包括两个方面：一方面是元数据库，最常见的是采用MySQL；另一方面是元数据服务，与其他查询引擎共享，比如Presto或Impala等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的Select语句之GroupBy子句</title>
    <link href="https://jordenbruce.com/2019/12/12/hql-select-groupby/"/>
    <id>https://jordenbruce.com/2019/12/12/hql-select-groupby/</id>
    <published>2019-12-12T03:37:03.000Z</published>
    <updated>2019-12-12T04:44:15.058Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道Select语句能访问存储在Hive表中的数据，做聚合查询时总是会使用到GroupBy子句，常见的聚合函数有count(),sum(),max()等。其实，GroupBy子句不仅包括基本的聚合作用，还可以做OLAP查询，比如cube,rollup,grouping sets等，统计根据不同维度上卷或下钻的指标。<br><a id="more"></a></p><h2 id="0x00-基本GroupBy聚合查询"><a href="#0x00-基本GroupBy聚合查询" class="headerlink" title="0x00 基本GroupBy聚合查询"></a>0x00 基本GroupBy聚合查询</h2><p>需求场景：统计每个每个客户的消费次数，消费金额，最大一次消费金额，最小一次消费金额，平均消费金额。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select order_date, user_name</span><br><span class="line">      ,count(*)    as total_cnt</span><br><span class="line">      ,sum(amount) as total_amt</span><br><span class="line">      ,max(amount) as max_amt</span><br><span class="line">      ,min(amount) as min_amt</span><br><span class="line">      ,avg(amount) as avg_amt</span><br><span class="line">from order_window</span><br><span class="line">group by order_date, user_name</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x01-高级GroupBy分析查询"><a href="#0x01-高级GroupBy分析查询" class="headerlink" title="0x01 高级GroupBy分析查询"></a>0x01 高级GroupBy分析查询</h2><p>官方支持的高级GroupBy功能有：</p><ul><li>CUBE子句：实现任意维度组合的分组聚合</li><li>ROLLUP子句：实现从右到左递减多级的分组聚合</li><li>GROUPING SETS子句：依据指定分组进行聚合</li><li>Grouping__ID函数：表示聚合结果属于哪一个分组集合</li><li>Grouping()函数：表示当前行是否参与了该分组的聚合</li></ul><p>需求场景：统计每天每个客户的消费金额的数据魔方，满足上卷与下钻。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">select order_date, user_name</span><br><span class="line">      ,grouping__id</span><br><span class="line">      ,sum(amount) as total_amt</span><br><span class="line">from order_window</span><br><span class="line">group by order_date, user_name</span><br><span class="line">    with cube</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">select order_date, user_name</span><br><span class="line">      ,grouping__id</span><br><span class="line">      ,sum(amount) as total_amt</span><br><span class="line">from order_window</span><br><span class="line">group by order_date, user_name</span><br><span class="line">    with rollup</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">select order_date, user_name</span><br><span class="line">      ,grouping__id</span><br><span class="line">      ,sum(amount) as total_amt</span><br><span class="line">from order_window</span><br><span class="line">group by order_date, user_name</span><br><span class="line">    grouping sets(</span><br><span class="line">     (order_date, user_name)</span><br><span class="line">    ,(order_date)</span><br><span class="line">    )</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x02-GroupBy子句的参数调优"><a href="#0x02-GroupBy子句的参数调优" class="headerlink" title="0x02 GroupBy子句的参数调优"></a>0x02 GroupBy子句的参数调优</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---MAP端聚合</span><br><span class="line">set hive.map.aggr=true;</span><br><span class="line">---发生倾斜时进行负载均衡</span><br><span class="line">set hive.groupby.skewindata=true;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation%2C+Cube%2C+Grouping+and+Rollup" target="_blank" rel="noopener">Enhanced Aggregation, Cube, Grouping and Rollup</a><br><a href="https://blog.csdn.net/mashroomxl/article/details/22578471" target="_blank" rel="noopener">Hive.GROUPING SETS</a><br><a href="https://blog.csdn.net/lzm1340458776/article/details/43231707" target="_blank" rel="noopener">Hive group by操作</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们知道Select语句能访问存储在Hive表中的数据，做聚合查询时总是会使用到GroupBy子句，常见的聚合函数有count(),sum(),max()等。其实，GroupBy子句不仅包括基本的聚合作用，还可以做OLAP查询，比如cube,rollup,grouping sets等，统计根据不同维度上卷或下钻的指标。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的分析函数</title>
    <link href="https://jordenbruce.com/2019/12/09/hql-function-analytic/"/>
    <id>https://jordenbruce.com/2019/12/09/hql-function-analytic/</id>
    <published>2019-12-09T12:10:53.000Z</published>
    <updated>2019-12-09T13:25:48.004Z</updated>
    
    <content type="html"><![CDATA[<p>在 Hive 中做数据分析或 OLAP 查询，除了使用上一篇提到的窗口函数之外，还有一批分析函数，主要用于排名、排序、分组等。当然，在不同的实际业务场景下，分析函数有着不同的表达，功能更加丰富。<br><a id="more"></a></p><h2 id="0x00-常用的分析函数"><a href="#0x00-常用的分析函数" class="headerlink" title="0x00 常用的分析函数"></a>0x00 常用的分析函数</h2><p>下表列出了一些分析函数以及描述信息：</p><table><thead><tr><th>分析函数</th><th>描述</th></tr></thead><tbody><tr><td>RANK</td><td>返回数据项在分区中的排名。排名值序列可能会有间隔</td></tr><tr><td>DENSE_RANK</td><td>返回数据项在分区中的排名。排名值序列是连续的，不会有间隔</td></tr><tr><td>PERCENT_RANK</td><td>计算当前行的百分比排名</td></tr><tr><td>CUME_DIST</td><td>计算分区中当前行的相对排名</td></tr><tr><td>ROW_NUMBER</td><td>确定分区中当前行的序号</td></tr><tr><td>NTILE</td><td>将每个分区的行尽可能均匀地划分为指定数量的分组</td></tr></tbody></table><h2 id="0x01-分析函数的使用要点"><a href="#0x01-分析函数的使用要点" class="headerlink" title="0x01 分析函数的使用要点"></a>0x01 分析函数的使用要点</h2><p>对比上一篇的窗口函数，分析函数的使用要点有：</p><ul><li>必须结合 over + order by 一起使用</li><li>不能使用 window 子句</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select gid, dt, pv</span><br><span class="line">      ,rank() over(partition by gid order by pv) as rank_1</span><br><span class="line">      ,dense_rank() over(partition by gid order by pv) as rank_2</span><br><span class="line">      ,percent_rank() over(partition by gid order by pv) as rank_3</span><br><span class="line">      ,cume_dist() over(partition by gid order by pv) as rank_4</span><br><span class="line">      ,row_number() over(partition by gid order by pv) as row_no</span><br><span class="line">      ,ntile(2) over(partition by gid order by pv) as bucket_no</span><br><span class="line">from page_analytic</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x02-分析函数的业务场景"><a href="#0x02-分析函数的业务场景" class="headerlink" title="0x02 分析函数的业务场景"></a>0x02 分析函数的业务场景</h2><p>给了用户和每个用户对应的消费信息表，计算花费前50%的用户的平均消费。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">---把用户和消费表，按消费下降顺序平均分成2份</span><br><span class="line">drop table if exists test_by_payment_ntile;</span><br><span class="line">create table test_by_payment_ntile as</span><br><span class="line">select nick</span><br><span class="line">      ,payment</span><br><span class="line">      ,ntile(2) over(order by payment desc) as rn </span><br><span class="line">from test_nick_payment;</span><br><span class="line"></span><br><span class="line">---分别对每一份计算平均值，就可以得到消费靠前50%和后50%的平均消费</span><br><span class="line">select &apos;avg_payment&apos; as inf</span><br><span class="line">      ,t1.avg_payment_up_50 as avg_payment_up_50</span><br><span class="line">      ,t2.avg_payment_down_50 as avg_payment_down_50</span><br><span class="line">from (</span><br><span class="line">    select avg(payment) as avg_payment_up_50 </span><br><span class="line">    from test_by_payment_ntile </span><br><span class="line">    where rn = 1</span><br><span class="line">) t1</span><br><span class="line">join (</span><br><span class="line">    select avg(payment) as avg_payment_down_50 </span><br><span class="line">    from test_by_payment_ntile </span><br><span class="line">    where rn = 2</span><br><span class="line">) t2</span><br><span class="line">on t1.dp_id = t2.dp_id</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">Windowing and Analytics Functions</a><br><a href="https://blog.csdn.net/SunnyYoona/article/details/56488568" target="_blank" rel="noopener">分析函数 RANK ROW_NUMBER CUME_DIST CUME_DIST</a><br><a href="https://blog.csdn.net/zhangxianx1an/article/details/80609514" target="_blank" rel="noopener">Hive分析函数–Ntile</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Hive 中做数据分析或 OLAP 查询，除了使用上一篇提到的窗口函数之外，还有一批分析函数，主要用于排名、排序、分组等。当然，在不同的实际业务场景下，分析函数有着不同的表达，功能更加丰富。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的窗口函数</title>
    <link href="https://jordenbruce.com/2019/12/08/hql-function-window/"/>
    <id>https://jordenbruce.com/2019/12/08/hql-function-window/</id>
    <published>2019-12-08T07:55:00.000Z</published>
    <updated>2019-12-09T13:06:16.278Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道在 SQL 中有一类函数叫做聚合函数，例如 sum()、avg()、max() 等等，这类函数可以将多行数据按照规则聚集为一行，一般来讲聚集后的行数是要少于聚集前的行数的。但是，有时候我们既要显示聚集前的数据，又要显示聚集后的数据，此时我们便引入了窗口函数。窗口函数主要用于 OLAP 数据分析。<br><a id="more"></a></p><h2 id="0x00-说在前面"><a href="#0x00-说在前面" class="headerlink" title="0x00 说在前面"></a>0x00 说在前面</h2><p>本文采用的数据表是 order_window(user_name, order_date, amount)，详细如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jack    2015-01-01    10</span><br><span class="line">tony    2015-01-02    15</span><br><span class="line">jack    2015-02-03    23</span><br><span class="line">tony    2015-01-04    29</span><br><span class="line">jack    2015-01-05    46</span><br><span class="line">jack    2015-04-06    42</span><br><span class="line">tony    2015-01-07    50</span><br><span class="line">jack    2015-01-08    55</span><br><span class="line">mart    2015-04-08    62</span><br><span class="line">mart    2015-04-09    68</span><br><span class="line">neil    2015-05-10    12</span><br><span class="line">mart    2015-04-11    75</span><br><span class="line">neil    2015-06-12    80</span><br><span class="line">mart    2015-04-13    94</span><br></pre></td></tr></table></figure><p>在深入研究 over子句 之前，一定要注意：在SQL处理中，窗口函数都是最后一步执行，而且仅位于order by字句之前。</p><h2 id="0x01-OVER子句"><a href="#0x01-OVER子句" class="headerlink" title="0x01 OVER子句"></a>0x01 OVER子句</h2><p>官方 OVER子句 包括几个部分：</p><ul><li>聚合函数（count, sum, min, max, avg）</li><li>OVER 子句</li><li>PARTITION BY 子句</li><li>ORDER BY 子句</li><li>WINDOW 子句</li></ul><p>结合具体的业务场景，SQL 语句如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">---1)201504月份的销售额</span><br><span class="line">select sum(amount) as total_amt</span><br><span class="line">from order_window </span><br><span class="line">where substr(order_date,1,7)=&apos;2015-04&apos;</span><br><span class="line">;</span><br><span class="line">---2)201504月份的订单明细与销售额</span><br><span class="line">select user_name, order_date, amount</span><br><span class="line">      ,sum(amount) over() as total_amt</span><br><span class="line">from order_window</span><br><span class="line">where substr(order_date,1,7)=&apos;2015-04&apos;</span><br><span class="line">;</span><br><span class="line">---3)客户的订单明细与月购买金额</span><br><span class="line">select user_name, order_date, amount</span><br><span class="line">      ,sum(amount) over (partition by month(order_date)) month_amt</span><br><span class="line">from order_window</span><br><span class="line">;</span><br><span class="line">---4)客户的订单明细与累计购买金额</span><br><span class="line">select user_name, order_date, amount</span><br><span class="line">      ,sum(amount) over (partition by month(order_date) order by order_date) month_add_amt</span><br><span class="line">from order_window</span><br><span class="line">;</span><br><span class="line">---5)不同窗口的销售额</span><br><span class="line">select </span><br><span class="line">     user_name</span><br><span class="line">    ,order_date</span><br><span class="line">    ,amount</span><br><span class="line">    ,sum(amount) over() as sample1 --所有行相加</span><br><span class="line">    ,sum(amount) over(partition by user_name) as sample2 --按name分组，组内数据相加</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date) as sample3 --按name分组，组内数据累加</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date rows between UNBOUNDED PRECEDING and current row) as sample4 --和sample3一样,由起点到当前行的聚合</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date rows between 1 PRECEDING and current row) as sample5 --当前行和前面一行做聚合</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date rows between 1 PRECEDING and 1 FOLLOWING) as sample6 --当前行和前边一行及后面一行</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date rows between current row and UNBOUNDED FOLLOWING) as sample7 --当前行及后面所有行</span><br><span class="line">from order_window</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x02-WINDOW子句"><a href="#0x02-WINDOW子句" class="headerlink" title="0x02 WINDOW子句"></a>0x02 WINDOW子句</h2><p>带有窗口规范的OVER子句。窗口可以在WINDOW子句中单独定义。窗口规范支持如下格式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure><table><thead><tr><th>关键字</th><th>说明</th></tr></thead><tbody><tr><td>PRECEDING</td><td>表示当前行之前的行</td></tr><tr><td>UNBOUNDED PRECEDING</td><td>表示当前行之前无边界行，即第一行</td></tr><tr><td>num PRECEDING</td><td>表示当前行之前第num行</td></tr><tr><td>CURRENT ROW</td><td>表示当前行</td></tr><tr><td>FOLLOWING</td><td>表示当前行后面的行</td></tr><tr><td>UNBOUNDED FOLLOWING</td><td>表示当前行后面无边界行，即最后一行</td></tr><tr><td>num FOLLOWING</td><td>表示当前行后面第num行</td></tr></tbody></table><p>当缺少WINDOW子句并指定使用ORDER BY时，窗口规范默认为RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW，即从第一行到当前行。</p><p>当缺少ORDER BY和WINDOW子句时，窗口规范默认为ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING，即第一行到最后一行。</p><h2 id="0x03-窗口函数"><a href="#0x03-窗口函数" class="headerlink" title="0x03 窗口函数"></a>0x03 窗口函数</h2><table><thead><tr><th>窗口函数</th><th>描述</th></tr></thead><tbody><tr><td>LAG()</td><td>返回分区中当前行之前行（可以指定第几行）的值。如果没有行，则返回null。</td></tr><tr><td>LEAD()</td><td>返回分区中当前行后面行（可以指定第几行）的值。如果没有行，则返回null。</td></tr><tr><td>FIRST_VALUE</td><td>返回相对于窗口中第一行的指定列的值。</td></tr><tr><td>LAST_VALUE</td><td>返回相对于窗口中最后一行的指定列的值。</td></tr></tbody></table><p>结合具体的业务场景，SQL 语句如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select user_name, order_date, amount</span><br><span class="line">      ,lag(order_date,1,&apos;1900-01-01&apos;) over(partition by user_name order by order_date) as pre_order_date --当前订单的上一单日期</span><br><span class="line">      ,lead(order_date,1,&apos;9999-12-31&apos;) over(partition by user_name order by order_date) as fol_order_date --当前订单的下一单日期</span><br><span class="line">      ,first_value(amount,true) over (partition by user_name order by order_date) as first_order_amt --截至当前订单的第一单金额</span><br><span class="line">      ,last_value(amount,true) over (partition by user_name order by order_date) as last_order_amt --截至当前订单的最后一单金额</span><br><span class="line">from order_window</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">Windowing and Analytics Functions</a><br><a href="https://blog.csdn.net/qq_26937525/article/details/54925827" target="_blank" rel="noopener">Hive窗口函数</a><br><a href="https://yq.aliyun.com/articles/632198" target="_blank" rel="noopener">窗口函数与分析函数</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们知道在 SQL 中有一类函数叫做聚合函数，例如 sum()、avg()、max() 等等，这类函数可以将多行数据按照规则聚集为一行，一般来讲聚集后的行数是要少于聚集前的行数的。但是，有时候我们既要显示聚集前的数据，又要显示聚集后的数据，此时我们便引入了窗口函数。窗口函数主要用于 OLAP 数据分析。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Yarn三种Scheduler调度器</title>
    <link href="https://jordenbruce.com/2019/11/17/hadoop-yarn-scheduler/"/>
    <id>https://jordenbruce.com/2019/11/17/hadoop-yarn-scheduler/</id>
    <published>2019-11-17T11:11:58.000Z</published>
    <updated>2019-11-22T13:36:56.415Z</updated>
    
    <content type="html"><![CDATA[<p>理想情况下，我们应用对 Yarn 资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在 Yarn 中，负责给应用分配资源的是 Scheduler 调度器；而调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此，Yarn 提供了三种调度器和可配置的策略供我们选择。<br><a id="more"></a></p><h2 id="0x00-FIFO-Scheduler"><a href="#0x00-FIFO-Scheduler" class="headerlink" title="0x00 FIFO Scheduler"></a>0x00 FIFO Scheduler</h2><p>FIFO Scheduler 把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。 </p><p><img src="https://i.loli.net/2019/11/17/z4G5bl3PgFjEV87.png" alt="FIFO Scheduler"></p><p>FIFO Scheduler 它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。</p><h2 id="0x01-Capacity-Scheduler"><a href="#0x01-Capacity-Scheduler" class="headerlink" title="0x01 Capacity Scheduler"></a>0x01 Capacity Scheduler</h2><p>Capacity 调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p><p><img src="https://i.loli.net/2019/11/17/PEaQlMzkgs4vbeq.png" alt="Capacity Scheduler"></p><p>Capacity 调度器允许多个组织共享整个集群，每个组织可以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。除此之外，队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略。 </p><p>当队列已满，Capacity 调度器不会强制释放Container，当一个队列资源不够用时，这个队列只能获得其它队列释放后的Container资源，这个称为“弹性队列”，也可以设置最大值，防止过多占用其他队列的资源。 </p><h2 id="0x02-Fair-Scheduler"><a href="#0x02-Fair-Scheduler" class="headerlink" title="0x02 Fair Scheduler"></a>0x02 Fair Scheduler</h2><p>Fair 调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如下图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。 </p><p><img src="https://i.loli.net/2019/11/17/Y97D6UeqdlLRivj.png" alt="Fair Scheduler"></p><p>需要注意的是，在上图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器既提高了的资源利用率又能保证小任务及时完成。 </p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="noopener">Hadoop: Capacity Scheduler</a><br><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">Hadoop: Fair Scheduler</a><br><a href="https://blog.csdn.net/suifeng3051/article/details/49508261" target="_blank" rel="noopener">Yarn 调度器Scheduler详解</a><br><a href="https://jordenbruce.com/2019/10/16/hadoop-yarn/">YARN技术原理</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;理想情况下，我们应用对 Yarn 资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在 Yarn 中，负责给应用分配资源的是 Scheduler 调度器；而调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此，Yarn 提供了三种调度器和可配置的策略供我们选择。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="yarn" scheme="https://jordenbruce.com/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>Hive数据倾斜之GroupBy</title>
    <link href="https://jordenbruce.com/2019/11/16/hive-skew-group/"/>
    <id>https://jordenbruce.com/2019/11/16/hive-skew-group/</id>
    <published>2019-11-16T11:36:45.000Z</published>
    <updated>2019-11-17T04:02:26.213Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 中分组聚合的常用子句是 Group By；在执行过程中，将 Group By 的字段组合为map的输出key值，利用 MapReduce 的排序，在reduce阶段保存LastKey区分不同的key；如果 Group By 的字段组合出现了数据分布不均匀，就会导致 Hive 数据倾斜；另外 Group By 一般与聚合函数一起使用，比如 SUM() 和 COUNT() 等。<br><a id="more"></a></p><h2 id="0x00-Group-By-典型场景"><a href="#0x00-Group-By-典型场景" class="headerlink" title="0x00 Group By 典型场景"></a>0x00 Group By 典型场景</h2><p>假设有一张表 dwd_bhv_log_di(logid,uuid,city,provice,dt)，需求是统计每个省份的UV与PV；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---常规实现方式</span><br><span class="line">select provice, count(distinct uuid) uv, count(logid) pv</span><br><span class="line">from dwd_bhv_log_di</span><br><span class="line">where dt = &apos;2019-11-10&apos;</span><br><span class="line">group by provice;</span><br></pre></td></tr></table></figure><h2 id="0x01-Group-By-优化"><a href="#0x01-Group-By-优化" class="headerlink" title="0x01 Group By 优化"></a>0x01 Group By 优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">set hive.map.aggr=true;</span><br><span class="line">set mapred.reduce.tasks=10;</span><br><span class="line">set hive.groupby.skewdata=true;</span><br><span class="line"></span><br><span class="line">select provice, sum(part_uv) uv, sum(part_pv) pv</span><br><span class="line">from (</span><br><span class="line">  select provice, part, count(uuid) part_uv, sum(part_pv) part_pv</span><br><span class="line">  from (</span><br><span class="line">    select provice, uuid, count(logid) part_pv, ceil(rand()*100) part</span><br><span class="line">from dwd_bhv_log_di</span><br><span class="line">where dt = &apos;2019-11-10&apos;</span><br><span class="line">group by provice, uuid</span><br><span class="line">  ) t1</span><br><span class="line">  group by provice, part</span><br><span class="line">) t2</span><br><span class="line">group by provice;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/10/13/hadoop-mapreduce/">MapReduce执行过程详解</a><br><a href="https://jordenbruce.com/2019/09/26/hql-select/">HiveQL的Select语句</a><br><a href="https://blog.csdn.net/u013668852/article/details/79866931" target="_blank" rel="noopener">Hive Group By的实现原理</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 中分组聚合的常用子句是 Group By；在执行过程中，将 Group By 的字段组合为map的输出key值，利用 MapReduce 的排序，在reduce阶段保存LastKey区分不同的key；如果 Group By 的字段组合出现了数据分布不均匀，就会导致 Hive 数据倾斜；另外 Group By 一般与聚合函数一起使用，比如 SUM() 和 COUNT() 等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive数据倾斜之Join</title>
    <link href="https://jordenbruce.com/2019/11/16/hive-skew-join/"/>
    <id>https://jordenbruce.com/2019/11/16/hive-skew-join/</id>
    <published>2019-11-16T05:49:36.000Z</published>
    <updated>2019-11-17T03:59:18.156Z</updated>
    
    <content type="html"><![CDATA[<p>按照 Kimball 维度模型设计出来的数据表，在开发 ETL 的过程中，不可避免地会使用到多表 Join 关联。在 Join 关联两个表时，每个表 Map 端输出的 key 会带上表别名标识，经过 shuffle 分发到 Reduce 端就有可能会出现数据分布不均匀，从而造成数据倾斜，影响业务的数据质量。然而，相对 count(distinct) 来说，Join 产生数据倾斜的场景会多一些，本文将会介绍几种 Join 典型场景并给出优化实践。<br><a id="more"></a></p><h2 id="0x00-说在前面"><a href="#0x00-说在前面" class="headerlink" title="0x00 说在前面"></a>0x00 说在前面</h2><p>假设有两张表：一张表是用户打开APP列表 device_open_app(device,appid)；另一张表是作弊用户表 device_cheat(device)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select a.device, a.appid, if(b.device is null,0,1) as is_cheat</span><br><span class="line">from device_open_app a</span><br><span class="line">left join device_cheat b</span><br><span class="line">on a.device = b.device</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x01-通用-Join-优化方式"><a href="#0x01-通用-Join-优化方式" class="headerlink" title="0x01 通用 Join 优化方式"></a>0x01 通用 Join 优化方式</h2><ul><li>做好列裁剪和filter操作，以达到两表做join时数据量相对较小；</li><li>关联时小表在前大表在后；</li><li>确保关联条件的字段类型是一致的；</li><li>选择join key分布最均匀的表作为驱动表；</li><li>设置在map端聚合，set hive.map.aggr=true(默认开启)；</li><li>可选set hive.optimize.skewjoin=false(默认关闭)；</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">set hive.map.aggr=true;</span><br><span class="line">set hive.optimize.skewjoin=true;</span><br><span class="line"></span><br><span class="line">select a.device, b.appid</span><br><span class="line">from (</span><br><span class="line">  select device</span><br><span class="line">  from device_cheat</span><br><span class="line">  where device is not null</span><br><span class="line">) a</span><br><span class="line">inner join (</span><br><span class="line">  select device, appid</span><br><span class="line">  from device_open_app</span><br><span class="line">  where device is not null</span><br><span class="line">) b</span><br><span class="line">on cast(a.device as string) = b.device</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x02-小表与大表-Join-优化方式"><a href="#0x02-小表与大表-Join-优化方式" class="headerlink" title="0x02 小表与大表 Join 优化方式"></a>0x02 小表与大表 Join 优化方式</h2><p>这是一种典型的场景，Hive 使用 MapJoin 方式对关联进行优化。</p><p><img src="https://i.loli.net/2019/11/16/UqHT9vapmeWKRfF.png" alt="MapJoin 原理"></p><p>MapJoin 简单说就是在 Map 阶段将小表读入分布式缓存，然后顺序扫描大表完成 Join。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---在0.7.0版本之后</span><br><span class="line">set hive.auto.convert.join=true;</span><br><span class="line"></span><br><span class="line">select /*+MAPJOIN(b)*/ a.device, a.appid</span><br><span class="line">  ,if(b.device is null,0,1) as is_cheat</span><br><span class="line">from device_open_app a</span><br><span class="line">left join device_cheat b</span><br><span class="line">on a.device = b.device</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x03-大表与大表-Join-优化方式"><a href="#0x03-大表与大表-Join-优化方式" class="headerlink" title="0x03 大表与大表 Join 优化方式"></a>0x03 大表与大表 Join 优化方式</h2><p>这种关联的场景比较多，最常见的场景是业务数据本身的特性，比如关联字段有大量的空值或特殊值；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">set hive.auto.convert.join=true;</span><br><span class="line"></span><br><span class="line">select a.device, a.appid, if(b.device is null,0,1) as is_cheat</span><br><span class="line">from (</span><br><span class="line">  select device, appid</span><br><span class="line">  from device_open_app</span><br><span class="line">  where device is not null</span><br><span class="line">) a</span><br><span class="line">left join (</span><br><span class="line">  select device</span><br><span class="line">  from device_cheat</span><br><span class="line">  where device is not null</span><br><span class="line">) b</span><br><span class="line">on if(a.device=&apos;0&apos;, concat(&apos;hive&apos;,rand()), a.device) = cast(b.device as string)</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/10/13/hadoop-mapreduce/">MapReduce执行过程详解</a><br><a href="https://jordenbruce.com/2019/09/26/hql-select/">HiveQL的Select语句</a><br><a href="http://ju.outofmemory.cn/entry/786" target="_blank" rel="noopener">Hive – JOIN实现过程</a><br><a href="https://blog.csdn.net/yeweiouyang/article/details/45665727" target="_blank" rel="noopener">Hive数据倾斜（大表join大表）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;按照 Kimball 维度模型设计出来的数据表，在开发 ETL 的过程中，不可避免地会使用到多表 Join 关联。在 Join 关联两个表时，每个表 Map 端输出的 key 会带上表别名标识，经过 shuffle 分发到 Reduce 端就有可能会出现数据分布不均匀，从而造成数据倾斜，影响业务的数据质量。然而，相对 count(distinct) 来说，Join 产生数据倾斜的场景会多一些，本文将会介绍几种 Join 典型场景并给出优化实践。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive数据倾斜之Distinct</title>
    <link href="https://jordenbruce.com/2019/11/16/hive-skew-distinct/"/>
    <id>https://jordenbruce.com/2019/11/16/hive-skew-distinct/</id>
    <published>2019-11-16T01:56:26.000Z</published>
    <updated>2019-11-17T03:57:02.623Z</updated>
    
    <content type="html"><![CDATA[<p>在 Hive 选择 MapReduce 为执行引擎的前提下，由于使用了 distinct，导致在 map 端的 combine 无法合并重复数据；对于这种 count() 全聚合操作时，即使设定了 reduce task 个数，hive 也只会启动一个 reducer。这就造成了所有 map 端传来的数据都在一个 task 中执行，成为了性能瓶颈。<br><a id="more"></a></p><h2 id="0x00-单字段-count-distinct-优化"><a href="#0x00-单字段-count-distinct-优化" class="headerlink" title="0x00 单字段 count(distinct) 优化"></a>0x00 单字段 count(distinct) 优化</h2><p>典型场景：某一天的日志数据有上亿条，计算当天的活跃用户数；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">---常规实现</span><br><span class="line">select count(distinct uuid) from log where dt = &apos;2019-11-15&apos;;</span><br><span class="line"></span><br><span class="line">---1.优化方式一</span><br><span class="line">select count(*) from (</span><br><span class="line">  select distinct uuid from log where dt = &apos;2019-11-15&apos;</span><br><span class="line">) t;</span><br><span class="line"></span><br><span class="line">---2.优化方式二</span><br><span class="line">select count(*) from (</span><br><span class="line">  select uuid </span><br><span class="line">  from log where dt = &apos;2019-11-15&apos;</span><br><span class="line">  group by uuid</span><br><span class="line">) t;</span><br><span class="line"></span><br><span class="line">---3.优化方式三</span><br><span class="line">select sum(part)</span><br><span class="line">from (</span><br><span class="line">  select tag, count(*) as part</span><br><span class="line">  from (</span><br><span class="line">    select uuid, cast(rand() * 100 as bigint) as tag</span><br><span class="line">    from log where dt = &apos;2019-11-15&apos; </span><br><span class="line">    group by uuid</span><br><span class="line">  ) t1</span><br><span class="line">  group by tag</span><br><span class="line">) t2;</span><br><span class="line"></span><br><span class="line">---辅助优化参数</span><br><span class="line">set mapreduce.job.reduces=100;</span><br></pre></td></tr></table></figure><h2 id="0x01-多字段-count-distinct-优化"><a href="#0x01-多字段-count-distinct-优化" class="headerlink" title="0x01 多字段 count(distinct) 优化"></a>0x01 多字段 count(distinct) 优化</h2><p>典型场景：某一天的日志数据有上亿条，计算当天的活跃设备数和活跃用户数；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">---常规实现</span><br><span class="line">select count(distinct device), count(distinct member)</span><br><span class="line">from log where dt = &apos;2019-11-15&apos;;</span><br><span class="line"></span><br><span class="line">---通用优化方式</span><br><span class="line">select count(device), count(member)</span><br><span class="line">from (</span><br><span class="line">    select device, null as member</span><br><span class="line">    from log where dt = &apos;2019-11-15&apos; </span><br><span class="line">    group by device</span><br><span class="line">  union all</span><br><span class="line">    select null as device, member</span><br><span class="line">    from log where dt = &apos;2019-11-15&apos; </span><br><span class="line">    group by member</span><br><span class="line">) t;</span><br><span class="line"></span><br><span class="line">---辅助优化参数</span><br><span class="line">set mapreduce.job.reduces=100;</span><br></pre></td></tr></table></figure><h2 id="0x02-优化思路"><a href="#0x02-优化思路" class="headerlink" title="0x02 优化思路"></a>0x02 优化思路</h2><p>由于 Hive 的执行引擎是 MapReduce，而 MapReduce 的设计思想是分而治之，所以解决数据倾斜的根本是将 map 输出数据经过 hash 均匀地分配到 reduce 中，再进行聚合等。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/10/13/hadoop-mapreduce/">MapReduce执行过程详解</a><br><a href="https://jordenbruce.com/2019/09/26/hql-select/">HiveQL的Select语句</a><br><a href="http://ju.outofmemory.cn/entry/784" target="_blank" rel="noopener">Hive – Distinct 的实现</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Hive 选择 MapReduce 为执行引擎的前提下，由于使用了 distinct，导致在 map 端的 combine 无法合并重复数据；对于这种 count() 全聚合操作时，即使设定了 reduce task 个数，hive 也只会启动一个 reducer。这就造成了所有 map 端传来的数据都在一个 task 中执行，成为了性能瓶颈。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>ShuffleError - error in shuffle in fetcher</title>
    <link href="https://jordenbruce.com/2019/11/09/hive-shuffle-oom/"/>
    <id>https://jordenbruce.com/2019/11/09/hive-shuffle-oom/</id>
    <published>2019-11-09T13:41:04.000Z</published>
    <updated>2019-11-16T04:42:36.024Z</updated>
    
    <content type="html"><![CDATA[<p>最近开发的一个 HQL 脚本，扔到集群上跑的时候报 shuffle 阶段出现 OOM 的错误，这个问题之前没有遇到过；于是上网搜了下，发现网友也遇到过类似的问题，详细阅读后再结合 MapReduce 任务的 shuffle 原理，将问题解决的过程记录下来，方便以后查阅。<br><a id="more"></a></p><h2 id="0x00-报错信息"><a href="#0x00-报错信息" class="headerlink" title="0x00 报错信息"></a>0x00 报错信息</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">- Task with the most failures(4): </span><br><span class="line">- -----</span><br><span class="line">- Task ID:</span><br><span class="line">- task_1548652929194_403817_r_000214</span><br><span class="line">- </span><br><span class="line">- URL:</span><br><span class="line">- http://0.0.0.0:8088/taskdetails.jsp?jobid=job_1548652929194_403817&amp;tipid=task_1548652929194_403817_r_000214</span><br><span class="line">- -----</span><br><span class="line">- Diagnostic Messages for this Task:</span><br><span class="line">- Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#19</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">- at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)</span><br><span class="line">- at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)</span><br><span class="line">- at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">- at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">- at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1727)</span><br><span class="line">- at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</span><br><span class="line">- Caused by: java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">- at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56)</span><br><span class="line">- at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:305)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:295)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:514)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)</span><br></pre></td></tr></table></figure><h2 id="0x01-原因分析"><a href="#0x01-原因分析" class="headerlink" title="0x01 原因分析"></a>0x01 原因分析</h2><p>(1) 文件拷贝：默认情况下，当整个 MapReduce 作业的所有已执行完成的 Map Task 任务数超过 Map Task 总数的 mapreduce.job.reduce.slowstart.completedmaps (默认为0.05) 后，ApplicationMaster 便会开始调度执行 Reduce Task 任务。然后 Reduce Task 任务默认启动 mapreduce.reduce.shuffle.parallelcopies (默认为5) 个 MapOutputCopier 线程到已完成的 Map Task 任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</p><p>(2) 内存缓冲区：这个内存缓冲区大小的控制就不像map那样可以通过 mapreduce.task.io.sort.mb 来设定了，而是通过另外一个参数来设置：mapreduce.reduce.shuffle.input.buffer.percent（default 0.7）， 这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。</p><p>(3) 原则上，mapreduce.reduce.shuffle.input.buffer.percent * mapreduce.reduce.shuffle.parallelcopies 必须小于等于1，否则就会出现如上错误。但是，这两个参数默认值的乘积为3.5，远远超过了1，为什么没有经常抛出以上的错误呢？</p><ul><li>首先，把默认值设为比较大，主要是基于性能考虑，将它们设为比较大，可以大大加快从map复制数据的速度；</li><li>其次，要抛出如上异常，还需满足另外一个条件，就是map任务的数据一下子准备好了等待shuffle去复制，在这种情况下，就会导致shuffle过程的“线程数量”和“内存buffer使用量”都是满负荷的值，自然就造成了内存不足的错误；而如果map任务的数据是断断续续完成的，那么没有一个时刻shuffle过程的“线程数量”和“内存buffer使用量”是满负荷值的，自然也就不会抛出如上错误。</li></ul><h2 id="0x02-调优方式"><a href="#0x02-调优方式" class="headerlink" title="0x02 调优方式"></a>0x02 调优方式</h2><p>通过上述的原因分析，调优方式如下：</p><ul><li>将 mapreduce.reduce.shuffle.input.buffer.percent 参数调小，辅助参数 mapreduce.reduce.shuffle.parallelcopies；</li><li>将 mapreduce.reduce.java.opts 参数调大，辅助参数 mapreduce.reduce.memory.mb；</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/11/09/hive-oom/">HQL内存溢出的参数调优</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近开发的一个 HQL 脚本，扔到集群上跑的时候报 shuffle 阶段出现 OOM 的错误，这个问题之前没有遇到过；于是上网搜了下，发现网友也遇到过类似的问题，详细阅读后再结合 MapReduce 任务的 shuffle 原理，将问题解决的过程记录下来，方便以后查阅。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HQL内存溢出的参数调优</title>
    <link href="https://jordenbruce.com/2019/11/09/hive-oom/"/>
    <id>https://jordenbruce.com/2019/11/09/hive-oom/</id>
    <published>2019-11-09T13:04:49.000Z</published>
    <updated>2019-11-17T11:36:10.220Z</updated>
    
    <content type="html"><![CDATA[<p>我们在使用 Hive 进行 ETL 开发的过程中，关注更多的是使用 HQL 语言来准确地表达业务逻辑，而很少考虑到 Hive 对 HQL 语句的执行情况。当你辛辛苦苦地码完，将 HQL 语句扔给 Hive 去执行时，就有可能出现各种各样的报错，而其中一种比较常见的错误就是内存溢出（OOM，out of memory），通俗地讲就是内存不够。<br><a id="more"></a></p><h2 id="0x00-写在前面"><a href="#0x00-写在前面" class="headerlink" title="0x00 写在前面"></a>0x00 写在前面</h2><p>本文采用的软件版本如下：</p><ul><li>hive-2.0.1</li><li>hadoop-2.7.2</li></ul><p>Hive 使用 MapReduce 执行引擎，Hadoop 使用 Yarn 进行资源调度。</p><p>接下来将从客户端提交 HQL 语句开始，Hive 生成物理执行计划、Yarn 资源分配、MapReduce 执行，到执行结束，三个重点过程进行阐述，先理论再参数，希望 OOM 参数调优的问题得到收敛。</p><h2 id="0x01-Hive-生成物理执行计划"><a href="#0x01-Hive-生成物理执行计划" class="headerlink" title="0x01 Hive 生成物理执行计划"></a>0x01 Hive 生成物理执行计划</h2><p>先给出官网上关于 Hive 架构的经典流程图：</p><p><img src="https://i.loli.net/2019/02/24/5c72ad01e0c56.png" alt="Hive架构"></p><p>从这张图中，我们只需要明白一点即可：客户端提交的HQL语句，在Hive端的最终输出是物理执行计划，或者说是Job的有向无环图（a DAG of stages）。主要包括三种操作：</p><ul><li>MapReduce作业（a map/reduce job）</li><li>元数据操作（a metadata operation）</li><li>HDFS操作（an operation on HDFS）</li></ul><p>需要说明的是：这个过程主要是Hive优化器的执行，没有相关参数去控制内存的使用。当然，对于某些HQL语句适当地设置一些参数，可以得到更优的物理执行计划。比如常见的 Map Join 参数 <code>hive.auto.convert.join</code> 等。</p><h2 id="0x02-Yarn-资源分配"><a href="#0x02-Yarn-资源分配" class="headerlink" title="0x02 Yarn 资源分配"></a>0x02 Yarn 资源分配</h2><p>YARN 是对 Mapreduce V1 重构得到的，有时候也成为 MapReduce V2。由Hive生成物理执行计划，其中的MapReduce作业提交给Yarn来执行，详细的执行过程如下：</p><p><img src="https://i.loli.net/2019/02/24/5c72b4b3294ea.jpg" alt="MapReduce在Yarn下执行过程"></p><p>从上图可以看出，Yarn（以Container方式分配）控制着 NodeManager、ApplicationMaster、Map和Reduce 的内存使用，相关的内存参数有：</p><table><thead><tr><th>重要级别</th><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>中</td><td>yarn.nodemanager.resource.memory-mb</td><td>8192</td></tr><tr><td>中</td><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td></tr><tr><td>中</td><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td></tr><tr><td>中</td><td>yarn.scheduler.increment-allocation-mb</td><td>1024</td></tr><tr><td>高</td><td>yarn.app.mapreduce.am.resource.mb</td><td>1536</td></tr><tr><td>中</td><td>yarn.app.mapreduce.am.command-opts</td><td>-Xmx1024m</td></tr><tr><td>低</td><td>yarn.app.mapreduce.am.admin-command-opts</td><td></td></tr><tr><td>中</td><td>yarn.nodemanager.vmem-pmem-ratio</td><td>2.1</td></tr><tr><td>低</td><td>yarn.nodemanager.pmem-check-enabled</td><td>true</td></tr><tr><td>低</td><td>yarn.nodemanager.vmem-check-enabled</td><td>true</td></tr><tr><td>高</td><td>mapreduce.reduce.memory.mb</td><td>1024</td></tr><tr><td>中</td><td>mapreduce.reduce.java.opts</td><td></td></tr><tr><td>高</td><td>mapreduce.map.memory.mb</td><td>1024</td></tr><tr><td>中</td><td>mapreduce.map.java.opts</td></tr></tbody></table><h3 id="1-基础"><a href="#1-基础" class="headerlink" title="(1) 基础"></a>(1) 基础</h3><ul><li>NodeManager可用于分配的最大内存是yarn.nodemanager.resource.memory-mb；</li><li>Yarn的ResourceManger（简称RM）通过逻辑上的队列分配内存等资源给application，默认情况下RM允许最大AM申请Container资源为8192MB(“yarn.scheduler.maximum-allocation-mb“)，默认情况下的最小分配资源为1024M(“yarn.scheduler.minimum-allocation-mb“)，如果参数中需要的资源在此范围之外，在任务submit的时候会被直接拒绝掉；</li><li>AM只能以增量 (“yarn.scheduler.minimum-allocation-mb”) + (“yarn.scheduler.increment-allocation-mb”) 规整每个task需要的内存，并且申请的内存只能在（”yarn.scheduler.minimum-allocation-mb“）和(“yarn.scheduler.maximum-allocation-mb“) 的范围内向RM申请资源；</li><li>每个Map任务或Reduce任务分配的内存为mapreduce.reduce.memory.mb或mapreduce.map.memory.mb；</li></ul><h3 id="2-mapreduce-map-java-opts-和-mapreduce-map-memory-mb-区别"><a href="#2-mapreduce-map-java-opts-和-mapreduce-map-memory-mb-区别" class="headerlink" title="(2) mapreduce.map.java.opts 和 mapreduce.map.memory.mb 区别"></a>(2) mapreduce.map.java.opts 和 mapreduce.map.memory.mb 区别</h3><p>JVM进程跑在container中，mapreduce.map.java.opts 能够通过Xmx设置JVM最大的heap的使用，一般设置为0.75倍的 mapreduce.map.memory.mb ，因为需要为java code，非JVM内存使用等预留些空间；mapreduce.reduce.java.opts 和 mapreduce.reduce.memory.mb 同理。</p><h3 id="3-虚拟内存"><a href="#3-虚拟内存" class="headerlink" title="(3) 虚拟内存"></a>(3) 虚拟内存</h3><p>默认的(“yarn.nodemanager.vmem-pmem-ratio“)设置为2.1，意味则 map container 或者 reduce container 分配的虚拟内存超过2.1倍的(“mapreduce.reduce.memory.mb“)或(“mapreduce.map.memory.mb“)就会被NM给KILL掉，如果 (“mapreduce.map.memory.mb”) 被设置为1536M那么总的虚拟内存为2.1*1536=3225.6MB</p><h3 id="4-内存检查"><a href="#4-内存检查" class="headerlink" title="(4) 内存检查"></a>(4) 内存检查</h3><p>如果虚拟内存检查被打开（yarn.nodemanager.vmem-check-enabled 默认情况下为true），然后YARN将把抽取出来的容器及其子进程的VSIZE加起来和容器最大允许使用的虚拟内存进行比较。最大允许使用的虚拟内存是容器最大可使用的物理内存乘以 yarn.nodemanager.vmem-pmem-ratio（默认值是2.1）。所以，如果你的YARN容器配置的最大可使用物理内存为2GB，然后我们乘以 2.1 得到的就是容器最大可用的虚拟内存 4.2G 。</p><p>如果物理内存检查被打开（yarn.nodemanager.pmem-check-enabled 默认情况为true），然后YARN将把抽取出来的容器及其子进程的RSS加起来和容器最大允许使用的物理内存进行比较。</p><p>如果物理内存或者虚拟内存其中一个的使用大于最大允许使用的，YARN将会被这个容器杀掉。</p><h3 id="5-参数全局图"><a href="#5-参数全局图" class="headerlink" title="(5) 参数全局图"></a>(5) 参数全局图</h3><p>参数多不要慌，下面来张图梳理下：</p><p><img src="https://i.loli.net/2019/02/25/5c72c07905cf3.jpg" alt="Yarn内存参数"></p><h2 id="0x03-MapReduce执行"><a href="#0x03-MapReduce执行" class="headerlink" title="0x03 MapReduce执行"></a>0x03 MapReduce执行</h2><p>MapReduce作业的重点是Shuffle过程，还是老套路，先给出官网上关于这个过程的经典流程图：</p><p><img src="https://i.loli.net/2019/11/09/Rh5CrkgyBeT6XSV.png" alt="Shuffle过程"></p><p>当Map任务或Reduce任务以Container方式申请到相应的内存资源后，就进入了实际的执行过程中，其中涉及的参数有：</p><table><thead><tr><th>重要级别</th><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>高</td><td>mapreduce.job.maps</td><td>2</td></tr><tr><td>中</td><td>mapreduce.input.fileinputformat.split.minsize</td><td>1</td></tr><tr><td>中</td><td>dfs.blocksize</td><td>134217728</td></tr><tr><td>高</td><td>mapreduce.job.reduces</td><td>1</td></tr><tr><td>中</td><td>mapreduce.task.io.sort.mb</td><td>100</td></tr><tr><td>中</td><td>mapreduce.map.sort.spill.percent</td><td>0.80</td></tr><tr><td>中</td><td>mapreduce.task.io.sort.factor</td><td>10</td></tr><tr><td>中</td><td>mapreduce.map.output.compress</td><td>false</td></tr><tr><td>中</td><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>低</td><td>mapreduce.job.reduce.slowstart.completedmaps</td><td>0.05</td></tr><tr><td>中</td><td>mapreduce.reduce.shuffle.parallelcopies</td><td>5</td></tr><tr><td>高</td><td>mapreduce.reduce.shuffle.input.buffer.percent</td><td>0.70</td></tr></tbody></table><p>为了更好地理解每个参数作用的阶段，建议先阅读 <a href="http://matt33.com/2016/03/02/hadoop-shuffle/" target="_blank" rel="noopener">MapReduce之Shuffle过程详解</a>。</p><h3 id="1-Map任务"><a href="#1-Map任务" class="headerlink" title="(1) Map任务"></a>(1) Map任务</h3><p>（1）split分片：split是在逻辑上对输入数据进行的分片，并不会在磁盘上将其切分成分片进行存储。每个split都作为一个独立单位分配给一个map task去处理。决定split分片大小的参数有：</p><ul><li>mapreduce.job.maps</li><li>mapreduce.input.fileinputformat.split.minsize</li><li>dfs.blocksize (会话级别不可设置)</li></ul><p>（2）内存缓冲区：经过map处理后的键值对，不会立马写入磁盘，而是暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，设置缓冲区大小的参数有：</p><ul><li>mapreduce.task.io.sort.mb</li><li>mapreduce.map.sort.spill.percent</li></ul><p>（3）压缩：map端在写磁盘的时候采用压缩的方式将map的输出结果进行压缩是一个减少网络开销很有效的方法。其实，在Hadoop中早已为我们提供了一些压缩算法的实现，直接配置参数即可。</p><ul><li>mapreduce.map.output.compress</li><li>mapreduce.map.output.compress.codec</li></ul><h3 id="2-Reduce任务"><a href="#2-Reduce任务" class="headerlink" title="(2) Reduce任务"></a>(2) Reduce任务</h3><p>（1）文件拷贝：默认情况下，当整个MapReduce作业的所有已执行完成的Map Task任务数超过Map Task总数的 <code>mapreduce.job.reduce.slowstart.completedmaps</code> (默认为0.05) 后，ApplicationMaster便会开始调度执行Reduce Task任务。然后Reduce Task任务默认启动 <code>mapred.reduce.parallel.copies</code> (默认为5) 个MapOutputCopier线程到已完成的Map Task任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</p><p>（2）内存缓冲区：这个内存缓冲区大小的控制就不像map那样可以通过 <code>mapreduce.task.io.sort.mb</code> 来设定了，而是通过另外一个参数来设置：<code>mapred.job.shuffle.input.buffer.percent</code>（default 0.7）， 这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。</p><h2 id="0x04-HQL语句的日志输出"><a href="#0x04-HQL语句的日志输出" class="headerlink" title="0x04 HQL语句的日志输出"></a>0x04 HQL语句的日志输出</h2><p>经过漫长的理论铺垫，终于要到解决问题的时候了，HQL语句的内存溢出主要从日志分析开始。</p><ul><li>HQL语句的执行过程中，有哪些日志输出呢？分别存放在什么地方？如何分析出有用信息？</li><li>内存溢出包括哪几类？典型日志有哪些？调优什么参数可以解决？</li><li>小文件太多是如何产生的？调优什么参数可以合并小文件？</li></ul><p>等等一系列有关问题，我们下一次接着说。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Design" target="_blank" rel="noopener">Hive Architecture Overview</a><br><a href="https://segmentfault.com/a/1190000003777237" target="_blank" rel="noopener">Yarn下Mapreduce的内存参数理解</a><br><a href="https://blog.csdn.net/lazythinker/article/details/75497774" target="_blank" rel="noopener">HIVE参数调优（汇总）</a><br><a href="https://blog.csdn.net/aijiudu/article/details/72353510" target="_blank" rel="noopener">MapReduce过程详解及其性能优化</a><br><a href="https://my.oschina.net/OttoWu/blog/816049" target="_blank" rel="noopener">hadoop fair scheduler 的坑</a><br><a href="https://blog.csdn.net/suifeng3051/article/details/48135521" target="_blank" rel="noopener">Yarn 内存分配管理机制及相关参数配置</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在使用 Hive 进行 ETL 开发的过程中，关注更多的是使用 HQL 语言来准确地表达业务逻辑，而很少考虑到 Hive 对 HQL 语句的执行情况。当你辛辛苦苦地码完，将 HQL 语句扔给 Hive 去执行时，就有可能出现各种各样的报错，而其中一种比较常见的错误就是内存溢出（OOM，out of memory），通俗地讲就是内存不够。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数GenericUDAF</title>
    <link href="https://jordenbruce.com/2019/11/09/hive-genericudaf/"/>
    <id>https://jordenbruce.com/2019/11/09/hive-genericudaf/</id>
    <published>2019-11-09T11:37:59.000Z</published>
    <updated>2019-11-09T11:41:45.847Z</updated>
    
    <content type="html"><![CDATA[<p>UDAF 是 Hive 中用户自定义的聚合函数，特点是输入多行输出一行，Hive 内置 UDAF 函数包括有 sum() 与 count() 等。UDAF 实现有简单与通用两种方式，简单 UDAF 因为使用 Java 反射导致性能损失，而且有些特性不能使用，已经被弃用了；在本文中我们将关注 Hive 中自定义聚合函数-GenericUDAF，即通用方式。<br><a id="more"></a></p><h2 id="0x00-自定义-GenericUDAF-开发"><a href="#0x00-自定义-GenericUDAF-开发" class="headerlink" title="0x00 自定义 GenericUDAF 开发"></a>0x00 自定义 GenericUDAF 开发</h2><p>编写 GenericUDAF 需要下面两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver</code> ，重写 <code>getEvaluator</code> 函数；</li><li>依据 <code>getEvaluator</code> 函数返回值，编写内部静态类，继承 <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator</code>，并重写其7个方法；</li></ul><p>上述过程涉及两个重要抽象类 Resolver 与 Evaluator，其中 Resolver 官方不建议使用 GenericUDAFResolver2 接口，而使用 AbstractGenericUDAFResolver 接口；Evaluator 使用 GenericUDAFEvaluator 接口，并且必须是 public static 子类，需要重写的方法有 <code>init</code>，<code>getNewAggregationBuffer</code>，<code>iterate</code>，<code>terminatePartial</code>，<code>merge</code>，<code>terminate</code>，<code>reset</code> 共7个；理解 Evaluator 之前，必须先理解 ObjectInspector 接口 与 Model 内部类；另外，UDAF 逻辑处理主要发生在 Evaluator 中。</p><p>ObjectInspector 作用主要是解耦数据使用与数据格式，使得数据流在输入输出端切换不同的输入输出格式，不同的 Operator 上使用不同的格式。</p><p>Model 代表了 UDAF 在 MapReduce 的各个阶段，具体如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public static enum Mode &#123;</span><br><span class="line">  /**</span><br><span class="line">   * PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合</span><br><span class="line">   * 将会调用iterate()和terminatePartial()</span><br><span class="line">   */</span><br><span class="line">  PARTIAL1,</span><br><span class="line">  /**</span><br><span class="line">   * PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合:</span><br><span class="line">   * 将会调用merge() 和 terminatePartial() </span><br><span class="line">   */</span><br><span class="line">  PARTIAL2,</span><br><span class="line">  /**</span><br><span class="line">   * FINAL: mapreduce的reduce阶段:从部分数据的聚合到完全聚合 </span><br><span class="line">   * 将会调用merge()和terminate()</span><br><span class="line">   */</span><br><span class="line">  FINAL,</span><br><span class="line">  /**</span><br><span class="line">   * COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了:从原始数据直接到完全聚合</span><br><span class="line">   * 将会调用 iterate()和terminate()</span><br><span class="line">   */</span><br><span class="line">  COMPLETE</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>一般情况下，完整的 UDAF 逻辑是一个 mapreduce 过程，如果有 mapper 和 reducer，就会经历 PARTIAL1(mapper)，FINAL(reducer)，如果还有 combiner，那就会经历 PARTIAL1(mapper)，PARTIAL2(combiner)，FINAL(reducer)。而有一些情况下的 mapreduce，只有 mapper 没有 reducer，所以就会只有 COMPLETE 阶段，这个阶段直接输入原始数据，出结果。</p><p><img src="https://i.loli.net/2019/11/09/xHGvRTEm5O9jk34.png" alt="Model各阶段对应Evaluator方法调用"></p><h2 id="0x01-散度-DivergenceGenericUDAF-示例代码"><a href="#0x01-散度-DivergenceGenericUDAF-示例代码" class="headerlink" title="0x01 散度 DivergenceGenericUDAF 示例代码"></a>0x01 散度 DivergenceGenericUDAF 示例代码</h2><p><img src="https://i.loli.net/2019/11/09/nGODqcvX7Ywr3xi.png" alt="散度数学公式"></p><p>上述公式的参考代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;</span><br><span class="line">import org.apache.hadoop.hive.serde2.io.DoubleWritable;</span><br><span class="line">import org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.ql.util.JavaDataModel;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">        name = &quot;divergence&quot;,</span><br><span class="line">        value = &quot;_FUNC_(px, qy) - Calculate divergence from px, qy &quot;)</span><br><span class="line">public class DivergenceGenericUDAF extends AbstractGenericUDAFResolver &#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException &#123;</span><br><span class="line">        if (parameters.length != 2) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(parameters.length - 1,</span><br><span class="line">                    &quot;Exactly two arguments are expected.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (parameters[0].getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(0,</span><br><span class="line">                    &quot;Only primitive type arguments are accepted but &quot;</span><br><span class="line">                            + parameters[0].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (parameters[1].getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(1,</span><br><span class="line">                    &quot;Only primitive type arguments are accepted but &quot;</span><br><span class="line">                            + parameters[1].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        switch (((PrimitiveTypeInfo) parameters[0]).getPrimitiveCategory()) &#123;</span><br><span class="line">            case BYTE:</span><br><span class="line">            case SHORT:</span><br><span class="line">            case INT:</span><br><span class="line">            case LONG:</span><br><span class="line">            case FLOAT:</span><br><span class="line">            case DOUBLE:</span><br><span class="line">            case TIMESTAMP:</span><br><span class="line">            case DECIMAL:</span><br><span class="line">                switch (((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()) &#123;</span><br><span class="line">                    case BYTE:</span><br><span class="line">                    case SHORT:</span><br><span class="line">                    case INT:</span><br><span class="line">                    case LONG:</span><br><span class="line">                    case FLOAT:</span><br><span class="line">                    case DOUBLE:</span><br><span class="line">                    case TIMESTAMP:</span><br><span class="line">                    case DECIMAL:</span><br><span class="line">                        return new GenericUDAFDivergenceEvaluator();</span><br><span class="line">                    case STRING:</span><br><span class="line">                    case BOOLEAN:</span><br><span class="line">                    case DATE:</span><br><span class="line">                    default:</span><br><span class="line">                        throw new UDFArgumentTypeException(1,</span><br><span class="line">                                &quot;Only numeric or string type arguments are accepted but &quot;</span><br><span class="line">                                        + parameters[1].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">            case STRING:</span><br><span class="line">            case BOOLEAN:</span><br><span class="line">            case DATE:</span><br><span class="line">            default:</span><br><span class="line">                throw new UDFArgumentTypeException(0,</span><br><span class="line">                        &quot;Only numeric or string type arguments are accepted but &quot;</span><br><span class="line">                                + parameters[0].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class GenericUDAFDivergenceEvaluator extends GenericUDAFEvaluator &#123;</span><br><span class="line"></span><br><span class="line">        private PrimitiveObjectInspector pInputOI;</span><br><span class="line">        private PrimitiveObjectInspector qInputOI;</span><br><span class="line"></span><br><span class="line">        private PrimitiveObjectInspector partialOI;</span><br><span class="line"></span><br><span class="line">        private Object partialResult;</span><br><span class="line"></span><br><span class="line">        private DoubleWritable result;</span><br><span class="line"></span><br><span class="line">        /*</span><br><span class="line">         * PARTIAL1  (input, partial)</span><br><span class="line">         * PARTIAL2  (partial, partial)</span><br><span class="line">         * FINAL     (partial, output)</span><br><span class="line">         * COMPLETE  (input, output)</span><br><span class="line">         */</span><br><span class="line">        @Override</span><br><span class="line">        public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException &#123;</span><br><span class="line">            super.init(m, parameters);</span><br><span class="line">            result = new DoubleWritable(0);</span><br><span class="line">            // initialize input</span><br><span class="line">            if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) &#123;</span><br><span class="line">                assert (parameters.length == 2);</span><br><span class="line">                pInputOI = (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">                qInputOI = (PrimitiveObjectInspector) parameters[1];</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                partialOI = (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">            &#125;</span><br><span class="line">            // initialize output</span><br><span class="line">            if (m == Mode.PARTIAL1 || m == Mode.PARTIAL2) &#123;</span><br><span class="line">                partialOI = (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">                partialResult = new DoubleWritable(0);</span><br><span class="line">                return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                partialResult = new DoubleWritable(0);</span><br><span class="line">                return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        static class SumDoubleAgg extends AbstractAggregationBuffer &#123;</span><br><span class="line">            boolean empty;</span><br><span class="line">            double sum;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public int estimate() &#123; return JavaDataModel.PRIMITIVES1 + JavaDataModel.PRIMITIVES2; &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void reset(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line">            divergenceAgg.empty = true;</span><br><span class="line">            divergenceAgg.sum = 0.0;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = new SumDoubleAgg();</span><br><span class="line">            reset(divergenceAgg);</span><br><span class="line">            return divergenceAgg;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException &#123;</span><br><span class="line">            assert (parameters.length == 2);</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line"></span><br><span class="line">            double vp = PrimitiveObjectInspectorUtils.getDouble(parameters[0], pInputOI);</span><br><span class="line">            double vq = PrimitiveObjectInspectorUtils.getDouble(parameters[1], qInputOI);</span><br><span class="line"></span><br><span class="line">            divergenceAgg.sum += vp * Math.log(vp / vq);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public Object terminatePartial(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line">            partialResult = new DoubleWritable(divergenceAgg.sum);</span><br><span class="line">            return partialResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void merge(AggregationBuffer agg, Object partial) throws HiveException &#123;</span><br><span class="line">            if (partial != null) &#123;</span><br><span class="line">                SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line"></span><br><span class="line">                double subSum = PrimitiveObjectInspectorUtils.getDouble(partial, partialOI);</span><br><span class="line">                divergenceAgg.sum += subSum;</span><br><span class="line"></span><br><span class="line">                divergenceAgg.empty = false;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public Object terminate(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line">            if (divergenceAgg.empty) &#123;</span><br><span class="line">                result = null;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                result.set(divergenceAgg.sum);</span><br><span class="line">            &#125;</span><br><span class="line">            return result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x02-代码走读"><a href="#0x02-代码走读" class="headerlink" title="0x02 代码走读"></a>0x02 代码走读</h2><ol><li><p>getEvaluator() 被调用<br>1.1 检查输入参数的个数；<br>1.2 检查输入参数的类型；<br>1.3 根据不同的输入参数类型，返回对应的 GenericUDAFEvaluator；</p></li><li><p>类 GenericUDAFDivergenceEvaluator 的方法说明<br>2.1 init() 方法，定义 mapreduce 不同阶段的输入与输出；<br>2.2 getNewAggregationBuffer() 方法，获取新的中间结果；<br>2.3 iterate() 方法，读取输入行的 p,q 累加至中间结果；<br>2.4 terminatePartial() 方法，输出中间结果；<br>2.5 merge() 方法，聚合中间结果；<br>2.6 terminate() 方法，输出聚合结果；<br>2.7 reset() 方法，重置中间结果；</p></li><li><p>方法调用过程（参考 Model各阶段对应Evaluator方法调用）</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy" target="_blank" rel="noopener">GenericUDAFCaseStudy</a><br><a href="https://blog.csdn.net/kent7306/article/details/50110067" target="_blank" rel="noopener">Hive UDAF开发详解</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;UDAF 是 Hive 中用户自定义的聚合函数，特点是输入多行输出一行，Hive 内置 UDAF 函数包括有 sum() 与 count() 等。UDAF 实现有简单与通用两种方式，简单 UDAF 因为使用 Java 反射导致性能损失，而且有些特性不能使用，已经被弃用了；在本文中我们将关注 Hive 中自定义聚合函数-GenericUDAF，即通用方式。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数GenericUDTF</title>
    <link href="https://jordenbruce.com/2019/11/07/hive-genericudtf/"/>
    <id>https://jordenbruce.com/2019/11/07/hive-genericudtf/</id>
    <published>2019-11-07T12:18:20.000Z</published>
    <updated>2019-11-07T14:20:12.045Z</updated>
    
    <content type="html"><![CDATA[<p>之前介绍的 UDF 特点是输入一行输出一行；本文将要介绍的是 UDTF，其特点是输入一行输出多行，而使用的接口是 GenericUDTF，比 UDF 更为复杂。<br><a id="more"></a></p><h2 id="0x00-自定义-GenericUDTF-开发"><a href="#0x00-自定义-GenericUDTF-开发" class="headerlink" title="0x00 自定义 GenericUDTF 开发"></a>0x00 自定义 GenericUDTF 开发</h2><p>编写 GenericUDTF 需要两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF</code> 类；</li><li>重写 <code>initialize</code> ，<code>process</code> ，<code>close</code> 三个方法；</li></ul><p>每个方法有着不同的作用，参考如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 该方法指定输入输出参数：输入参数的ObjectInspector与输出参数的StructObjectInspector</span><br><span class="line">abstract StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException; </span><br><span class="line"> </span><br><span class="line">// 该方法处理输入记录，然后通过forward()方法返回输出结果</span><br><span class="line">abstract void process(Object[] record) throws HiveException;</span><br><span class="line"> </span><br><span class="line">// 该方法用于通知UDTF没有行可以处理了 </span><br><span class="line">// 另外，可以在该方法中清理代码或者产生额外的输出</span><br><span class="line">abstract void close() throws HiveException;</span><br></pre></td></tr></table></figure><p>其中 process() 方法中有个 forward() 方法需要解释下，对于每一行的输入都会有多行的输出，每一行输出时都要调用 forward() 方法，并定义输出行的格式（一列或多列）。</p><h2 id="0x01-官方-ExplodeGenericUDTF-示例代码"><a href="#0x01-官方-ExplodeGenericUDTF-示例代码" class="headerlink" title="0x01 官方 ExplodeGenericUDTF 示例代码"></a>0x01 官方 ExplodeGenericUDTF 示例代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.Map.Entry;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.TaskExecutionException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">        name = &quot;explode&quot;,</span><br><span class="line">        value = &quot;_FUNC_(a) - separates the elements of array a into multiple rows,&quot;</span><br><span class="line">                + &quot; or the elements of a map into multiple rows and columns &quot;)</span><br><span class="line">public class ExplodeGenericUDTF extends GenericUDTF &#123;</span><br><span class="line"></span><br><span class="line">    private transient ObjectInspector inputOI = null;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close() throws HiveException &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException &#123;</span><br><span class="line">        if (args.length != 1) &#123;</span><br><span class="line">            throw new UDFArgumentException(&quot;explode() takes only one argument&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;();</span><br><span class="line">        ArrayList&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;();</span><br><span class="line"></span><br><span class="line">        switch (args[0].getCategory()) &#123;</span><br><span class="line">            case LIST:</span><br><span class="line">                inputOI = args[0];</span><br><span class="line">                fieldNames.add(&quot;col&quot;);</span><br><span class="line">                fieldOIs.add(((ListObjectInspector)inputOI).getListElementObjectInspector());</span><br><span class="line">                break;</span><br><span class="line">            case MAP:</span><br><span class="line">                inputOI = args[0];</span><br><span class="line">                fieldNames.add(&quot;key&quot;);</span><br><span class="line">                fieldNames.add(&quot;value&quot;);</span><br><span class="line">                fieldOIs.add(((MapObjectInspector)inputOI).getMapKeyObjectInspector());</span><br><span class="line">                fieldOIs.add(((MapObjectInspector)inputOI).getMapValueObjectInspector());</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">                throw new UDFArgumentException(&quot;explode() takes an array or a map as a parameter&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private transient final Object[] forwardListObj = new Object[1];</span><br><span class="line">    private transient final Object[] forwardMapObj = new Object[2];</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void process(Object[] o) throws HiveException &#123;</span><br><span class="line">        switch (inputOI.getCategory()) &#123;</span><br><span class="line">            case LIST:</span><br><span class="line">                ListObjectInspector listOI = (ListObjectInspector)inputOI;</span><br><span class="line">                List&lt;?&gt; list = listOI.getList(o[0]);</span><br><span class="line">                if (list == null) &#123;</span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">                for (Object r : list) &#123;</span><br><span class="line">                    forwardListObj[0] = r;</span><br><span class="line">                    forward(forwardListObj); //输出一行</span><br><span class="line">                &#125;</span><br><span class="line">                break;</span><br><span class="line">            case MAP:</span><br><span class="line">                MapObjectInspector mapOI = (MapObjectInspector)inputOI;</span><br><span class="line">                Map&lt;?,?&gt; map = mapOI.getMap(o[0]);</span><br><span class="line">                if (map == null) &#123;</span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">                for (Entry&lt;?,?&gt; r : map.entrySet()) &#123;</span><br><span class="line">                    forwardMapObj[0] = r.getKey();</span><br><span class="line">                    forwardMapObj[1] = r.getValue();</span><br><span class="line">                    forward(forwardMapObj); //输出一行</span><br><span class="line">                &#125;</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">                throw new TaskExecutionException(&quot;explode() can only operate on an array or a map&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return &quot;explode&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x02-代码走读"><a href="#0x02-代码走读" class="headerlink" title="0x02 代码走读"></a>0x02 代码走读</h2><p>方法的调用关系如下：</p><ol><li><p>initialize() 被调用<br>1.1 检查输入参数的个数<br>1.2 检查参数的类型是否为 LIST 或 MAP，并保存 inputOI 用以供 process() 使用<br>1.3 返回 StructObjectInspector，并定义好输出行的格式</p></li><li><p>process() 被调用<br>2.1. 判断输入参数的数据类型<br>2.2. 对于 LIST 类型，每一个元素输出一行，且只有一列<br>2.3. 对于 MAP 类型，每一对键值输出一行，且有两列<br>2.4. 其他数据类型抛出异常</p></li><li><p>close() 被调用，什么都不做</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide+UDTF" target="_blank" rel="noopener">DeveloperGuide UDTF</a><br><a href="https://jordenbruce.com/2019/09/30/hql-function/">HiveQL的函数概览</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前介绍的 UDF 特点是输入一行输出一行；本文将要介绍的是 UDTF，其特点是输入一行输出多行，而使用的接口是 GenericUDTF，比 UDF 更为复杂。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数GenericUDF</title>
    <link href="https://jordenbruce.com/2019/11/06/hive-genericudf/"/>
    <id>https://jordenbruce.com/2019/11/06/hive-genericudf/</id>
    <published>2019-11-06T13:39:44.000Z</published>
    <updated>2019-11-07T14:02:38.818Z</updated>
    
    <content type="html"><![CDATA[<p>编写 Apache Hive 用户自定义函数（UDF）有两个不同的接口：一个是非常简单的 UDF，上一篇已经介绍过了；还有一个是 GenericUDF，相对复杂点。两个 API 的区别是：如果函数的参数和返回都是基础数据类型，那么简单 API（UDF）可以胜任；但是，如果你想写一个函数用来操作内嵌数据结构，如 Map、List 和 Set，此时就需要去熟悉复杂 API（GenericUDF）。<br><a id="more"></a></p><h2 id="0x00-自定义-GenericUDF-开发"><a href="#0x00-自定义-GenericUDF-开发" class="headerlink" title="0x00 自定义 GenericUDF 开发"></a>0x00 自定义 GenericUDF 开发</h2><p>编写 GenericUDF 需要两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</code> 类；</li><li>重写 <code>initialize</code> ，<code>evaluate</code> ，<code>getDisplayString</code> 三个方法；</li></ul><p>每个方法有着不同的作用，参考如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// 这个方法只调用一次，并且在evaluate()方法之前调用。</span><br><span class="line">// 该方法接受的参数是一个ObjectInspectors数组，表示函数输入参数类型。</span><br><span class="line">// 1.检查接受正确的参数个数；</span><br><span class="line">// 2.检查接受正确的参数类型；</span><br><span class="line">// 3.定义返回值被序列化的一致类型。</span><br><span class="line">abstract ObjectInspector initialize(ObjectInspector[] arguments);</span><br><span class="line"></span><br><span class="line">// 这个方法类似UDF的evaluate()方法。它处理真实的参数，并返回最终结果。</span><br><span class="line">abstract Object evaluate(DeferredObject[] arguments);</span><br><span class="line"></span><br><span class="line">// 这个方法用于当实现的GenericUDF出错的时候，打印出提示信息。</span><br><span class="line">// 而提示信息就是你实现该方法最后返回的字符串。 </span><br><span class="line">abstract String getDisplayString(String[] children);</span><br></pre></td></tr></table></figure><p>其中有个 ObjectInspector 需要解释下，简单来说是帮助使用者访问需要序列化或者反序列化的对象，为数据类型提供一致性的访问接口。有关 ObjectInspector 更深入地理解，请参考 <a href="https://www.jianshu.com/p/5ea006282238" target="_blank" rel="noopener">Hive-ObjectInspector</a></p><h2 id="0x01-官方-ArrayGenericUDF-示例代码"><a href="#0x01-官方-ArrayGenericUDF-示例代码" class="headerlink" title="0x01 官方 ArrayGenericUDF 示例代码"></a>0x01 官方 ArrayGenericUDF 示例代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">        name = &quot;array&quot;,</span><br><span class="line">        value = &quot;_FUNC_(n0, n1...) - Creates an array with the given elements &quot;)</span><br><span class="line">public class ArrayGenericUDF extends GenericUDF &#123;</span><br><span class="line"></span><br><span class="line">    private transient Converter[] converters;</span><br><span class="line">    private transient ArrayList&lt;Object&gt; ret = new ArrayList&lt;Object&gt;();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123;</span><br><span class="line"></span><br><span class="line">        if (arguments.length &lt; 1) &#123;</span><br><span class="line">            throw new UDFArgumentLengthException(&quot;array() takes at least one parameter.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);</span><br><span class="line"></span><br><span class="line">        for (int i = 0; i &lt; arguments.length; i++) &#123;</span><br><span class="line">            if (!returnOIResolver.update(arguments[i])) &#123;</span><br><span class="line">                throw new UDFArgumentTypeException(i, &quot;Argument type \&quot;&quot;</span><br><span class="line">                        + arguments[i].getTypeName()</span><br><span class="line">                        + &quot;\&quot; is different from preceding arguments. &quot;</span><br><span class="line">                        + &quot;Previous type was \&quot;&quot; + arguments[i - 1].getTypeName() + &quot;\&quot;&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        converters = new Converter[arguments.length];</span><br><span class="line"></span><br><span class="line">        ObjectInspector returnOI =</span><br><span class="line">                returnOIResolver.get(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">        for (int i = 0; i &lt; arguments.length; i++) &#123;</span><br><span class="line">            converters[i] = ObjectInspectorConverters.getConverter(arguments[i], returnOI);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return ObjectInspectorFactory.getStandardListObjectInspector(returnOI);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public Object evaluate(DeferredObject[] arguments) throws HiveException &#123;</span><br><span class="line">        ret.clear();</span><br><span class="line">        for (int i = 0; i &lt; arguments.length; i++) &#123;</span><br><span class="line">            ret.add(converters[i].convert(arguments[i].get()));</span><br><span class="line">        &#125;</span><br><span class="line">        return ret;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String getDisplayString(String[] children) &#123;</span><br><span class="line">        return getStandardDisplayString(&quot;array&quot;, children, &quot;,&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x02-代码走读"><a href="#0x02-代码走读" class="headerlink" title="0x02 代码走读"></a>0x02 代码走读</h2><p>方法的调用关系如下：</p><ol><li><p>ArrayGenericUDF 用默认的构造器来初始化；</p></li><li><p>initialize() 被调用，传入函数参数的 ObjectInspector[] 数组；<br>2.1. 检查传入的参数个数与每个参数的数据类型是正确的；<br>2.2. 保存 converters (ObjectInspector) 用以供 evaluate() 使用；<br>2.3. 返回 ListObjectInspector，让 Hive 能够读取该函数的返回结果；</p></li><li><p>对于查询中的每一行，evaluate() 方法都会被调用，并传入该行的指定列；<br>3.1. 利用 initialize() 方法中存储的 converters (ObjectInspector) 来抽取出正确的值；<br>3.2. 执行处理逻辑然后用 initialize() 返回的 ListObjectInspector 来序列化返回值；</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/SerDe" target="_blank" rel="noopener">SerDe</a><br><a href="https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html" target="_blank" rel="noopener">Hadoop Hive UDF Tutorial - Extending Hive with Custom Functions</a><br><a href="https://www.jianshu.com/p/5ea006282238" target="_blank" rel="noopener">Hive-ObjectInspector</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;编写 Apache Hive 用户自定义函数（UDF）有两个不同的接口：一个是非常简单的 UDF，上一篇已经介绍过了；还有一个是 GenericUDF，相对复杂点。两个 API 的区别是：如果函数的参数和返回都是基础数据类型，那么简单 API（UDF）可以胜任；但是，如果你想写一个函数用来操作内嵌数据结构，如 Map、List 和 Set，此时就需要去熟悉复杂 API（GenericUDF）。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数UDF</title>
    <link href="https://jordenbruce.com/2019/11/03/hive-udf/"/>
    <id>https://jordenbruce.com/2019/11/03/hive-udf/</id>
    <published>2019-11-03T13:47:18.000Z</published>
    <updated>2019-11-04T13:52:33.524Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 有丰富的内置函数（Built-in Functions），方便数据处理与数据分析等。即便如此，内置函数有时候还是无法满足需求，这时就需要自定义函数（User-Defined Functions , UDF）来扩展 Hive 函数库，实现用户想要的功能。<br><a id="more"></a></p><h2 id="0x00-自定义-UDF-开发"><a href="#0x00-自定义-UDF-开发" class="headerlink" title="0x00 自定义 UDF 开发"></a>0x00 自定义 UDF 开发</h2><p>编写 UDF 需要下面两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.exec.UDF</code> ；</li><li>实现 <code>evaluate</code> 函数，这个函数必须要有返回值，不能设置为 void。同时建议使用 mapreduce 编程模型中的数据类型( Text, IntWritable 等)，因为 SQL 语句会被转换为 mapreduce 任务；</li></ul><p>示例代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import java.util.Date;</span><br><span class="line">import java.text.ParseException;</span><br><span class="line">import java.text.SimpleDateFormat;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">name = &quot;calc_remain&quot;,</span><br><span class="line">value = &quot;_FUNC_(active_list, active_list_date, base_date, remain) - Calculate remain_N based on a date.&quot;,</span><br><span class="line">extended = &quot;Example:\n&quot; +</span><br><span class="line">   &quot;select calc_remain(&apos;1,0,1,0,1,1,0,1&apos;,&apos;2019-03-15&apos;,&apos;2019-03-10&apos;,3);\n&quot;)</span><br><span class="line">public class CalcRemain extends UDF &#123;</span><br><span class="line"></span><br><span class="line">private interface Status &#123;</span><br><span class="line">String UnActive = &quot;0&quot;;</span><br><span class="line">String Active = &quot;1&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public int evaluate(String strActiveList, String strListDate, String strBaseDate, int intRemain) &#123;</span><br><span class="line">if (IsEmpty(strActiveList) || IsEmpty(strListDate)) &#123;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int intOuput = 0;</span><br><span class="line"></span><br><span class="line">Date dtBase = StringToDate(strBaseDate);</span><br><span class="line">Date dtLast = StringToDate(strListDate);</span><br><span class="line"></span><br><span class="line">long lngDiffDays = (dtLast.getTime() - dtBase.getTime()) / (24*3600*1000);</span><br><span class="line">if (intRemain &lt; 0 || lngDiffDays &lt; 1 || lngDiffDays &lt; intRemain) &#123;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int intDiffDays = (int)lngDiffDays;</span><br><span class="line">int lenActiveList = strActiveList.length();</span><br><span class="line"></span><br><span class="line">String[] arrActiveList = strActiveList</span><br><span class="line">                .substring(lenActiveList-intDiffDays*2-1, lenActiveList)</span><br><span class="line">                .split(&quot;,&quot;);</span><br><span class="line"></span><br><span class="line">if (arrActiveList[0].equals(Status.UnActive)) &#123;</span><br><span class="line">return 0;</span><br><span class="line">&#125; else if (arrActiveList[intRemain].equals(Status.Active)) &#123;</span><br><span class="line">intOuput = 1;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">intOuput = 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return intOuput;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private Date StringToDate(String strTime) &#123;</span><br><span class="line">Date dtOutput = null;</span><br><span class="line">SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);</span><br><span class="line">try &#123;</span><br><span class="line">dtOutput = sdf.parse(strTime);</span><br><span class="line">&#125; catch (ParseException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">return dtOutput;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private boolean IsEmpty (String strArgument) &#123;</span><br><span class="line">if (strArgument == null || strArgument.length() &lt;= 0) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x01-自定义-UDF-部署方式"><a href="#0x01-自定义-UDF-部署方式" class="headerlink" title="0x01 自定义 UDF 部署方式"></a>0x01 自定义 UDF 部署方式</h2><p>官方提供了两种部署 UDF 的方式：</p><ul><li>临时部署（Temporary Functions）</li><li>永久部署（Permanent Functions）</li></ul><p>两者的区别在于：临时部署的方式，只会在当前 Session 下有效并可用；永久部署的方式，在部署成功后任何一个 Hive 客户端（重新启动的 Hive 客户端，已经启动的客户端需要重新加载）都可以使用。</p><p>(1) 临时部署</p><p>这个是最常见的 Hive 使用方式，通过 hive 命令来完成 UDF 的部署；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; add jar /path/to/local.jar; </span><br><span class="line">hive&gt; create temporary function calc_remain as &apos;com.data.hive.CalcRemain&apos;;</span><br></pre></td></tr></table></figure><p>建议函数名使用 <strong>下划线命名法</strong>（全部小写字母）。</p><p>(2) 永久部署</p><p>这种方式是 hive-0.13 版本以后开始支持的注册方法；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create function udf.calc_remain </span><br><span class="line">hive&gt; as &apos;com.data.hive.CalcRemain&apos; </span><br><span class="line">hive&gt; using jar &apos;hdfs:///path/to/hdfs.jar&apos;;</span><br></pre></td></tr></table></figure><p>需要注意的是：函数名称前面一定要带上数据库名称。</p><h2 id="0x02-函数相关的-HQL-语句"><a href="#0x02-函数相关的-HQL-语句" class="headerlink" title="0x02 函数相关的 HQL 语句"></a>0x02 函数相关的 HQL 语句</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">-- 查看所有函数(内置函数+自定义函数)</span><br><span class="line">show functions;</span><br><span class="line">-- 查看某个函数的使用说明</span><br><span class="line">describe function function_name;</span><br><span class="line">-- 创建临时自定义函数</span><br><span class="line">create temporary function function_name as class_name;</span><br><span class="line">-- 删除临时自定义函数</span><br><span class="line">drop temporary function [if exists] function_name;</span><br><span class="line">-- 创建永久自定义函数</span><br><span class="line">create function [db_name.]function_name as class_name</span><br><span class="line">  [using jar|file|archive &apos;file_uri&apos; [, jar|file|archive &apos;file_uri&apos;] ];</span><br><span class="line">-- 删除永久自定义函数</span><br><span class="line">drop function [if exists] function_name;</span><br><span class="line">-- 重载函数</span><br><span class="line">reload function;</span><br></pre></td></tr></table></figure><h2 id="0x03-扩展与延伸"><a href="#0x03-扩展与延伸" class="headerlink" title="0x03 扩展与延伸"></a>0x03 扩展与延伸</h2><p>Hive 自定义函数的扩展，不仅有 UDF（User-Defined Functions），还有 UDAF（User-Defined Aggregate Functions）和 UDTF（User-Defined Table-Generating Functions），详情请参考官方说明。</p><p>另外，如果数据处理在函数级别不能解决，还可以借助 <code>TRANSFORM</code> 自定义 Map 和 Reduce 函数，或者编写 MapReduce 程序。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins" target="_blank" rel="noopener">Creating Custom UDFs</a><br><a href="https://blog.csdn.net/qq_32653877/article/details/87182898" target="_blank" rel="noopener">Java编写Hive的UDF</a><br><a href="http://chaozi204.github.io/blog/hive-udf-deploy/" target="_blank" rel="noopener">Hive UDF 部署方式小结</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy" target="_blank" rel="noopener">Writing GenericUDAFs</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform" target="_blank" rel="noopener">Hive’s Transform functionality</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 有丰富的内置函数（Built-in Functions），方便数据处理与数据分析等。即便如此，内置函数有时候还是无法满足需求，这时就需要自定义函数（User-Defined Functions , UDF）来扩展 Hive 函数库，实现用户想要的功能。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Java通过JDBC连接Hive</title>
    <link href="https://jordenbruce.com/2019/11/03/hive-jdbc-client/"/>
    <id>https://jordenbruce.com/2019/11/03/hive-jdbc-client/</id>
    <published>2019-11-03T12:56:18.000Z</published>
    <updated>2019-11-03T14:21:36.883Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 是大数据技术簇中进行数据仓库应用的基础组件，是其它类似数据仓库应用的对比基准。基础的数据操作可以通过脚本方式以 cli 进行处理；若需要开发应用程序，则需要使用 hive-jdbc 驱动进行连接。<br><a id="more"></a></p><h2 id="0x00-添加-hive-site-xml-配置项"><a href="#0x00-添加-hive-site-xml-配置项" class="headerlink" title="0x00 添加 hive-site.xml 配置项"></a>0x00 添加 hive-site.xml 配置项</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">&lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="0x01-启动-hiveserver2-服务"><a href="#0x01-启动-hiveserver2-服务" class="headerlink" title="0x01 启动 hiveserver2 服务"></a>0x01 启动 hiveserver2 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$&#123;HIVE_HOME&#125;/bin/hive --service metastore &amp;</span><br><span class="line">$&#123;HIVE_HOME&#125;/bin/hive --service hiveserver2 &amp;</span><br><span class="line"></span><br><span class="line">netstat -lnt | grep 10000</span><br></pre></td></tr></table></figure><h2 id="0x02-beeline-方式连接"><a href="#0x02-beeline-方式连接" class="headerlink" title="0x02 beeline 方式连接"></a>0x02 beeline 方式连接</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">shell&gt;&gt; $&#123;HIVE_HOME&#125;/bin/beeline</span><br><span class="line">Beeline version 1.2.2 by Apache Hive</span><br><span class="line">beeline&gt; !connect jdbc:hive2://localhost:10000/default</span><br><span class="line">Connecting to jdbc:hive2://localhost:10000/default</span><br><span class="line">Enter username for jdbc:hive2://localhost:10000/default: hadoop</span><br><span class="line">Enter password for jdbc:hive2://localhost:10000/default: ******</span><br><span class="line">Connected to: Apache Hive (version 1.2.2)</span><br><span class="line">Driver: Hive JDBC (version 1.2.2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">+-------------------+--+</span><br><span class="line">|     tab_name      |</span><br><span class="line">+-------------------+--+</span><br><span class="line">| managed_user      |</span><br><span class="line">| ods_user          |</span><br><span class="line">| partitioned_user  |</span><br><span class="line">+-------------------+--+</span><br><span class="line">3 rows selected (0.358 seconds)</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt; !quit</span><br><span class="line">Closing: 0: jdbc:hive2://localhost:10000/default</span><br></pre></td></tr></table></figure><h2 id="0x03-jdbc-方式连接"><a href="#0x03-jdbc-方式连接" class="headerlink" title="0x03 jdbc 方式连接"></a>0x03 jdbc 方式连接</h2><p>建议使用 IntelliJ IDEA 开发工具，创建 Maven 工程，Java 代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import java.sql.SQLException;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line"></span><br><span class="line">public class HiveJDBC &#123;</span><br><span class="line"></span><br><span class="line">    private static String DriverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;;</span><br><span class="line"></span><br><span class="line">    private static String ServerUrl = &quot;jdbc:hive2://localhost:10000/default&quot;;</span><br><span class="line">    private static String ServerUser = &quot;hadoop&quot;;</span><br><span class="line">    private static String ServerPwd = &quot;hadoop&quot;;</span><br><span class="line"></span><br><span class="line">    public static void main(String args[]) throws SQLException &#123;</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            Class.forName(DriverName);</span><br><span class="line">        &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            System.exit(1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Connection hiveConn = DriverManager.getConnection(ServerUrl, ServerUser, ServerPwd);</span><br><span class="line">        Statement hiveStmt = hiveConn.createStatement();</span><br><span class="line"></span><br><span class="line">        ResultSet hqlOutput = hiveStmt.executeQuery(&quot;show tables&quot;);</span><br><span class="line">        while (hqlOutput.next()) &#123;</span><br><span class="line">            System.out.println(hqlOutput.getString(1));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        hqlOutput.close();</span><br><span class="line">        hiveStmt.close();</span><br><span class="line">        hiveConn.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>添加 Maven 依赖，pom.xml 依赖配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">&lt;project.build.sourceEncoding&gt;UTF8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">&lt;hadoop.version&gt;2.7.7&lt;/hadoop.version&gt;</span><br><span class="line">&lt;hive.version&gt;1.2.2&lt;/hive.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">&lt;id&gt;Apache Hadoop&lt;/id&gt;</span><br><span class="line">&lt;name&gt;Apache Hadoop&lt;/name&gt;</span><br><span class="line">&lt;url&gt;https://repo1.maven.org/maven2/&lt;/url&gt;</span><br><span class="line">&lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-metastore&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p>大功告成！</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/HiveClient" target="_blank" rel="noopener">Hive Client</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients" target="_blank" rel="noopener">HiveServer2 Clients</a><br><a href="https://www.cnblogs.com/gridmix/p/5102725.html" target="_blank" rel="noopener">通过JDBC连接hive</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 是大数据技术簇中进行数据仓库应用的基础组件，是其它类似数据仓库应用的对比基准。基础的数据操作可以通过脚本方式以 cli 进行处理；若需要开发应用程序，则需要使用 hive-jdbc 驱动进行连接。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive体系结构</title>
    <link href="https://jordenbruce.com/2019/11/02/hive-architecture/"/>
    <id>https://jordenbruce.com/2019/11/02/hive-architecture/</id>
    <published>2019-11-02T03:46:32.000Z</published>
    <updated>2019-11-03T12:59:19.852Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 最初是由 FaceBook 公司开发的一个基于 Hadoop 框架并且开源的一个数据仓库工具，后贡献给了 Apache 基金会由 Apache 来进行维护和更新。Hive 可以将结构化的文件映射为一张数据表，但并不提供查询功能，而是将 HiveQL 转化为 MapReduce 任务进行运行。同时，Hive 本身不存储数据，只是存储数据的路径或者操作信息，真的数据是存储在可靠的文件系统当中（如HDFS、Derby等）。<br><a id="more"></a></p><h2 id="0x00-Hive-架构"><a href="#0x00-Hive-架构" class="headerlink" title="0x00 Hive 架构"></a>0x00 Hive 架构</h2><p><img src="https://i.loli.net/2019/11/02/VUOGX45StkYbwMF.jpg" alt="Hive 架构"></p><p>Hive 架构主要包括如下组件：CLI（command line interface）、JDBC/ODBC、Web GUI（Hive WEB Interface）、Thrift Server、Metastore 和 Driver(Complier、Optimizer 和 Executor)，这些组件可以分为两大类：服务端组件和客户端组件。</p><p>(1) 客户端组件</p><ul><li>CLI：最常用的用户接口，cli 启动的时候，会同时启动一个 Hive 副本；</li><li>JDBC/ODBC：Client 是 Hive 的客户端，用户连接至 Hive Server。在启动 Client 模式的时候，需要指出 Hive Server 所在节点，并且在该节点启动 Hive Server ；</li><li>HWI：通过浏览器访问 Hive；</li></ul><p>(2) 服务端组件</p><ul><li>Thrift Server：Thrift 是 facebook 开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，hive 集成了Thrift Server 服务，能让不同的编程语言调用 hive 的接口；</li><li>Metastore：元数据服务组件，这个组件用于存储 hive 的元数据，包括表名、表所属的数据库、表的拥有者、列/分区字段、表的类型、表的数据所在目录等内容。hive 的元数据存储在关系数据库里，支持 derby、mysql 两种关系型数据库；</li><li>Driver：包括 Complier、Optimizer 和 Executor，它们的作用是将我们写的 HiveQL 语句进行解析、编译、优化，生成执行计划，然后调用底层的 MapReduce 计算框架；</li></ul><h2 id="0x01-Hive-工作原理"><a href="#0x01-Hive-工作原理" class="headerlink" title="0x01 Hive 工作原理"></a>0x01 Hive 工作原理</h2><p><img src="https://i.loli.net/2019/11/02/GIvJ2RqS6d3Hf4i.png" alt="Hive 查询的工作流程"></p><ol><li>UI 提交查询（HiveQL）给 Driver；</li><li>Driver 为查询创建会话句柄，并将查询发给 Complier 生成执行计划；</li><li>Complier 从 Metastore 获取必要的元数据信息；</li><li>由 Complier 生成的执行计划是 作业的DAG，包括三种作业：MapReduce 作业、元数据操作、HDFS 操作；</li><li>Execution Engine 将不同作业提交给对应的组件；</li><li>Execution Engine 将匹配上的查询结果经过 Driver 发送给 UI。</li></ol><p>细心的读者会发现，上述执行过程还是 JobTracker 版本，从 Hadoop-2.x 开始升级为 YARN，MapReduce 作为一种应用程序运行在 YARN 框架下。</p><h2 id="0x02-Hive-特点"><a href="#0x02-Hive-特点" class="headerlink" title="0x02 Hive 特点"></a>0x02 Hive 特点</h2><p>(1) 优点</p><ul><li>简单容易上手：提供了类 SQL 查询语言 HiveQL</li><li>可扩展：为超大数据集设计了计算/扩展能力（MR 作为计算引擎，HDFS 作为存储系统）</li><li>提供统一的元数据管理</li><li>延展性：Hive 支持用户自定义函数，还允许 TRANSFORM 嵌入不同语言的 map-reduce 脚本</li><li>容错：良好的容错性，节点出现问题 SQL 仍可完成执行</li></ul><p>(2) 缺点</p><ul><li>HiveQL 表达能力有限（迭代式算法，数据挖掘）</li><li>hive 效率比较低（mapreduce，调优）</li><li>hive 可控性差</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Design" target="_blank" rel="noopener">Hive Architecture Overview</a><br><a href="https://www.cnblogs.com/zimo-jing/p/9028949.html" target="_blank" rel="noopener">深入学习Hive应用场景及架构原理</a><br><a href="https://blog.csdn.net/m0_37914799/article/details/85109143" target="_blank" rel="noopener">Hive的架构及元数据三种存储模式</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 最初是由 FaceBook 公司开发的一个基于 Hadoop 框架并且开源的一个数据仓库工具，后贡献给了 Apache 基金会由 Apache 来进行维护和更新。Hive 可以将结构化的文件映射为一张数据表，但并不提供查询功能，而是将 HiveQL 转化为 MapReduce 任务进行运行。同时，Hive 本身不存储数据，只是存储数据的路径或者操作信息，真的数据是存储在可靠的文件系统当中（如HDFS、Derby等）。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
</feed>
