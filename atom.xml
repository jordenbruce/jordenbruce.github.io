<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JordenBruce</title>
  
  <subtitle>A thousand miles begins with a single step.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jordenbruce.com/"/>
  <updated>2019-12-20T09:15:45.218Z</updated>
  <id>https://jordenbruce.com/</id>
  
  <author>
    <name>JordenBruce</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hive存储格式与压缩</title>
    <link href="https://jordenbruce.com/2019/12/20/hive-fileformat-compress/"/>
    <id>https://jordenbruce.com/2019/12/20/hive-fileformat-compress/</id>
    <published>2019-12-20T08:03:34.000Z</published>
    <updated>2019-12-20T09:15:45.218Z</updated>
    
    <content type="html"><![CDATA[<p>采用Hive作为数据仓库工具，由于数仓既要存储来自不同系统的数据源，还要执行ETL数据任务，所以Hive不仅提供了多种存储格式（TEXTFILE，ORC，PARQUET），降低了存储成本，而且还支持多种压缩方式（zlib，snappy，bzip2），提高了计算效率。那么，不同的业务场景该选择哪一种存储格式或压缩方式呢？<br><a id="more"></a></p><h2 id="0x00-存储格式"><a href="#0x00-存储格式" class="headerlink" title="0x00 存储格式"></a>0x00 存储格式</h2><p>TEXTFILE是Hive默认的存储格式，存储空间消耗比较大，并且压缩的text无法分割和合并，查询的效率最低，可以直接存储，加载数据的速度最高；</p><p>ORC存储格式是一种Hadoop生态圈中的列式存储格式，它的产生早在2013年初，最初产生自Apache Hive，用于降低Hadoop数据存储空间和加速Hive查询速度。</p><p>Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定，目前能够和Parquet适配的组件很多，基本上通常使用的查询引擎和计算框架都已适配，并且可以很方便的将其它序列化工具生成的数据转换成Parquet格式。</p><table><thead><tr><th>文件格式</th><th>存储方式</th><th>压缩比</th><th>查询速度</th><th>文件拆分</th><th>支持压缩</th></tr></thead><tbody><tr><td>TEXTFILE</td><td>行式</td><td>小</td><td>慢</td><td>不可切分</td><td>NONE, GZIP</td></tr><tr><td>ORC</td><td>列式</td><td>大</td><td>快</td><td>可切分</td><td>NONE, ZLIB, SNAPPY</td></tr><tr><td>PARQUET</td><td>列式</td><td>中</td><td>快</td><td>可切分</td><td>NONE, SNAPPY，BZIP2</td></tr></tbody></table><h2 id="0x01-压缩方式"><a href="#0x01-压缩方式" class="headerlink" title="0x01 压缩方式"></a>0x01 压缩方式</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;采用Hive作为数据仓库工具，由于数仓既要存储来自不同系统的数据源，还要执行ETL数据任务，所以Hive不仅提供了多种存储格式（TEXTFILE，ORC，PARQUET），降低了存储成本，而且还支持多种压缩方式（zlib，snappy，bzip2），提高了计算效率。那么，不同的业务场景该选择哪一种存储格式或压缩方式呢？&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive元数据的解析</title>
    <link href="https://jordenbruce.com/2019/12/12/hive-metastore/"/>
    <id>https://jordenbruce.com/2019/12/12/hive-metastore/</id>
    <published>2019-12-12T10:21:51.000Z</published>
    <updated>2019-12-12T11:31:46.159Z</updated>
    
    <content type="html"><![CDATA[<p>Hive体系结构的元数据（Metastore）是一个重要的组件，保存了Hive有关库、表、存储、分区等信息。元数据主要包括两个方面：一方面是元数据库，最常见的是采用MySQL；另一方面是元数据服务，与其他查询引擎共享，比如Presto或Impala等。<br><a id="more"></a></p><h2 id="0x00-Hive元数据库"><a href="#0x00-Hive元数据库" class="headerlink" title="0x00 Hive元数据库"></a>0x00 Hive元数据库</h2><p>Hive支持两种类型的元数据库：</p><ul><li>本地或嵌入的元数据库：Derby</li><li>远程的元数据库：MySQL</li></ul><p>说明：嵌入的元数据库主要用于单元测试，并且一次只能有一个进程来连接，所以生产环境不推荐使用。实际上，线上使用最多的是采用MySQL作为远程的元数据库。</p><p>（1）配置Hive元数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://master:3306/hive?characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;mysql&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>（2）元数据库E-R图</p><p><img src="https://i.loli.net/2019/12/12/I24QhomPgeUJVNb.jpg" alt="Hive Metastore"></p><p>（3）元数据库的表说明</p><p><img src="https://i.loli.net/2019/12/12/mJMhpey5fqgbAkl.png" alt="Hive Metastore Tables"></p><h2 id="0x01-Hive元数据服务"><a href="#0x01-Hive元数据服务" class="headerlink" title="0x01 Hive元数据服务"></a>0x01 Hive元数据服务</h2><p>尽管Hive元数据服务也支持两种方式，可是生产环境采用的是MySQL作为元数据库，所以这里只介绍远程服务的配置与启动。</p><p>（1）配置Hive元数据服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">&lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>（2）启动Hive元数据服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service hivestore &amp;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration" target="_blank" rel="noopener">Metastore Administration</a><br><a href="https://jordenbruce.com/2019/11/02/hive-architecture/">Hive体系结构</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive体系结构的元数据（Metastore）是一个重要的组件，保存了Hive有关库、表、存储、分区等信息。元数据主要包括两个方面：一方面是元数据库，最常见的是采用MySQL；另一方面是元数据服务，与其他查询引擎共享，比如Presto或Impala等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的Select语句之GroupBy子句</title>
    <link href="https://jordenbruce.com/2019/12/12/hql-select-groupby/"/>
    <id>https://jordenbruce.com/2019/12/12/hql-select-groupby/</id>
    <published>2019-12-12T03:37:03.000Z</published>
    <updated>2019-12-12T04:44:15.058Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道Select语句能访问存储在Hive表中的数据，做聚合查询时总是会使用到GroupBy子句，常见的聚合函数有count(),sum(),max()等。其实，GroupBy子句不仅包括基本的聚合作用，还可以做OLAP查询，比如cube,rollup,grouping sets等，统计根据不同维度上卷或下钻的指标。<br><a id="more"></a></p><h2 id="0x00-基本GroupBy聚合查询"><a href="#0x00-基本GroupBy聚合查询" class="headerlink" title="0x00 基本GroupBy聚合查询"></a>0x00 基本GroupBy聚合查询</h2><p>需求场景：统计每个每个客户的消费次数，消费金额，最大一次消费金额，最小一次消费金额，平均消费金额。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select order_date, user_name</span><br><span class="line">      ,count(*)    as total_cnt</span><br><span class="line">      ,sum(amount) as total_amt</span><br><span class="line">      ,max(amount) as max_amt</span><br><span class="line">      ,min(amount) as min_amt</span><br><span class="line">      ,avg(amount) as avg_amt</span><br><span class="line">from order_window</span><br><span class="line">group by order_date, user_name</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x01-高级GroupBy分析查询"><a href="#0x01-高级GroupBy分析查询" class="headerlink" title="0x01 高级GroupBy分析查询"></a>0x01 高级GroupBy分析查询</h2><p>官方支持的高级GroupBy功能有：</p><ul><li>CUBE子句：实现任意维度组合的分组聚合</li><li>ROLLUP子句：实现从右到左递减多级的分组聚合</li><li>GROUPING SETS子句：依据指定分组进行聚合</li><li>Grouping__ID函数：表示聚合结果属于哪一个分组集合</li><li>Grouping()函数：表示当前行是否参与了该分组的聚合</li></ul><p>需求场景：统计每天每个客户的消费金额的数据魔方，满足上卷与下钻。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">select order_date, user_name</span><br><span class="line">      ,grouping__id</span><br><span class="line">      ,sum(amount) as total_amt</span><br><span class="line">from order_window</span><br><span class="line">group by order_date, user_name</span><br><span class="line">    with cube</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">select order_date, user_name</span><br><span class="line">      ,grouping__id</span><br><span class="line">      ,sum(amount) as total_amt</span><br><span class="line">from order_window</span><br><span class="line">group by order_date, user_name</span><br><span class="line">    with rollup</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">select order_date, user_name</span><br><span class="line">      ,grouping__id</span><br><span class="line">      ,sum(amount) as total_amt</span><br><span class="line">from order_window</span><br><span class="line">group by order_date, user_name</span><br><span class="line">    grouping sets(</span><br><span class="line">     (order_date, user_name)</span><br><span class="line">    ,(order_date)</span><br><span class="line">    )</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x02-GroupBy子句的参数调优"><a href="#0x02-GroupBy子句的参数调优" class="headerlink" title="0x02 GroupBy子句的参数调优"></a>0x02 GroupBy子句的参数调优</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---MAP端聚合</span><br><span class="line">set hive.map.aggr=true;</span><br><span class="line">---发生倾斜时进行负载均衡</span><br><span class="line">set hive.groupby.skewindata=true;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation%2C+Cube%2C+Grouping+and+Rollup" target="_blank" rel="noopener">Enhanced Aggregation, Cube, Grouping and Rollup</a><br><a href="https://blog.csdn.net/mashroomxl/article/details/22578471" target="_blank" rel="noopener">Hive.GROUPING SETS</a><br><a href="https://blog.csdn.net/lzm1340458776/article/details/43231707" target="_blank" rel="noopener">Hive group by操作</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们知道Select语句能访问存储在Hive表中的数据，做聚合查询时总是会使用到GroupBy子句，常见的聚合函数有count(),sum(),max()等。其实，GroupBy子句不仅包括基本的聚合作用，还可以做OLAP查询，比如cube,rollup,grouping sets等，统计根据不同维度上卷或下钻的指标。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的分析函数</title>
    <link href="https://jordenbruce.com/2019/12/09/hql-function-analytic/"/>
    <id>https://jordenbruce.com/2019/12/09/hql-function-analytic/</id>
    <published>2019-12-09T12:10:53.000Z</published>
    <updated>2019-12-09T13:25:48.004Z</updated>
    
    <content type="html"><![CDATA[<p>在 Hive 中做数据分析或 OLAP 查询，除了使用上一篇提到的窗口函数之外，还有一批分析函数，主要用于排名、排序、分组等。当然，在不同的实际业务场景下，分析函数有着不同的表达，功能更加丰富。<br><a id="more"></a></p><h2 id="0x00-常用的分析函数"><a href="#0x00-常用的分析函数" class="headerlink" title="0x00 常用的分析函数"></a>0x00 常用的分析函数</h2><p>下表列出了一些分析函数以及描述信息：</p><table><thead><tr><th>分析函数</th><th>描述</th></tr></thead><tbody><tr><td>RANK</td><td>返回数据项在分区中的排名。排名值序列可能会有间隔</td></tr><tr><td>DENSE_RANK</td><td>返回数据项在分区中的排名。排名值序列是连续的，不会有间隔</td></tr><tr><td>PERCENT_RANK</td><td>计算当前行的百分比排名</td></tr><tr><td>CUME_DIST</td><td>计算分区中当前行的相对排名</td></tr><tr><td>ROW_NUMBER</td><td>确定分区中当前行的序号</td></tr><tr><td>NTILE</td><td>将每个分区的行尽可能均匀地划分为指定数量的分组</td></tr></tbody></table><h2 id="0x01-分析函数的使用要点"><a href="#0x01-分析函数的使用要点" class="headerlink" title="0x01 分析函数的使用要点"></a>0x01 分析函数的使用要点</h2><p>对比上一篇的窗口函数，分析函数的使用要点有：</p><ul><li>必须结合 over + order by 一起使用</li><li>不能使用 window 子句</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select gid, dt, pv</span><br><span class="line">      ,rank() over(partition by gid order by pv) as rank_1</span><br><span class="line">      ,dense_rank() over(partition by gid order by pv) as rank_2</span><br><span class="line">      ,percent_rank() over(partition by gid order by pv) as rank_3</span><br><span class="line">      ,cume_dist() over(partition by gid order by pv) as rank_4</span><br><span class="line">      ,row_number() over(partition by gid order by pv) as row_no</span><br><span class="line">      ,ntile(2) over(partition by gid order by pv) as bucket_no</span><br><span class="line">from page_analytic</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x02-分析函数的业务场景"><a href="#0x02-分析函数的业务场景" class="headerlink" title="0x02 分析函数的业务场景"></a>0x02 分析函数的业务场景</h2><p>给了用户和每个用户对应的消费信息表，计算花费前50%的用户的平均消费。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">---把用户和消费表，按消费下降顺序平均分成2份</span><br><span class="line">drop table if exists test_by_payment_ntile;</span><br><span class="line">create table test_by_payment_ntile as</span><br><span class="line">select nick</span><br><span class="line">      ,payment</span><br><span class="line">      ,ntile(2) over(order by payment desc) as rn </span><br><span class="line">from test_nick_payment;</span><br><span class="line"></span><br><span class="line">---分别对每一份计算平均值，就可以得到消费靠前50%和后50%的平均消费</span><br><span class="line">select &apos;avg_payment&apos; as inf</span><br><span class="line">      ,t1.avg_payment_up_50 as avg_payment_up_50</span><br><span class="line">      ,t2.avg_payment_down_50 as avg_payment_down_50</span><br><span class="line">from (</span><br><span class="line">    select avg(payment) as avg_payment_up_50 </span><br><span class="line">    from test_by_payment_ntile </span><br><span class="line">    where rn = 1</span><br><span class="line">) t1</span><br><span class="line">join (</span><br><span class="line">    select avg(payment) as avg_payment_down_50 </span><br><span class="line">    from test_by_payment_ntile </span><br><span class="line">    where rn = 2</span><br><span class="line">) t2</span><br><span class="line">on t1.dp_id = t2.dp_id</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">Windowing and Analytics Functions</a><br><a href="https://blog.csdn.net/SunnyYoona/article/details/56488568" target="_blank" rel="noopener">分析函数 RANK ROW_NUMBER CUME_DIST CUME_DIST</a><br><a href="https://blog.csdn.net/zhangxianx1an/article/details/80609514" target="_blank" rel="noopener">Hive分析函数–Ntile</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Hive 中做数据分析或 OLAP 查询，除了使用上一篇提到的窗口函数之外，还有一批分析函数，主要用于排名、排序、分组等。当然，在不同的实际业务场景下，分析函数有着不同的表达，功能更加丰富。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HiveQL的窗口函数</title>
    <link href="https://jordenbruce.com/2019/12/08/hql-function-window/"/>
    <id>https://jordenbruce.com/2019/12/08/hql-function-window/</id>
    <published>2019-12-08T07:55:00.000Z</published>
    <updated>2019-12-09T13:06:16.278Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道在 SQL 中有一类函数叫做聚合函数，例如 sum()、avg()、max() 等等，这类函数可以将多行数据按照规则聚集为一行，一般来讲聚集后的行数是要少于聚集前的行数的。但是，有时候我们既要显示聚集前的数据，又要显示聚集后的数据，此时我们便引入了窗口函数。窗口函数主要用于 OLAP 数据分析。<br><a id="more"></a></p><h2 id="0x00-说在前面"><a href="#0x00-说在前面" class="headerlink" title="0x00 说在前面"></a>0x00 说在前面</h2><p>本文采用的数据表是 order_window(user_name, order_date, amount)，详细如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jack    2015-01-01    10</span><br><span class="line">tony    2015-01-02    15</span><br><span class="line">jack    2015-02-03    23</span><br><span class="line">tony    2015-01-04    29</span><br><span class="line">jack    2015-01-05    46</span><br><span class="line">jack    2015-04-06    42</span><br><span class="line">tony    2015-01-07    50</span><br><span class="line">jack    2015-01-08    55</span><br><span class="line">mart    2015-04-08    62</span><br><span class="line">mart    2015-04-09    68</span><br><span class="line">neil    2015-05-10    12</span><br><span class="line">mart    2015-04-11    75</span><br><span class="line">neil    2015-06-12    80</span><br><span class="line">mart    2015-04-13    94</span><br></pre></td></tr></table></figure><p>在深入研究 over子句 之前，一定要注意：在SQL处理中，窗口函数都是最后一步执行，而且仅位于order by字句之前。</p><h2 id="0x01-OVER子句"><a href="#0x01-OVER子句" class="headerlink" title="0x01 OVER子句"></a>0x01 OVER子句</h2><p>官方 OVER子句 包括几个部分：</p><ul><li>聚合函数（count, sum, min, max, avg）</li><li>OVER 子句</li><li>PARTITION BY 子句</li><li>ORDER BY 子句</li><li>WINDOW 子句</li></ul><p>结合具体的业务场景，SQL 语句如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">---1)201504月份的销售额</span><br><span class="line">select sum(amount) as total_amt</span><br><span class="line">from order_window </span><br><span class="line">where substr(order_date,1,7)=&apos;2015-04&apos;</span><br><span class="line">;</span><br><span class="line">---2)201504月份的订单明细与销售额</span><br><span class="line">select user_name, order_date, amount</span><br><span class="line">      ,sum(amount) over() as total_amt</span><br><span class="line">from order_window</span><br><span class="line">where substr(order_date,1,7)=&apos;2015-04&apos;</span><br><span class="line">;</span><br><span class="line">---3)客户的订单明细与月购买金额</span><br><span class="line">select user_name, order_date, amount</span><br><span class="line">      ,sum(amount) over (partition by month(order_date)) month_amt</span><br><span class="line">from order_window</span><br><span class="line">;</span><br><span class="line">---4)客户的订单明细与累计购买金额</span><br><span class="line">select user_name, order_date, amount</span><br><span class="line">      ,sum(amount) over (partition by month(order_date) order by order_date) month_add_amt</span><br><span class="line">from order_window</span><br><span class="line">;</span><br><span class="line">---5)不同窗口的销售额</span><br><span class="line">select </span><br><span class="line">     user_name</span><br><span class="line">    ,order_date</span><br><span class="line">    ,amount</span><br><span class="line">    ,sum(amount) over() as sample1 --所有行相加</span><br><span class="line">    ,sum(amount) over(partition by user_name) as sample2 --按name分组，组内数据相加</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date) as sample3 --按name分组，组内数据累加</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date rows between UNBOUNDED PRECEDING and current row) as sample4 --和sample3一样,由起点到当前行的聚合</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date rows between 1 PRECEDING and current row) as sample5 --当前行和前面一行做聚合</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date rows between 1 PRECEDING and 1 FOLLOWING) as sample6 --当前行和前边一行及后面一行</span><br><span class="line">    ,sum(amount) over(partition by user_name order by order_date rows between current row and UNBOUNDED FOLLOWING) as sample7 --当前行及后面所有行</span><br><span class="line">from order_window</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x02-WINDOW子句"><a href="#0x02-WINDOW子句" class="headerlink" title="0x02 WINDOW子句"></a>0x02 WINDOW子句</h2><p>带有窗口规范的OVER子句。窗口可以在WINDOW子句中单独定义。窗口规范支持如下格式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)</span><br><span class="line">(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING</span><br></pre></td></tr></table></figure><table><thead><tr><th>关键字</th><th>说明</th></tr></thead><tbody><tr><td>PRECEDING</td><td>表示当前行之前的行</td></tr><tr><td>UNBOUNDED PRECEDING</td><td>表示当前行之前无边界行，即第一行</td></tr><tr><td>num PRECEDING</td><td>表示当前行之前第num行</td></tr><tr><td>CURRENT ROW</td><td>表示当前行</td></tr><tr><td>FOLLOWING</td><td>表示当前行后面的行</td></tr><tr><td>UNBOUNDED FOLLOWING</td><td>表示当前行后面无边界行，即最后一行</td></tr><tr><td>num FOLLOWING</td><td>表示当前行后面第num行</td></tr></tbody></table><p>当缺少WINDOW子句并指定使用ORDER BY时，窗口规范默认为RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW，即从第一行到当前行。</p><p>当缺少ORDER BY和WINDOW子句时，窗口规范默认为ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING，即第一行到最后一行。</p><h2 id="0x03-窗口函数"><a href="#0x03-窗口函数" class="headerlink" title="0x03 窗口函数"></a>0x03 窗口函数</h2><table><thead><tr><th>窗口函数</th><th>描述</th></tr></thead><tbody><tr><td>LAG()</td><td>返回分区中当前行之前行（可以指定第几行）的值。如果没有行，则返回null。</td></tr><tr><td>LEAD()</td><td>返回分区中当前行后面行（可以指定第几行）的值。如果没有行，则返回null。</td></tr><tr><td>FIRST_VALUE</td><td>返回相对于窗口中第一行的指定列的值。</td></tr><tr><td>LAST_VALUE</td><td>返回相对于窗口中最后一行的指定列的值。</td></tr></tbody></table><p>结合具体的业务场景，SQL 语句如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select user_name, order_date, amount</span><br><span class="line">      ,lag(order_date,1,&apos;1900-01-01&apos;) over(partition by user_name order by order_date) as pre_order_date --当前订单的上一单日期</span><br><span class="line">      ,lead(order_date,1,&apos;9999-12-31&apos;) over(partition by user_name order by order_date) as fol_order_date --当前订单的下一单日期</span><br><span class="line">      ,first_value(amount,true) over (partition by user_name order by order_date) as first_order_amt --截至当前订单的第一单金额</span><br><span class="line">      ,last_value(amount,true) over (partition by user_name order by order_date) as last_order_amt --截至当前订单的最后一单金额</span><br><span class="line">from order_window</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="noopener">Windowing and Analytics Functions</a><br><a href="https://blog.csdn.net/qq_26937525/article/details/54925827" target="_blank" rel="noopener">Hive窗口函数</a><br><a href="https://yq.aliyun.com/articles/632198" target="_blank" rel="noopener">窗口函数与分析函数</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们知道在 SQL 中有一类函数叫做聚合函数，例如 sum()、avg()、max() 等等，这类函数可以将多行数据按照规则聚集为一行，一般来讲聚集后的行数是要少于聚集前的行数的。但是，有时候我们既要显示聚集前的数据，又要显示聚集后的数据，此时我们便引入了窗口函数。窗口函数主要用于 OLAP 数据分析。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Yarn三种Scheduler调度器</title>
    <link href="https://jordenbruce.com/2019/11/17/hadoop-yarn-scheduler/"/>
    <id>https://jordenbruce.com/2019/11/17/hadoop-yarn-scheduler/</id>
    <published>2019-11-17T11:11:58.000Z</published>
    <updated>2019-11-22T13:36:56.415Z</updated>
    
    <content type="html"><![CDATA[<p>理想情况下，我们应用对 Yarn 资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在 Yarn 中，负责给应用分配资源的是 Scheduler 调度器；而调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此，Yarn 提供了三种调度器和可配置的策略供我们选择。<br><a id="more"></a></p><h2 id="0x00-FIFO-Scheduler"><a href="#0x00-FIFO-Scheduler" class="headerlink" title="0x00 FIFO Scheduler"></a>0x00 FIFO Scheduler</h2><p>FIFO Scheduler 把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。 </p><p><img src="https://i.loli.net/2019/11/17/z4G5bl3PgFjEV87.png" alt="FIFO Scheduler"></p><p>FIFO Scheduler 它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。</p><h2 id="0x01-Capacity-Scheduler"><a href="#0x01-Capacity-Scheduler" class="headerlink" title="0x01 Capacity Scheduler"></a>0x01 Capacity Scheduler</h2><p>Capacity 调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p><p><img src="https://i.loli.net/2019/11/17/PEaQlMzkgs4vbeq.png" alt="Capacity Scheduler"></p><p>Capacity 调度器允许多个组织共享整个集群，每个组织可以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。除此之外，队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略。 </p><p>当队列已满，Capacity 调度器不会强制释放Container，当一个队列资源不够用时，这个队列只能获得其它队列释放后的Container资源，这个称为“弹性队列”，也可以设置最大值，防止过多占用其他队列的资源。 </p><h2 id="0x02-Fair-Scheduler"><a href="#0x02-Fair-Scheduler" class="headerlink" title="0x02 Fair Scheduler"></a>0x02 Fair Scheduler</h2><p>Fair 调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。如下图所示，当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。 </p><p><img src="https://i.loli.net/2019/11/17/Y97D6UeqdlLRivj.png" alt="Fair Scheduler"></p><p>需要注意的是，在上图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器既提高了的资源利用率又能保证小任务及时完成。 </p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="noopener">Hadoop: Capacity Scheduler</a><br><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">Hadoop: Fair Scheduler</a><br><a href="https://blog.csdn.net/suifeng3051/article/details/49508261" target="_blank" rel="noopener">Yarn 调度器Scheduler详解</a><br><a href="https://jordenbruce.com/2019/10/16/hadoop-yarn/">YARN技术原理</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;理想情况下，我们应用对 Yarn 资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在 Yarn 中，负责给应用分配资源的是 Scheduler 调度器；而调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此，Yarn 提供了三种调度器和可配置的策略供我们选择。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="yarn" scheme="https://jordenbruce.com/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>Hive数据倾斜之GroupBy</title>
    <link href="https://jordenbruce.com/2019/11/16/hive-skew-group/"/>
    <id>https://jordenbruce.com/2019/11/16/hive-skew-group/</id>
    <published>2019-11-16T11:36:45.000Z</published>
    <updated>2019-11-17T04:02:26.213Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 中分组聚合的常用子句是 Group By；在执行过程中，将 Group By 的字段组合为map的输出key值，利用 MapReduce 的排序，在reduce阶段保存LastKey区分不同的key；如果 Group By 的字段组合出现了数据分布不均匀，就会导致 Hive 数据倾斜；另外 Group By 一般与聚合函数一起使用，比如 SUM() 和 COUNT() 等。<br><a id="more"></a></p><h2 id="0x00-Group-By-典型场景"><a href="#0x00-Group-By-典型场景" class="headerlink" title="0x00 Group By 典型场景"></a>0x00 Group By 典型场景</h2><p>假设有一张表 dwd_bhv_log_di(logid,uuid,city,provice,dt)，需求是统计每个省份的UV与PV；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---常规实现方式</span><br><span class="line">select provice, count(distinct uuid) uv, count(logid) pv</span><br><span class="line">from dwd_bhv_log_di</span><br><span class="line">where dt = &apos;2019-11-10&apos;</span><br><span class="line">group by provice;</span><br></pre></td></tr></table></figure><h2 id="0x01-Group-By-优化"><a href="#0x01-Group-By-优化" class="headerlink" title="0x01 Group By 优化"></a>0x01 Group By 优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">set hive.map.aggr=true;</span><br><span class="line">set mapred.reduce.tasks=10;</span><br><span class="line">set hive.groupby.skewdata=true;</span><br><span class="line"></span><br><span class="line">select provice, sum(part_uv) uv, sum(part_pv) pv</span><br><span class="line">from (</span><br><span class="line">  select provice, part, count(uuid) part_uv, sum(part_pv) part_pv</span><br><span class="line">  from (</span><br><span class="line">    select provice, uuid, count(logid) part_pv, ceil(rand()*100) part</span><br><span class="line">from dwd_bhv_log_di</span><br><span class="line">where dt = &apos;2019-11-10&apos;</span><br><span class="line">group by provice, uuid</span><br><span class="line">  ) t1</span><br><span class="line">  group by provice, part</span><br><span class="line">) t2</span><br><span class="line">group by provice;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/10/13/hadoop-mapreduce/">MapReduce执行过程详解</a><br><a href="https://jordenbruce.com/2019/09/26/hql-select/">HiveQL的Select语句</a><br><a href="https://blog.csdn.net/u013668852/article/details/79866931" target="_blank" rel="noopener">Hive Group By的实现原理</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 中分组聚合的常用子句是 Group By；在执行过程中，将 Group By 的字段组合为map的输出key值，利用 MapReduce 的排序，在reduce阶段保存LastKey区分不同的key；如果 Group By 的字段组合出现了数据分布不均匀，就会导致 Hive 数据倾斜；另外 Group By 一般与聚合函数一起使用，比如 SUM() 和 COUNT() 等。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive数据倾斜之Join</title>
    <link href="https://jordenbruce.com/2019/11/16/hive-skew-join/"/>
    <id>https://jordenbruce.com/2019/11/16/hive-skew-join/</id>
    <published>2019-11-16T05:49:36.000Z</published>
    <updated>2019-11-17T03:59:18.156Z</updated>
    
    <content type="html"><![CDATA[<p>按照 Kimball 维度模型设计出来的数据表，在开发 ETL 的过程中，不可避免地会使用到多表 Join 关联。在 Join 关联两个表时，每个表 Map 端输出的 key 会带上表别名标识，经过 shuffle 分发到 Reduce 端就有可能会出现数据分布不均匀，从而造成数据倾斜，影响业务的数据质量。然而，相对 count(distinct) 来说，Join 产生数据倾斜的场景会多一些，本文将会介绍几种 Join 典型场景并给出优化实践。<br><a id="more"></a></p><h2 id="0x00-说在前面"><a href="#0x00-说在前面" class="headerlink" title="0x00 说在前面"></a>0x00 说在前面</h2><p>假设有两张表：一张表是用户打开APP列表 device_open_app(device,appid)；另一张表是作弊用户表 device_cheat(device)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select a.device, a.appid, if(b.device is null,0,1) as is_cheat</span><br><span class="line">from device_open_app a</span><br><span class="line">left join device_cheat b</span><br><span class="line">on a.device = b.device</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x01-通用-Join-优化方式"><a href="#0x01-通用-Join-优化方式" class="headerlink" title="0x01 通用 Join 优化方式"></a>0x01 通用 Join 优化方式</h2><ul><li>做好列裁剪和filter操作，以达到两表做join时数据量相对较小；</li><li>关联时小表在前大表在后；</li><li>确保关联条件的字段类型是一致的；</li><li>选择join key分布最均匀的表作为驱动表；</li><li>设置在map端聚合，set hive.map.aggr=true(默认开启)；</li><li>可选set hive.optimize.skewjoin=false(默认关闭)；</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">set hive.map.aggr=true;</span><br><span class="line">set hive.optimize.skewjoin=true;</span><br><span class="line"></span><br><span class="line">select a.device, b.appid</span><br><span class="line">from (</span><br><span class="line">  select device</span><br><span class="line">  from device_cheat</span><br><span class="line">  where device is not null</span><br><span class="line">) a</span><br><span class="line">inner join (</span><br><span class="line">  select device, appid</span><br><span class="line">  from device_open_app</span><br><span class="line">  where device is not null</span><br><span class="line">) b</span><br><span class="line">on cast(a.device as string) = b.device</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x02-小表与大表-Join-优化方式"><a href="#0x02-小表与大表-Join-优化方式" class="headerlink" title="0x02 小表与大表 Join 优化方式"></a>0x02 小表与大表 Join 优化方式</h2><p>这是一种典型的场景，Hive 使用 MapJoin 方式对关联进行优化。</p><p><img src="https://i.loli.net/2019/11/16/UqHT9vapmeWKRfF.png" alt="MapJoin 原理"></p><p>MapJoin 简单说就是在 Map 阶段将小表读入分布式缓存，然后顺序扫描大表完成 Join。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---在0.7.0版本之后</span><br><span class="line">set hive.auto.convert.join=true;</span><br><span class="line"></span><br><span class="line">select /*+MAPJOIN(b)*/ a.device, a.appid</span><br><span class="line">  ,if(b.device is null,0,1) as is_cheat</span><br><span class="line">from device_open_app a</span><br><span class="line">left join device_cheat b</span><br><span class="line">on a.device = b.device</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="0x03-大表与大表-Join-优化方式"><a href="#0x03-大表与大表-Join-优化方式" class="headerlink" title="0x03 大表与大表 Join 优化方式"></a>0x03 大表与大表 Join 优化方式</h2><p>这种关联的场景比较多，最常见的场景是业务数据本身的特性，比如关联字段有大量的空值或特殊值；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">set hive.auto.convert.join=true;</span><br><span class="line"></span><br><span class="line">select a.device, a.appid, if(b.device is null,0,1) as is_cheat</span><br><span class="line">from (</span><br><span class="line">  select device, appid</span><br><span class="line">  from device_open_app</span><br><span class="line">  where device is not null</span><br><span class="line">) a</span><br><span class="line">left join (</span><br><span class="line">  select device</span><br><span class="line">  from device_cheat</span><br><span class="line">  where device is not null</span><br><span class="line">) b</span><br><span class="line">on if(a.device=&apos;0&apos;, concat(&apos;hive&apos;,rand()), a.device) = cast(b.device as string)</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/10/13/hadoop-mapreduce/">MapReduce执行过程详解</a><br><a href="https://jordenbruce.com/2019/09/26/hql-select/">HiveQL的Select语句</a><br><a href="http://ju.outofmemory.cn/entry/786" target="_blank" rel="noopener">Hive – JOIN实现过程</a><br><a href="https://blog.csdn.net/yeweiouyang/article/details/45665727" target="_blank" rel="noopener">Hive数据倾斜（大表join大表）</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;按照 Kimball 维度模型设计出来的数据表，在开发 ETL 的过程中，不可避免地会使用到多表 Join 关联。在 Join 关联两个表时，每个表 Map 端输出的 key 会带上表别名标识，经过 shuffle 分发到 Reduce 端就有可能会出现数据分布不均匀，从而造成数据倾斜，影响业务的数据质量。然而，相对 count(distinct) 来说，Join 产生数据倾斜的场景会多一些，本文将会介绍几种 Join 典型场景并给出优化实践。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive数据倾斜之Distinct</title>
    <link href="https://jordenbruce.com/2019/11/16/hive-skew-distinct/"/>
    <id>https://jordenbruce.com/2019/11/16/hive-skew-distinct/</id>
    <published>2019-11-16T01:56:26.000Z</published>
    <updated>2019-11-17T03:57:02.623Z</updated>
    
    <content type="html"><![CDATA[<p>在 Hive 选择 MapReduce 为执行引擎的前提下，由于使用了 distinct，导致在 map 端的 combine 无法合并重复数据；对于这种 count() 全聚合操作时，即使设定了 reduce task 个数，hive 也只会启动一个 reducer。这就造成了所有 map 端传来的数据都在一个 task 中执行，成为了性能瓶颈。<br><a id="more"></a></p><h2 id="0x00-单字段-count-distinct-优化"><a href="#0x00-单字段-count-distinct-优化" class="headerlink" title="0x00 单字段 count(distinct) 优化"></a>0x00 单字段 count(distinct) 优化</h2><p>典型场景：某一天的日志数据有上亿条，计算当天的活跃用户数；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">---常规实现</span><br><span class="line">select count(distinct uuid) from log where dt = &apos;2019-11-15&apos;;</span><br><span class="line"></span><br><span class="line">---1.优化方式一</span><br><span class="line">select count(*) from (</span><br><span class="line">  select distinct uuid from log where dt = &apos;2019-11-15&apos;</span><br><span class="line">) t;</span><br><span class="line"></span><br><span class="line">---2.优化方式二</span><br><span class="line">select count(*) from (</span><br><span class="line">  select uuid </span><br><span class="line">  from log where dt = &apos;2019-11-15&apos;</span><br><span class="line">  group by uuid</span><br><span class="line">) t;</span><br><span class="line"></span><br><span class="line">---3.优化方式三</span><br><span class="line">select sum(part)</span><br><span class="line">from (</span><br><span class="line">  select tag, count(*) as part</span><br><span class="line">  from (</span><br><span class="line">    select uuid, cast(rand() * 100 as bigint) as tag</span><br><span class="line">    from log where dt = &apos;2019-11-15&apos; </span><br><span class="line">    group by uuid</span><br><span class="line">  ) t1</span><br><span class="line">  group by tag</span><br><span class="line">) t2;</span><br><span class="line"></span><br><span class="line">---辅助优化参数</span><br><span class="line">set mapreduce.job.reduces=100;</span><br></pre></td></tr></table></figure><h2 id="0x01-多字段-count-distinct-优化"><a href="#0x01-多字段-count-distinct-优化" class="headerlink" title="0x01 多字段 count(distinct) 优化"></a>0x01 多字段 count(distinct) 优化</h2><p>典型场景：某一天的日志数据有上亿条，计算当天的活跃设备数和活跃用户数；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">---常规实现</span><br><span class="line">select count(distinct device), count(distinct member)</span><br><span class="line">from log where dt = &apos;2019-11-15&apos;;</span><br><span class="line"></span><br><span class="line">---通用优化方式</span><br><span class="line">select count(device), count(member)</span><br><span class="line">from (</span><br><span class="line">    select device, null as member</span><br><span class="line">    from log where dt = &apos;2019-11-15&apos; </span><br><span class="line">    group by device</span><br><span class="line">  union all</span><br><span class="line">    select null as device, member</span><br><span class="line">    from log where dt = &apos;2019-11-15&apos; </span><br><span class="line">    group by member</span><br><span class="line">) t;</span><br><span class="line"></span><br><span class="line">---辅助优化参数</span><br><span class="line">set mapreduce.job.reduces=100;</span><br></pre></td></tr></table></figure><h2 id="0x02-优化思路"><a href="#0x02-优化思路" class="headerlink" title="0x02 优化思路"></a>0x02 优化思路</h2><p>由于 Hive 的执行引擎是 MapReduce，而 MapReduce 的设计思想是分而治之，所以解决数据倾斜的根本是将 map 输出数据经过 hash 均匀地分配到 reduce 中，再进行聚合等。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/10/13/hadoop-mapreduce/">MapReduce执行过程详解</a><br><a href="https://jordenbruce.com/2019/09/26/hql-select/">HiveQL的Select语句</a><br><a href="http://ju.outofmemory.cn/entry/784" target="_blank" rel="noopener">Hive – Distinct 的实现</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Hive 选择 MapReduce 为执行引擎的前提下，由于使用了 distinct，导致在 map 端的 combine 无法合并重复数据；对于这种 count() 全聚合操作时，即使设定了 reduce task 个数，hive 也只会启动一个 reducer。这就造成了所有 map 端传来的数据都在一个 task 中执行，成为了性能瓶颈。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>ShuffleError - error in shuffle in fetcher</title>
    <link href="https://jordenbruce.com/2019/11/09/hive-shuffle-oom/"/>
    <id>https://jordenbruce.com/2019/11/09/hive-shuffle-oom/</id>
    <published>2019-11-09T13:41:04.000Z</published>
    <updated>2019-11-16T04:42:36.024Z</updated>
    
    <content type="html"><![CDATA[<p>最近开发的一个 HQL 脚本，扔到集群上跑的时候报 shuffle 阶段出现 OOM 的错误，这个问题之前没有遇到过；于是上网搜了下，发现网友也遇到过类似的问题，详细阅读后再结合 MapReduce 任务的 shuffle 原理，将问题解决的过程记录下来，方便以后查阅。<br><a id="more"></a></p><h2 id="0x00-报错信息"><a href="#0x00-报错信息" class="headerlink" title="0x00 报错信息"></a>0x00 报错信息</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">- Task with the most failures(4): </span><br><span class="line">- -----</span><br><span class="line">- Task ID:</span><br><span class="line">- task_1548652929194_403817_r_000214</span><br><span class="line">- </span><br><span class="line">- URL:</span><br><span class="line">- http://0.0.0.0:8088/taskdetails.jsp?jobid=job_1548652929194_403817&amp;tipid=task_1548652929194_403817_r_000214</span><br><span class="line">- -----</span><br><span class="line">- Diagnostic Messages for this Task:</span><br><span class="line">- Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#19</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">- at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)</span><br><span class="line">- at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)</span><br><span class="line">- at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">- at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">- at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1727)</span><br><span class="line">- at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</span><br><span class="line">- Caused by: java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">- at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56)</span><br><span class="line">- at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:305)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:295)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:514)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)</span><br><span class="line">- at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)</span><br></pre></td></tr></table></figure><h2 id="0x01-原因分析"><a href="#0x01-原因分析" class="headerlink" title="0x01 原因分析"></a>0x01 原因分析</h2><p>(1) 文件拷贝：默认情况下，当整个 MapReduce 作业的所有已执行完成的 Map Task 任务数超过 Map Task 总数的 mapreduce.job.reduce.slowstart.completedmaps (默认为0.05) 后，ApplicationMaster 便会开始调度执行 Reduce Task 任务。然后 Reduce Task 任务默认启动 mapreduce.reduce.shuffle.parallelcopies (默认为5) 个 MapOutputCopier 线程到已完成的 Map Task 任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</p><p>(2) 内存缓冲区：这个内存缓冲区大小的控制就不像map那样可以通过 mapreduce.task.io.sort.mb 来设定了，而是通过另外一个参数来设置：mapreduce.reduce.shuffle.input.buffer.percent（default 0.7）， 这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。</p><p>(3) 原则上，mapreduce.reduce.shuffle.input.buffer.percent * mapreduce.reduce.shuffle.parallelcopies 必须小于等于1，否则就会出现如上错误。但是，这两个参数默认值的乘积为3.5，远远超过了1，为什么没有经常抛出以上的错误呢？</p><ul><li>首先，把默认值设为比较大，主要是基于性能考虑，将它们设为比较大，可以大大加快从map复制数据的速度；</li><li>其次，要抛出如上异常，还需满足另外一个条件，就是map任务的数据一下子准备好了等待shuffle去复制，在这种情况下，就会导致shuffle过程的“线程数量”和“内存buffer使用量”都是满负荷的值，自然就造成了内存不足的错误；而如果map任务的数据是断断续续完成的，那么没有一个时刻shuffle过程的“线程数量”和“内存buffer使用量”是满负荷值的，自然也就不会抛出如上错误。</li></ul><h2 id="0x02-调优方式"><a href="#0x02-调优方式" class="headerlink" title="0x02 调优方式"></a>0x02 调优方式</h2><p>通过上述的原因分析，调优方式如下：</p><ul><li>将 mapreduce.reduce.shuffle.input.buffer.percent 参数调小，辅助参数 mapreduce.reduce.shuffle.parallelcopies；</li><li>将 mapreduce.reduce.java.opts 参数调大，辅助参数 mapreduce.reduce.memory.mb；</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://jordenbruce.com/2019/11/09/hive-oom/">HQL内存溢出的参数调优</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近开发的一个 HQL 脚本，扔到集群上跑的时候报 shuffle 阶段出现 OOM 的错误，这个问题之前没有遇到过；于是上网搜了下，发现网友也遇到过类似的问题，详细阅读后再结合 MapReduce 任务的 shuffle 原理，将问题解决的过程记录下来，方便以后查阅。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HQL内存溢出的参数调优</title>
    <link href="https://jordenbruce.com/2019/11/09/hive-oom/"/>
    <id>https://jordenbruce.com/2019/11/09/hive-oom/</id>
    <published>2019-11-09T13:04:49.000Z</published>
    <updated>2019-11-17T11:36:10.220Z</updated>
    
    <content type="html"><![CDATA[<p>我们在使用 Hive 进行 ETL 开发的过程中，关注更多的是使用 HQL 语言来准确地表达业务逻辑，而很少考虑到 Hive 对 HQL 语句的执行情况。当你辛辛苦苦地码完，将 HQL 语句扔给 Hive 去执行时，就有可能出现各种各样的报错，而其中一种比较常见的错误就是内存溢出（OOM，out of memory），通俗地讲就是内存不够。<br><a id="more"></a></p><h2 id="0x00-写在前面"><a href="#0x00-写在前面" class="headerlink" title="0x00 写在前面"></a>0x00 写在前面</h2><p>本文采用的软件版本如下：</p><ul><li>hive-2.0.1</li><li>hadoop-2.7.2</li></ul><p>Hive 使用 MapReduce 执行引擎，Hadoop 使用 Yarn 进行资源调度。</p><p>接下来将从客户端提交 HQL 语句开始，Hive 生成物理执行计划、Yarn 资源分配、MapReduce 执行，到执行结束，三个重点过程进行阐述，先理论再参数，希望 OOM 参数调优的问题得到收敛。</p><h2 id="0x01-Hive-生成物理执行计划"><a href="#0x01-Hive-生成物理执行计划" class="headerlink" title="0x01 Hive 生成物理执行计划"></a>0x01 Hive 生成物理执行计划</h2><p>先给出官网上关于 Hive 架构的经典流程图：</p><p><img src="https://i.loli.net/2019/02/24/5c72ad01e0c56.png" alt="Hive架构"></p><p>从这张图中，我们只需要明白一点即可：客户端提交的HQL语句，在Hive端的最终输出是物理执行计划，或者说是Job的有向无环图（a DAG of stages）。主要包括三种操作：</p><ul><li>MapReduce作业（a map/reduce job）</li><li>元数据操作（a metadata operation）</li><li>HDFS操作（an operation on HDFS）</li></ul><p>需要说明的是：这个过程主要是Hive优化器的执行，没有相关参数去控制内存的使用。当然，对于某些HQL语句适当地设置一些参数，可以得到更优的物理执行计划。比如常见的 Map Join 参数 <code>hive.auto.convert.join</code> 等。</p><h2 id="0x02-Yarn-资源分配"><a href="#0x02-Yarn-资源分配" class="headerlink" title="0x02 Yarn 资源分配"></a>0x02 Yarn 资源分配</h2><p>YARN 是对 Mapreduce V1 重构得到的，有时候也成为 MapReduce V2。由Hive生成物理执行计划，其中的MapReduce作业提交给Yarn来执行，详细的执行过程如下：</p><p><img src="https://i.loli.net/2019/02/24/5c72b4b3294ea.jpg" alt="MapReduce在Yarn下执行过程"></p><p>从上图可以看出，Yarn（以Container方式分配）控制着 NodeManager、ApplicationMaster、Map和Reduce 的内存使用，相关的内存参数有：</p><table><thead><tr><th>重要级别</th><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>中</td><td>yarn.nodemanager.resource.memory-mb</td><td>8192</td></tr><tr><td>中</td><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td></tr><tr><td>中</td><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td></tr><tr><td>中</td><td>yarn.scheduler.increment-allocation-mb</td><td>1024</td></tr><tr><td>高</td><td>yarn.app.mapreduce.am.resource.mb</td><td>1536</td></tr><tr><td>中</td><td>yarn.app.mapreduce.am.command-opts</td><td>-Xmx1024m</td></tr><tr><td>低</td><td>yarn.app.mapreduce.am.admin-command-opts</td><td></td></tr><tr><td>中</td><td>yarn.nodemanager.vmem-pmem-ratio</td><td>2.1</td></tr><tr><td>低</td><td>yarn.nodemanager.pmem-check-enabled</td><td>true</td></tr><tr><td>低</td><td>yarn.nodemanager.vmem-check-enabled</td><td>true</td></tr><tr><td>高</td><td>mapreduce.reduce.memory.mb</td><td>1024</td></tr><tr><td>中</td><td>mapreduce.reduce.java.opts</td><td></td></tr><tr><td>高</td><td>mapreduce.map.memory.mb</td><td>1024</td></tr><tr><td>中</td><td>mapreduce.map.java.opts</td></tr></tbody></table><h3 id="1-基础"><a href="#1-基础" class="headerlink" title="(1) 基础"></a>(1) 基础</h3><ul><li>NodeManager可用于分配的最大内存是yarn.nodemanager.resource.memory-mb；</li><li>Yarn的ResourceManger（简称RM）通过逻辑上的队列分配内存等资源给application，默认情况下RM允许最大AM申请Container资源为8192MB(“yarn.scheduler.maximum-allocation-mb“)，默认情况下的最小分配资源为1024M(“yarn.scheduler.minimum-allocation-mb“)，如果参数中需要的资源在此范围之外，在任务submit的时候会被直接拒绝掉；</li><li>AM只能以增量 (“yarn.scheduler.minimum-allocation-mb”) + (“yarn.scheduler.increment-allocation-mb”) 规整每个task需要的内存，并且申请的内存只能在（”yarn.scheduler.minimum-allocation-mb“）和(“yarn.scheduler.maximum-allocation-mb“) 的范围内向RM申请资源；</li><li>每个Map任务或Reduce任务分配的内存为mapreduce.reduce.memory.mb或mapreduce.map.memory.mb；</li></ul><h3 id="2-mapreduce-map-java-opts-和-mapreduce-map-memory-mb-区别"><a href="#2-mapreduce-map-java-opts-和-mapreduce-map-memory-mb-区别" class="headerlink" title="(2) mapreduce.map.java.opts 和 mapreduce.map.memory.mb 区别"></a>(2) mapreduce.map.java.opts 和 mapreduce.map.memory.mb 区别</h3><p>JVM进程跑在container中，mapreduce.map.java.opts 能够通过Xmx设置JVM最大的heap的使用，一般设置为0.75倍的 mapreduce.map.memory.mb ，因为需要为java code，非JVM内存使用等预留些空间；mapreduce.reduce.java.opts 和 mapreduce.reduce.memory.mb 同理。</p><h3 id="3-虚拟内存"><a href="#3-虚拟内存" class="headerlink" title="(3) 虚拟内存"></a>(3) 虚拟内存</h3><p>默认的(“yarn.nodemanager.vmem-pmem-ratio“)设置为2.1，意味则 map container 或者 reduce container 分配的虚拟内存超过2.1倍的(“mapreduce.reduce.memory.mb“)或(“mapreduce.map.memory.mb“)就会被NM给KILL掉，如果 (“mapreduce.map.memory.mb”) 被设置为1536M那么总的虚拟内存为2.1*1536=3225.6MB</p><h3 id="4-内存检查"><a href="#4-内存检查" class="headerlink" title="(4) 内存检查"></a>(4) 内存检查</h3><p>如果虚拟内存检查被打开（yarn.nodemanager.vmem-check-enabled 默认情况下为true），然后YARN将把抽取出来的容器及其子进程的VSIZE加起来和容器最大允许使用的虚拟内存进行比较。最大允许使用的虚拟内存是容器最大可使用的物理内存乘以 yarn.nodemanager.vmem-pmem-ratio（默认值是2.1）。所以，如果你的YARN容器配置的最大可使用物理内存为2GB，然后我们乘以 2.1 得到的就是容器最大可用的虚拟内存 4.2G 。</p><p>如果物理内存检查被打开（yarn.nodemanager.pmem-check-enabled 默认情况为true），然后YARN将把抽取出来的容器及其子进程的RSS加起来和容器最大允许使用的物理内存进行比较。</p><p>如果物理内存或者虚拟内存其中一个的使用大于最大允许使用的，YARN将会被这个容器杀掉。</p><h3 id="5-参数全局图"><a href="#5-参数全局图" class="headerlink" title="(5) 参数全局图"></a>(5) 参数全局图</h3><p>参数多不要慌，下面来张图梳理下：</p><p><img src="https://i.loli.net/2019/02/25/5c72c07905cf3.jpg" alt="Yarn内存参数"></p><h2 id="0x03-MapReduce执行"><a href="#0x03-MapReduce执行" class="headerlink" title="0x03 MapReduce执行"></a>0x03 MapReduce执行</h2><p>MapReduce作业的重点是Shuffle过程，还是老套路，先给出官网上关于这个过程的经典流程图：</p><p><img src="https://i.loli.net/2019/11/09/Rh5CrkgyBeT6XSV.png" alt="Shuffle过程"></p><p>当Map任务或Reduce任务以Container方式申请到相应的内存资源后，就进入了实际的执行过程中，其中涉及的参数有：</p><table><thead><tr><th>重要级别</th><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>高</td><td>mapreduce.job.maps</td><td>2</td></tr><tr><td>中</td><td>mapreduce.input.fileinputformat.split.minsize</td><td>1</td></tr><tr><td>中</td><td>dfs.blocksize</td><td>134217728</td></tr><tr><td>高</td><td>mapreduce.job.reduces</td><td>1</td></tr><tr><td>中</td><td>mapreduce.task.io.sort.mb</td><td>100</td></tr><tr><td>中</td><td>mapreduce.map.sort.spill.percent</td><td>0.80</td></tr><tr><td>中</td><td>mapreduce.task.io.sort.factor</td><td>10</td></tr><tr><td>中</td><td>mapreduce.map.output.compress</td><td>false</td></tr><tr><td>中</td><td>mapreduce.map.output.compress.codec</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>低</td><td>mapreduce.job.reduce.slowstart.completedmaps</td><td>0.05</td></tr><tr><td>中</td><td>mapreduce.reduce.shuffle.parallelcopies</td><td>5</td></tr><tr><td>高</td><td>mapreduce.reduce.shuffle.input.buffer.percent</td><td>0.70</td></tr></tbody></table><p>为了更好地理解每个参数作用的阶段，建议先阅读 <a href="http://matt33.com/2016/03/02/hadoop-shuffle/" target="_blank" rel="noopener">MapReduce之Shuffle过程详解</a>。</p><h3 id="1-Map任务"><a href="#1-Map任务" class="headerlink" title="(1) Map任务"></a>(1) Map任务</h3><p>（1）split分片：split是在逻辑上对输入数据进行的分片，并不会在磁盘上将其切分成分片进行存储。每个split都作为一个独立单位分配给一个map task去处理。决定split分片大小的参数有：</p><ul><li>mapreduce.job.maps</li><li>mapreduce.input.fileinputformat.split.minsize</li><li>dfs.blocksize (会话级别不可设置)</li></ul><p>（2）内存缓冲区：经过map处理后的键值对，不会立马写入磁盘，而是暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，设置缓冲区大小的参数有：</p><ul><li>mapreduce.task.io.sort.mb</li><li>mapreduce.map.sort.spill.percent</li></ul><p>（3）压缩：map端在写磁盘的时候采用压缩的方式将map的输出结果进行压缩是一个减少网络开销很有效的方法。其实，在Hadoop中早已为我们提供了一些压缩算法的实现，直接配置参数即可。</p><ul><li>mapreduce.map.output.compress</li><li>mapreduce.map.output.compress.codec</li></ul><h3 id="2-Reduce任务"><a href="#2-Reduce任务" class="headerlink" title="(2) Reduce任务"></a>(2) Reduce任务</h3><p>（1）文件拷贝：默认情况下，当整个MapReduce作业的所有已执行完成的Map Task任务数超过Map Task总数的 <code>mapreduce.job.reduce.slowstart.completedmaps</code> (默认为0.05) 后，ApplicationMaster便会开始调度执行Reduce Task任务。然后Reduce Task任务默认启动 <code>mapred.reduce.parallel.copies</code> (默认为5) 个MapOutputCopier线程到已完成的Map Task任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</p><p>（2）内存缓冲区：这个内存缓冲区大小的控制就不像map那样可以通过 <code>mapreduce.task.io.sort.mb</code> 来设定了，而是通过另外一个参数来设置：<code>mapred.job.shuffle.input.buffer.percent</code>（default 0.7）， 这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。</p><h2 id="0x04-HQL语句的日志输出"><a href="#0x04-HQL语句的日志输出" class="headerlink" title="0x04 HQL语句的日志输出"></a>0x04 HQL语句的日志输出</h2><p>经过漫长的理论铺垫，终于要到解决问题的时候了，HQL语句的内存溢出主要从日志分析开始。</p><ul><li>HQL语句的执行过程中，有哪些日志输出呢？分别存放在什么地方？如何分析出有用信息？</li><li>内存溢出包括哪几类？典型日志有哪些？调优什么参数可以解决？</li><li>小文件太多是如何产生的？调优什么参数可以合并小文件？</li></ul><p>等等一系列有关问题，我们下一次接着说。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Design" target="_blank" rel="noopener">Hive Architecture Overview</a><br><a href="https://segmentfault.com/a/1190000003777237" target="_blank" rel="noopener">Yarn下Mapreduce的内存参数理解</a><br><a href="https://blog.csdn.net/lazythinker/article/details/75497774" target="_blank" rel="noopener">HIVE参数调优（汇总）</a><br><a href="https://blog.csdn.net/aijiudu/article/details/72353510" target="_blank" rel="noopener">MapReduce过程详解及其性能优化</a><br><a href="https://my.oschina.net/OttoWu/blog/816049" target="_blank" rel="noopener">hadoop fair scheduler 的坑</a><br><a href="https://blog.csdn.net/suifeng3051/article/details/48135521" target="_blank" rel="noopener">Yarn 内存分配管理机制及相关参数配置</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在使用 Hive 进行 ETL 开发的过程中，关注更多的是使用 HQL 语言来准确地表达业务逻辑，而很少考虑到 Hive 对 HQL 语句的执行情况。当你辛辛苦苦地码完，将 HQL 语句扔给 Hive 去执行时，就有可能出现各种各样的报错，而其中一种比较常见的错误就是内存溢出（OOM，out of memory），通俗地讲就是内存不够。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数GenericUDAF</title>
    <link href="https://jordenbruce.com/2019/11/09/hive-genericudaf/"/>
    <id>https://jordenbruce.com/2019/11/09/hive-genericudaf/</id>
    <published>2019-11-09T11:37:59.000Z</published>
    <updated>2019-11-09T11:41:45.847Z</updated>
    
    <content type="html"><![CDATA[<p>UDAF 是 Hive 中用户自定义的聚合函数，特点是输入多行输出一行，Hive 内置 UDAF 函数包括有 sum() 与 count() 等。UDAF 实现有简单与通用两种方式，简单 UDAF 因为使用 Java 反射导致性能损失，而且有些特性不能使用，已经被弃用了；在本文中我们将关注 Hive 中自定义聚合函数-GenericUDAF，即通用方式。<br><a id="more"></a></p><h2 id="0x00-自定义-GenericUDAF-开发"><a href="#0x00-自定义-GenericUDAF-开发" class="headerlink" title="0x00 自定义 GenericUDAF 开发"></a>0x00 自定义 GenericUDAF 开发</h2><p>编写 GenericUDAF 需要下面两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver</code> ，重写 <code>getEvaluator</code> 函数；</li><li>依据 <code>getEvaluator</code> 函数返回值，编写内部静态类，继承 <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator</code>，并重写其7个方法；</li></ul><p>上述过程涉及两个重要抽象类 Resolver 与 Evaluator，其中 Resolver 官方不建议使用 GenericUDAFResolver2 接口，而使用 AbstractGenericUDAFResolver 接口；Evaluator 使用 GenericUDAFEvaluator 接口，并且必须是 public static 子类，需要重写的方法有 <code>init</code>，<code>getNewAggregationBuffer</code>，<code>iterate</code>，<code>terminatePartial</code>，<code>merge</code>，<code>terminate</code>，<code>reset</code> 共7个；理解 Evaluator 之前，必须先理解 ObjectInspector 接口 与 Model 内部类；另外，UDAF 逻辑处理主要发生在 Evaluator 中。</p><p>ObjectInspector 作用主要是解耦数据使用与数据格式，使得数据流在输入输出端切换不同的输入输出格式，不同的 Operator 上使用不同的格式。</p><p>Model 代表了 UDAF 在 MapReduce 的各个阶段，具体如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public static enum Mode &#123;</span><br><span class="line">  /**</span><br><span class="line">   * PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合</span><br><span class="line">   * 将会调用iterate()和terminatePartial()</span><br><span class="line">   */</span><br><span class="line">  PARTIAL1,</span><br><span class="line">  /**</span><br><span class="line">   * PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合:</span><br><span class="line">   * 将会调用merge() 和 terminatePartial() </span><br><span class="line">   */</span><br><span class="line">  PARTIAL2,</span><br><span class="line">  /**</span><br><span class="line">   * FINAL: mapreduce的reduce阶段:从部分数据的聚合到完全聚合 </span><br><span class="line">   * 将会调用merge()和terminate()</span><br><span class="line">   */</span><br><span class="line">  FINAL,</span><br><span class="line">  /**</span><br><span class="line">   * COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了:从原始数据直接到完全聚合</span><br><span class="line">   * 将会调用 iterate()和terminate()</span><br><span class="line">   */</span><br><span class="line">  COMPLETE</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>一般情况下，完整的 UDAF 逻辑是一个 mapreduce 过程，如果有 mapper 和 reducer，就会经历 PARTIAL1(mapper)，FINAL(reducer)，如果还有 combiner，那就会经历 PARTIAL1(mapper)，PARTIAL2(combiner)，FINAL(reducer)。而有一些情况下的 mapreduce，只有 mapper 没有 reducer，所以就会只有 COMPLETE 阶段，这个阶段直接输入原始数据，出结果。</p><p><img src="https://i.loli.net/2019/11/09/xHGvRTEm5O9jk34.png" alt="Model各阶段对应Evaluator方法调用"></p><h2 id="0x01-散度-DivergenceGenericUDAF-示例代码"><a href="#0x01-散度-DivergenceGenericUDAF-示例代码" class="headerlink" title="0x01 散度 DivergenceGenericUDAF 示例代码"></a>0x01 散度 DivergenceGenericUDAF 示例代码</h2><p><img src="https://i.loli.net/2019/11/09/nGODqcvX7Ywr3xi.png" alt="散度数学公式"></p><p>上述公式的参考代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;</span><br><span class="line">import org.apache.hadoop.hive.serde2.io.DoubleWritable;</span><br><span class="line">import org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.ql.util.JavaDataModel;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">        name = &quot;divergence&quot;,</span><br><span class="line">        value = &quot;_FUNC_(px, qy) - Calculate divergence from px, qy &quot;)</span><br><span class="line">public class DivergenceGenericUDAF extends AbstractGenericUDAFResolver &#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException &#123;</span><br><span class="line">        if (parameters.length != 2) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(parameters.length - 1,</span><br><span class="line">                    &quot;Exactly two arguments are expected.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (parameters[0].getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(0,</span><br><span class="line">                    &quot;Only primitive type arguments are accepted but &quot;</span><br><span class="line">                            + parameters[0].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (parameters[1].getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(1,</span><br><span class="line">                    &quot;Only primitive type arguments are accepted but &quot;</span><br><span class="line">                            + parameters[1].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        switch (((PrimitiveTypeInfo) parameters[0]).getPrimitiveCategory()) &#123;</span><br><span class="line">            case BYTE:</span><br><span class="line">            case SHORT:</span><br><span class="line">            case INT:</span><br><span class="line">            case LONG:</span><br><span class="line">            case FLOAT:</span><br><span class="line">            case DOUBLE:</span><br><span class="line">            case TIMESTAMP:</span><br><span class="line">            case DECIMAL:</span><br><span class="line">                switch (((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory()) &#123;</span><br><span class="line">                    case BYTE:</span><br><span class="line">                    case SHORT:</span><br><span class="line">                    case INT:</span><br><span class="line">                    case LONG:</span><br><span class="line">                    case FLOAT:</span><br><span class="line">                    case DOUBLE:</span><br><span class="line">                    case TIMESTAMP:</span><br><span class="line">                    case DECIMAL:</span><br><span class="line">                        return new GenericUDAFDivergenceEvaluator();</span><br><span class="line">                    case STRING:</span><br><span class="line">                    case BOOLEAN:</span><br><span class="line">                    case DATE:</span><br><span class="line">                    default:</span><br><span class="line">                        throw new UDFArgumentTypeException(1,</span><br><span class="line">                                &quot;Only numeric or string type arguments are accepted but &quot;</span><br><span class="line">                                        + parameters[1].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">            case STRING:</span><br><span class="line">            case BOOLEAN:</span><br><span class="line">            case DATE:</span><br><span class="line">            default:</span><br><span class="line">                throw new UDFArgumentTypeException(0,</span><br><span class="line">                        &quot;Only numeric or string type arguments are accepted but &quot;</span><br><span class="line">                                + parameters[0].getTypeName() + &quot; is passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class GenericUDAFDivergenceEvaluator extends GenericUDAFEvaluator &#123;</span><br><span class="line"></span><br><span class="line">        private PrimitiveObjectInspector pInputOI;</span><br><span class="line">        private PrimitiveObjectInspector qInputOI;</span><br><span class="line"></span><br><span class="line">        private PrimitiveObjectInspector partialOI;</span><br><span class="line"></span><br><span class="line">        private Object partialResult;</span><br><span class="line"></span><br><span class="line">        private DoubleWritable result;</span><br><span class="line"></span><br><span class="line">        /*</span><br><span class="line">         * PARTIAL1  (input, partial)</span><br><span class="line">         * PARTIAL2  (partial, partial)</span><br><span class="line">         * FINAL     (partial, output)</span><br><span class="line">         * COMPLETE  (input, output)</span><br><span class="line">         */</span><br><span class="line">        @Override</span><br><span class="line">        public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException &#123;</span><br><span class="line">            super.init(m, parameters);</span><br><span class="line">            result = new DoubleWritable(0);</span><br><span class="line">            // initialize input</span><br><span class="line">            if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) &#123;</span><br><span class="line">                assert (parameters.length == 2);</span><br><span class="line">                pInputOI = (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">                qInputOI = (PrimitiveObjectInspector) parameters[1];</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                partialOI = (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">            &#125;</span><br><span class="line">            // initialize output</span><br><span class="line">            if (m == Mode.PARTIAL1 || m == Mode.PARTIAL2) &#123;</span><br><span class="line">                partialOI = (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">                partialResult = new DoubleWritable(0);</span><br><span class="line">                return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                partialResult = new DoubleWritable(0);</span><br><span class="line">                return PrimitiveObjectInspectorFactory.writableDoubleObjectInspector;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        static class SumDoubleAgg extends AbstractAggregationBuffer &#123;</span><br><span class="line">            boolean empty;</span><br><span class="line">            double sum;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public int estimate() &#123; return JavaDataModel.PRIMITIVES1 + JavaDataModel.PRIMITIVES2; &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void reset(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line">            divergenceAgg.empty = true;</span><br><span class="line">            divergenceAgg.sum = 0.0;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = new SumDoubleAgg();</span><br><span class="line">            reset(divergenceAgg);</span><br><span class="line">            return divergenceAgg;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException &#123;</span><br><span class="line">            assert (parameters.length == 2);</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line"></span><br><span class="line">            double vp = PrimitiveObjectInspectorUtils.getDouble(parameters[0], pInputOI);</span><br><span class="line">            double vq = PrimitiveObjectInspectorUtils.getDouble(parameters[1], qInputOI);</span><br><span class="line"></span><br><span class="line">            divergenceAgg.sum += vp * Math.log(vp / vq);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public Object terminatePartial(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line">            partialResult = new DoubleWritable(divergenceAgg.sum);</span><br><span class="line">            return partialResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void merge(AggregationBuffer agg, Object partial) throws HiveException &#123;</span><br><span class="line">            if (partial != null) &#123;</span><br><span class="line">                SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line"></span><br><span class="line">                double subSum = PrimitiveObjectInspectorUtils.getDouble(partial, partialOI);</span><br><span class="line">                divergenceAgg.sum += subSum;</span><br><span class="line"></span><br><span class="line">                divergenceAgg.empty = false;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public Object terminate(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">            SumDoubleAgg divergenceAgg = (SumDoubleAgg) agg;</span><br><span class="line">            if (divergenceAgg.empty) &#123;</span><br><span class="line">                result = null;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                result.set(divergenceAgg.sum);</span><br><span class="line">            &#125;</span><br><span class="line">            return result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x02-代码走读"><a href="#0x02-代码走读" class="headerlink" title="0x02 代码走读"></a>0x02 代码走读</h2><ol><li><p>getEvaluator() 被调用<br>1.1 检查输入参数的个数；<br>1.2 检查输入参数的类型；<br>1.3 根据不同的输入参数类型，返回对应的 GenericUDAFEvaluator；</p></li><li><p>类 GenericUDAFDivergenceEvaluator 的方法说明<br>2.1 init() 方法，定义 mapreduce 不同阶段的输入与输出；<br>2.2 getNewAggregationBuffer() 方法，获取新的中间结果；<br>2.3 iterate() 方法，读取输入行的 p,q 累加至中间结果；<br>2.4 terminatePartial() 方法，输出中间结果；<br>2.5 merge() 方法，聚合中间结果；<br>2.6 terminate() 方法，输出聚合结果；<br>2.7 reset() 方法，重置中间结果；</p></li><li><p>方法调用过程（参考 Model各阶段对应Evaluator方法调用）</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy" target="_blank" rel="noopener">GenericUDAFCaseStudy</a><br><a href="https://blog.csdn.net/kent7306/article/details/50110067" target="_blank" rel="noopener">Hive UDAF开发详解</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;UDAF 是 Hive 中用户自定义的聚合函数，特点是输入多行输出一行，Hive 内置 UDAF 函数包括有 sum() 与 count() 等。UDAF 实现有简单与通用两种方式，简单 UDAF 因为使用 Java 反射导致性能损失，而且有些特性不能使用，已经被弃用了；在本文中我们将关注 Hive 中自定义聚合函数-GenericUDAF，即通用方式。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数GenericUDTF</title>
    <link href="https://jordenbruce.com/2019/11/07/hive-genericudtf/"/>
    <id>https://jordenbruce.com/2019/11/07/hive-genericudtf/</id>
    <published>2019-11-07T12:18:20.000Z</published>
    <updated>2019-11-07T14:20:12.045Z</updated>
    
    <content type="html"><![CDATA[<p>之前介绍的 UDF 特点是输入一行输出一行；本文将要介绍的是 UDTF，其特点是输入一行输出多行，而使用的接口是 GenericUDTF，比 UDF 更为复杂。<br><a id="more"></a></p><h2 id="0x00-自定义-GenericUDTF-开发"><a href="#0x00-自定义-GenericUDTF-开发" class="headerlink" title="0x00 自定义 GenericUDTF 开发"></a>0x00 自定义 GenericUDTF 开发</h2><p>编写 GenericUDTF 需要两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF</code> 类；</li><li>重写 <code>initialize</code> ，<code>process</code> ，<code>close</code> 三个方法；</li></ul><p>每个方法有着不同的作用，参考如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 该方法指定输入输出参数：输入参数的ObjectInspector与输出参数的StructObjectInspector</span><br><span class="line">abstract StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException; </span><br><span class="line"> </span><br><span class="line">// 该方法处理输入记录，然后通过forward()方法返回输出结果</span><br><span class="line">abstract void process(Object[] record) throws HiveException;</span><br><span class="line"> </span><br><span class="line">// 该方法用于通知UDTF没有行可以处理了 </span><br><span class="line">// 另外，可以在该方法中清理代码或者产生额外的输出</span><br><span class="line">abstract void close() throws HiveException;</span><br></pre></td></tr></table></figure><p>其中 process() 方法中有个 forward() 方法需要解释下，对于每一行的输入都会有多行的输出，每一行输出时都要调用 forward() 方法，并定义输出行的格式（一列或多列）。</p><h2 id="0x01-官方-ExplodeGenericUDTF-示例代码"><a href="#0x01-官方-ExplodeGenericUDTF-示例代码" class="headerlink" title="0x01 官方 ExplodeGenericUDTF 示例代码"></a>0x01 官方 ExplodeGenericUDTF 示例代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.Map.Entry;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.TaskExecutionException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">        name = &quot;explode&quot;,</span><br><span class="line">        value = &quot;_FUNC_(a) - separates the elements of array a into multiple rows,&quot;</span><br><span class="line">                + &quot; or the elements of a map into multiple rows and columns &quot;)</span><br><span class="line">public class ExplodeGenericUDTF extends GenericUDTF &#123;</span><br><span class="line"></span><br><span class="line">    private transient ObjectInspector inputOI = null;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close() throws HiveException &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException &#123;</span><br><span class="line">        if (args.length != 1) &#123;</span><br><span class="line">            throw new UDFArgumentException(&quot;explode() takes only one argument&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;();</span><br><span class="line">        ArrayList&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;();</span><br><span class="line"></span><br><span class="line">        switch (args[0].getCategory()) &#123;</span><br><span class="line">            case LIST:</span><br><span class="line">                inputOI = args[0];</span><br><span class="line">                fieldNames.add(&quot;col&quot;);</span><br><span class="line">                fieldOIs.add(((ListObjectInspector)inputOI).getListElementObjectInspector());</span><br><span class="line">                break;</span><br><span class="line">            case MAP:</span><br><span class="line">                inputOI = args[0];</span><br><span class="line">                fieldNames.add(&quot;key&quot;);</span><br><span class="line">                fieldNames.add(&quot;value&quot;);</span><br><span class="line">                fieldOIs.add(((MapObjectInspector)inputOI).getMapKeyObjectInspector());</span><br><span class="line">                fieldOIs.add(((MapObjectInspector)inputOI).getMapValueObjectInspector());</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">                throw new UDFArgumentException(&quot;explode() takes an array or a map as a parameter&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private transient final Object[] forwardListObj = new Object[1];</span><br><span class="line">    private transient final Object[] forwardMapObj = new Object[2];</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void process(Object[] o) throws HiveException &#123;</span><br><span class="line">        switch (inputOI.getCategory()) &#123;</span><br><span class="line">            case LIST:</span><br><span class="line">                ListObjectInspector listOI = (ListObjectInspector)inputOI;</span><br><span class="line">                List&lt;?&gt; list = listOI.getList(o[0]);</span><br><span class="line">                if (list == null) &#123;</span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">                for (Object r : list) &#123;</span><br><span class="line">                    forwardListObj[0] = r;</span><br><span class="line">                    forward(forwardListObj); //输出一行</span><br><span class="line">                &#125;</span><br><span class="line">                break;</span><br><span class="line">            case MAP:</span><br><span class="line">                MapObjectInspector mapOI = (MapObjectInspector)inputOI;</span><br><span class="line">                Map&lt;?,?&gt; map = mapOI.getMap(o[0]);</span><br><span class="line">                if (map == null) &#123;</span><br><span class="line">                    return;</span><br><span class="line">                &#125;</span><br><span class="line">                for (Entry&lt;?,?&gt; r : map.entrySet()) &#123;</span><br><span class="line">                    forwardMapObj[0] = r.getKey();</span><br><span class="line">                    forwardMapObj[1] = r.getValue();</span><br><span class="line">                    forward(forwardMapObj); //输出一行</span><br><span class="line">                &#125;</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">                throw new TaskExecutionException(&quot;explode() can only operate on an array or a map&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return &quot;explode&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x02-代码走读"><a href="#0x02-代码走读" class="headerlink" title="0x02 代码走读"></a>0x02 代码走读</h2><p>方法的调用关系如下：</p><ol><li><p>initialize() 被调用<br>1.1 检查输入参数的个数<br>1.2 检查参数的类型是否为 LIST 或 MAP，并保存 inputOI 用以供 process() 使用<br>1.3 返回 StructObjectInspector，并定义好输出行的格式</p></li><li><p>process() 被调用<br>2.1. 判断输入参数的数据类型<br>2.2. 对于 LIST 类型，每一个元素输出一行，且只有一列<br>2.3. 对于 MAP 类型，每一对键值输出一行，且有两列<br>2.4. 其他数据类型抛出异常</p></li><li><p>close() 被调用，什么都不做</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide+UDTF" target="_blank" rel="noopener">DeveloperGuide UDTF</a><br><a href="https://jordenbruce.com/2019/09/30/hql-function/">HiveQL的函数概览</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前介绍的 UDF 特点是输入一行输出一行；本文将要介绍的是 UDTF，其特点是输入一行输出多行，而使用的接口是 GenericUDTF，比 UDF 更为复杂。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数GenericUDF</title>
    <link href="https://jordenbruce.com/2019/11/06/hive-genericudf/"/>
    <id>https://jordenbruce.com/2019/11/06/hive-genericudf/</id>
    <published>2019-11-06T13:39:44.000Z</published>
    <updated>2019-11-07T14:02:38.818Z</updated>
    
    <content type="html"><![CDATA[<p>编写 Apache Hive 用户自定义函数（UDF）有两个不同的接口：一个是非常简单的 UDF，上一篇已经介绍过了；还有一个是 GenericUDF，相对复杂点。两个 API 的区别是：如果函数的参数和返回都是基础数据类型，那么简单 API（UDF）可以胜任；但是，如果你想写一个函数用来操作内嵌数据结构，如 Map、List 和 Set，此时就需要去熟悉复杂 API（GenericUDF）。<br><a id="more"></a></p><h2 id="0x00-自定义-GenericUDF-开发"><a href="#0x00-自定义-GenericUDF-开发" class="headerlink" title="0x00 自定义 GenericUDF 开发"></a>0x00 自定义 GenericUDF 开发</h2><p>编写 GenericUDF 需要两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.udf.generic.GenericUDF</code> 类；</li><li>重写 <code>initialize</code> ，<code>evaluate</code> ，<code>getDisplayString</code> 三个方法；</li></ul><p>每个方法有着不同的作用，参考如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// 这个方法只调用一次，并且在evaluate()方法之前调用。</span><br><span class="line">// 该方法接受的参数是一个ObjectInspectors数组，表示函数输入参数类型。</span><br><span class="line">// 1.检查接受正确的参数个数；</span><br><span class="line">// 2.检查接受正确的参数类型；</span><br><span class="line">// 3.定义返回值被序列化的一致类型。</span><br><span class="line">abstract ObjectInspector initialize(ObjectInspector[] arguments);</span><br><span class="line"></span><br><span class="line">// 这个方法类似UDF的evaluate()方法。它处理真实的参数，并返回最终结果。</span><br><span class="line">abstract Object evaluate(DeferredObject[] arguments);</span><br><span class="line"></span><br><span class="line">// 这个方法用于当实现的GenericUDF出错的时候，打印出提示信息。</span><br><span class="line">// 而提示信息就是你实现该方法最后返回的字符串。 </span><br><span class="line">abstract String getDisplayString(String[] children);</span><br></pre></td></tr></table></figure><p>其中有个 ObjectInspector 需要解释下，简单来说是帮助使用者访问需要序列化或者反序列化的对象，为数据类型提供一致性的访问接口。有关 ObjectInspector 更深入地理解，请参考 <a href="https://www.jianshu.com/p/5ea006282238" target="_blank" rel="noopener">Hive-ObjectInspector</a></p><h2 id="0x01-官方-ArrayGenericUDF-示例代码"><a href="#0x01-官方-ArrayGenericUDF-示例代码" class="headerlink" title="0x01 官方 ArrayGenericUDF 示例代码"></a>0x01 官方 ArrayGenericUDF 示例代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">        name = &quot;array&quot;,</span><br><span class="line">        value = &quot;_FUNC_(n0, n1...) - Creates an array with the given elements &quot;)</span><br><span class="line">public class ArrayGenericUDF extends GenericUDF &#123;</span><br><span class="line"></span><br><span class="line">    private transient Converter[] converters;</span><br><span class="line">    private transient ArrayList&lt;Object&gt; ret = new ArrayList&lt;Object&gt;();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123;</span><br><span class="line"></span><br><span class="line">        if (arguments.length &lt; 1) &#123;</span><br><span class="line">            throw new UDFArgumentLengthException(&quot;array() takes at least one parameter.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);</span><br><span class="line"></span><br><span class="line">        for (int i = 0; i &lt; arguments.length; i++) &#123;</span><br><span class="line">            if (!returnOIResolver.update(arguments[i])) &#123;</span><br><span class="line">                throw new UDFArgumentTypeException(i, &quot;Argument type \&quot;&quot;</span><br><span class="line">                        + arguments[i].getTypeName()</span><br><span class="line">                        + &quot;\&quot; is different from preceding arguments. &quot;</span><br><span class="line">                        + &quot;Previous type was \&quot;&quot; + arguments[i - 1].getTypeName() + &quot;\&quot;&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        converters = new Converter[arguments.length];</span><br><span class="line"></span><br><span class="line">        ObjectInspector returnOI =</span><br><span class="line">                returnOIResolver.get(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">        for (int i = 0; i &lt; arguments.length; i++) &#123;</span><br><span class="line">            converters[i] = ObjectInspectorConverters.getConverter(arguments[i], returnOI);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return ObjectInspectorFactory.getStandardListObjectInspector(returnOI);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public Object evaluate(DeferredObject[] arguments) throws HiveException &#123;</span><br><span class="line">        ret.clear();</span><br><span class="line">        for (int i = 0; i &lt; arguments.length; i++) &#123;</span><br><span class="line">            ret.add(converters[i].convert(arguments[i].get()));</span><br><span class="line">        &#125;</span><br><span class="line">        return ret;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String getDisplayString(String[] children) &#123;</span><br><span class="line">        return getStandardDisplayString(&quot;array&quot;, children, &quot;,&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x02-代码走读"><a href="#0x02-代码走读" class="headerlink" title="0x02 代码走读"></a>0x02 代码走读</h2><p>方法的调用关系如下：</p><ol><li><p>ArrayGenericUDF 用默认的构造器来初始化；</p></li><li><p>initialize() 被调用，传入函数参数的 ObjectInspector[] 数组；<br>2.1. 检查传入的参数个数与每个参数的数据类型是正确的；<br>2.2. 保存 converters (ObjectInspector) 用以供 evaluate() 使用；<br>2.3. 返回 ListObjectInspector，让 Hive 能够读取该函数的返回结果；</p></li><li><p>对于查询中的每一行，evaluate() 方法都会被调用，并传入该行的指定列；<br>3.1. 利用 initialize() 方法中存储的 converters (ObjectInspector) 来抽取出正确的值；<br>3.2. 执行处理逻辑然后用 initialize() 返回的 ListObjectInspector 来序列化返回值；</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/SerDe" target="_blank" rel="noopener">SerDe</a><br><a href="https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html" target="_blank" rel="noopener">Hadoop Hive UDF Tutorial - Extending Hive with Custom Functions</a><br><a href="https://www.jianshu.com/p/5ea006282238" target="_blank" rel="noopener">Hive-ObjectInspector</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;编写 Apache Hive 用户自定义函数（UDF）有两个不同的接口：一个是非常简单的 UDF，上一篇已经介绍过了；还有一个是 GenericUDF，相对复杂点。两个 API 的区别是：如果函数的参数和返回都是基础数据类型，那么简单 API（UDF）可以胜任；但是，如果你想写一个函数用来操作内嵌数据结构，如 Map、List 和 Set，此时就需要去熟悉复杂 API（GenericUDF）。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive自定义函数UDF</title>
    <link href="https://jordenbruce.com/2019/11/03/hive-udf/"/>
    <id>https://jordenbruce.com/2019/11/03/hive-udf/</id>
    <published>2019-11-03T13:47:18.000Z</published>
    <updated>2019-11-04T13:52:33.524Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 有丰富的内置函数（Built-in Functions），方便数据处理与数据分析等。即便如此，内置函数有时候还是无法满足需求，这时就需要自定义函数（User-Defined Functions , UDF）来扩展 Hive 函数库，实现用户想要的功能。<br><a id="more"></a></p><h2 id="0x00-自定义-UDF-开发"><a href="#0x00-自定义-UDF-开发" class="headerlink" title="0x00 自定义 UDF 开发"></a>0x00 自定义 UDF 开发</h2><p>编写 UDF 需要下面两个步骤：</p><ul><li>继承 <code>org.apache.hadoop.hive.ql.exec.UDF</code> ；</li><li>实现 <code>evaluate</code> 函数，这个函数必须要有返回值，不能设置为 void。同时建议使用 mapreduce 编程模型中的数据类型( Text, IntWritable 等)，因为 SQL 语句会被转换为 mapreduce 任务；</li></ul><p>示例代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import java.util.Date;</span><br><span class="line">import java.text.ParseException;</span><br><span class="line">import java.text.SimpleDateFormat;</span><br><span class="line"></span><br><span class="line">@Description(</span><br><span class="line">name = &quot;calc_remain&quot;,</span><br><span class="line">value = &quot;_FUNC_(active_list, active_list_date, base_date, remain) - Calculate remain_N based on a date.&quot;,</span><br><span class="line">extended = &quot;Example:\n&quot; +</span><br><span class="line">   &quot;select calc_remain(&apos;1,0,1,0,1,1,0,1&apos;,&apos;2019-03-15&apos;,&apos;2019-03-10&apos;,3);\n&quot;)</span><br><span class="line">public class CalcRemain extends UDF &#123;</span><br><span class="line"></span><br><span class="line">private interface Status &#123;</span><br><span class="line">String UnActive = &quot;0&quot;;</span><br><span class="line">String Active = &quot;1&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public int evaluate(String strActiveList, String strListDate, String strBaseDate, int intRemain) &#123;</span><br><span class="line">if (IsEmpty(strActiveList) || IsEmpty(strListDate)) &#123;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int intOuput = 0;</span><br><span class="line"></span><br><span class="line">Date dtBase = StringToDate(strBaseDate);</span><br><span class="line">Date dtLast = StringToDate(strListDate);</span><br><span class="line"></span><br><span class="line">long lngDiffDays = (dtLast.getTime() - dtBase.getTime()) / (24*3600*1000);</span><br><span class="line">if (intRemain &lt; 0 || lngDiffDays &lt; 1 || lngDiffDays &lt; intRemain) &#123;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int intDiffDays = (int)lngDiffDays;</span><br><span class="line">int lenActiveList = strActiveList.length();</span><br><span class="line"></span><br><span class="line">String[] arrActiveList = strActiveList</span><br><span class="line">                .substring(lenActiveList-intDiffDays*2-1, lenActiveList)</span><br><span class="line">                .split(&quot;,&quot;);</span><br><span class="line"></span><br><span class="line">if (arrActiveList[0].equals(Status.UnActive)) &#123;</span><br><span class="line">return 0;</span><br><span class="line">&#125; else if (arrActiveList[intRemain].equals(Status.Active)) &#123;</span><br><span class="line">intOuput = 1;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">intOuput = 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return intOuput;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private Date StringToDate(String strTime) &#123;</span><br><span class="line">Date dtOutput = null;</span><br><span class="line">SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);</span><br><span class="line">try &#123;</span><br><span class="line">dtOutput = sdf.parse(strTime);</span><br><span class="line">&#125; catch (ParseException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">return dtOutput;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private boolean IsEmpty (String strArgument) &#123;</span><br><span class="line">if (strArgument == null || strArgument.length() &lt;= 0) &#123;</span><br><span class="line">return true;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0x01-自定义-UDF-部署方式"><a href="#0x01-自定义-UDF-部署方式" class="headerlink" title="0x01 自定义 UDF 部署方式"></a>0x01 自定义 UDF 部署方式</h2><p>官方提供了两种部署 UDF 的方式：</p><ul><li>临时部署（Temporary Functions）</li><li>永久部署（Permanent Functions）</li></ul><p>两者的区别在于：临时部署的方式，只会在当前 Session 下有效并可用；永久部署的方式，在部署成功后任何一个 Hive 客户端（重新启动的 Hive 客户端，已经启动的客户端需要重新加载）都可以使用。</p><p>(1) 临时部署</p><p>这个是最常见的 Hive 使用方式，通过 hive 命令来完成 UDF 的部署；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; add jar /path/to/local.jar; </span><br><span class="line">hive&gt; create temporary function calc_remain as &apos;com.data.hive.CalcRemain&apos;;</span><br></pre></td></tr></table></figure><p>建议函数名使用 <strong>下划线命名法</strong>（全部小写字母）。</p><p>(2) 永久部署</p><p>这种方式是 hive-0.13 版本以后开始支持的注册方法；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create function udf.calc_remain </span><br><span class="line">hive&gt; as &apos;com.data.hive.CalcRemain&apos; </span><br><span class="line">hive&gt; using jar &apos;hdfs:///path/to/hdfs.jar&apos;;</span><br></pre></td></tr></table></figure><p>需要注意的是：函数名称前面一定要带上数据库名称。</p><h2 id="0x02-函数相关的-HQL-语句"><a href="#0x02-函数相关的-HQL-语句" class="headerlink" title="0x02 函数相关的 HQL 语句"></a>0x02 函数相关的 HQL 语句</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">-- 查看所有函数(内置函数+自定义函数)</span><br><span class="line">show functions;</span><br><span class="line">-- 查看某个函数的使用说明</span><br><span class="line">describe function function_name;</span><br><span class="line">-- 创建临时自定义函数</span><br><span class="line">create temporary function function_name as class_name;</span><br><span class="line">-- 删除临时自定义函数</span><br><span class="line">drop temporary function [if exists] function_name;</span><br><span class="line">-- 创建永久自定义函数</span><br><span class="line">create function [db_name.]function_name as class_name</span><br><span class="line">  [using jar|file|archive &apos;file_uri&apos; [, jar|file|archive &apos;file_uri&apos;] ];</span><br><span class="line">-- 删除永久自定义函数</span><br><span class="line">drop function [if exists] function_name;</span><br><span class="line">-- 重载函数</span><br><span class="line">reload function;</span><br></pre></td></tr></table></figure><h2 id="0x03-扩展与延伸"><a href="#0x03-扩展与延伸" class="headerlink" title="0x03 扩展与延伸"></a>0x03 扩展与延伸</h2><p>Hive 自定义函数的扩展，不仅有 UDF（User-Defined Functions），还有 UDAF（User-Defined Aggregate Functions）和 UDTF（User-Defined Table-Generating Functions），详情请参考官方说明。</p><p>另外，如果数据处理在函数级别不能解决，还可以借助 <code>TRANSFORM</code> 自定义 Map 和 Reduce 函数，或者编写 MapReduce 程序。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins" target="_blank" rel="noopener">Creating Custom UDFs</a><br><a href="https://blog.csdn.net/qq_32653877/article/details/87182898" target="_blank" rel="noopener">Java编写Hive的UDF</a><br><a href="http://chaozi204.github.io/blog/hive-udf-deploy/" target="_blank" rel="noopener">Hive UDF 部署方式小结</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy" target="_blank" rel="noopener">Writing GenericUDAFs</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform" target="_blank" rel="noopener">Hive’s Transform functionality</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 有丰富的内置函数（Built-in Functions），方便数据处理与数据分析等。即便如此，内置函数有时候还是无法满足需求，这时就需要自定义函数（User-Defined Functions , UDF）来扩展 Hive 函数库，实现用户想要的功能。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Java通过JDBC连接Hive</title>
    <link href="https://jordenbruce.com/2019/11/03/hive-jdbc-client/"/>
    <id>https://jordenbruce.com/2019/11/03/hive-jdbc-client/</id>
    <published>2019-11-03T12:56:18.000Z</published>
    <updated>2019-11-03T14:21:36.883Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 是大数据技术簇中进行数据仓库应用的基础组件，是其它类似数据仓库应用的对比基准。基础的数据操作可以通过脚本方式以 cli 进行处理；若需要开发应用程序，则需要使用 hive-jdbc 驱动进行连接。<br><a id="more"></a></p><h2 id="0x00-添加-hive-site-xml-配置项"><a href="#0x00-添加-hive-site-xml-配置项" class="headerlink" title="0x00 添加 hive-site.xml 配置项"></a>0x00 添加 hive-site.xml 配置项</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">&lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h2 id="0x01-启动-hiveserver2-服务"><a href="#0x01-启动-hiveserver2-服务" class="headerlink" title="0x01 启动 hiveserver2 服务"></a>0x01 启动 hiveserver2 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$&#123;HIVE_HOME&#125;/bin/hive --service metastore &amp;</span><br><span class="line">$&#123;HIVE_HOME&#125;/bin/hive --service hiveserver2 &amp;</span><br><span class="line"></span><br><span class="line">netstat -lnt | grep 10000</span><br></pre></td></tr></table></figure><h2 id="0x02-beeline-方式连接"><a href="#0x02-beeline-方式连接" class="headerlink" title="0x02 beeline 方式连接"></a>0x02 beeline 方式连接</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">shell&gt;&gt; $&#123;HIVE_HOME&#125;/bin/beeline</span><br><span class="line">Beeline version 1.2.2 by Apache Hive</span><br><span class="line">beeline&gt; !connect jdbc:hive2://localhost:10000/default</span><br><span class="line">Connecting to jdbc:hive2://localhost:10000/default</span><br><span class="line">Enter username for jdbc:hive2://localhost:10000/default: hadoop</span><br><span class="line">Enter password for jdbc:hive2://localhost:10000/default: ******</span><br><span class="line">Connected to: Apache Hive (version 1.2.2)</span><br><span class="line">Driver: Hive JDBC (version 1.2.2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">+-------------------+--+</span><br><span class="line">|     tab_name      |</span><br><span class="line">+-------------------+--+</span><br><span class="line">| managed_user      |</span><br><span class="line">| ods_user          |</span><br><span class="line">| partitioned_user  |</span><br><span class="line">+-------------------+--+</span><br><span class="line">3 rows selected (0.358 seconds)</span><br><span class="line">0: jdbc:hive2://localhost:10000/default&gt; !quit</span><br><span class="line">Closing: 0: jdbc:hive2://localhost:10000/default</span><br></pre></td></tr></table></figure><h2 id="0x03-jdbc-方式连接"><a href="#0x03-jdbc-方式连接" class="headerlink" title="0x03 jdbc 方式连接"></a>0x03 jdbc 方式连接</h2><p>建议使用 IntelliJ IDEA 开发工具，创建 Maven 工程，Java 代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">package com.data.hive;</span><br><span class="line"></span><br><span class="line">import java.sql.SQLException;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line"></span><br><span class="line">public class HiveJDBC &#123;</span><br><span class="line"></span><br><span class="line">    private static String DriverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;;</span><br><span class="line"></span><br><span class="line">    private static String ServerUrl = &quot;jdbc:hive2://localhost:10000/default&quot;;</span><br><span class="line">    private static String ServerUser = &quot;hadoop&quot;;</span><br><span class="line">    private static String ServerPwd = &quot;hadoop&quot;;</span><br><span class="line"></span><br><span class="line">    public static void main(String args[]) throws SQLException &#123;</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            Class.forName(DriverName);</span><br><span class="line">        &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            System.exit(1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Connection hiveConn = DriverManager.getConnection(ServerUrl, ServerUser, ServerPwd);</span><br><span class="line">        Statement hiveStmt = hiveConn.createStatement();</span><br><span class="line"></span><br><span class="line">        ResultSet hqlOutput = hiveStmt.executeQuery(&quot;show tables&quot;);</span><br><span class="line">        while (hqlOutput.next()) &#123;</span><br><span class="line">            System.out.println(hqlOutput.getString(1));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        hqlOutput.close();</span><br><span class="line">        hiveStmt.close();</span><br><span class="line">        hiveConn.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>添加 Maven 依赖，pom.xml 依赖配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">&lt;project.build.sourceEncoding&gt;UTF8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">&lt;hadoop.version&gt;2.7.7&lt;/hadoop.version&gt;</span><br><span class="line">&lt;hive.version&gt;1.2.2&lt;/hive.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">&lt;id&gt;Apache Hadoop&lt;/id&gt;</span><br><span class="line">&lt;name&gt;Apache Hadoop&lt;/name&gt;</span><br><span class="line">&lt;url&gt;https://repo1.maven.org/maven2/&lt;/url&gt;</span><br><span class="line">&lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-metastore&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p>大功告成！</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/HiveClient" target="_blank" rel="noopener">Hive Client</a><br><a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients" target="_blank" rel="noopener">HiveServer2 Clients</a><br><a href="https://www.cnblogs.com/gridmix/p/5102725.html" target="_blank" rel="noopener">通过JDBC连接hive</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 是大数据技术簇中进行数据仓库应用的基础组件，是其它类似数据仓库应用的对比基准。基础的数据操作可以通过脚本方式以 cli 进行处理；若需要开发应用程序，则需要使用 hive-jdbc 驱动进行连接。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive体系结构</title>
    <link href="https://jordenbruce.com/2019/11/02/hive-architecture/"/>
    <id>https://jordenbruce.com/2019/11/02/hive-architecture/</id>
    <published>2019-11-02T03:46:32.000Z</published>
    <updated>2019-11-03T12:59:19.852Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 最初是由 FaceBook 公司开发的一个基于 Hadoop 框架并且开源的一个数据仓库工具，后贡献给了 Apache 基金会由 Apache 来进行维护和更新。Hive 可以将结构化的文件映射为一张数据表，但并不提供查询功能，而是将 HiveQL 转化为 MapReduce 任务进行运行。同时，Hive 本身不存储数据，只是存储数据的路径或者操作信息，真的数据是存储在可靠的文件系统当中（如HDFS、Derby等）。<br><a id="more"></a></p><h2 id="0x00-Hive-架构"><a href="#0x00-Hive-架构" class="headerlink" title="0x00 Hive 架构"></a>0x00 Hive 架构</h2><p><img src="https://i.loli.net/2019/11/02/VUOGX45StkYbwMF.jpg" alt="Hive 架构"></p><p>Hive 架构主要包括如下组件：CLI（command line interface）、JDBC/ODBC、Web GUI（Hive WEB Interface）、Thrift Server、Metastore 和 Driver(Complier、Optimizer 和 Executor)，这些组件可以分为两大类：服务端组件和客户端组件。</p><p>(1) 客户端组件</p><ul><li>CLI：最常用的用户接口，cli 启动的时候，会同时启动一个 Hive 副本；</li><li>JDBC/ODBC：Client 是 Hive 的客户端，用户连接至 Hive Server。在启动 Client 模式的时候，需要指出 Hive Server 所在节点，并且在该节点启动 Hive Server ；</li><li>HWI：通过浏览器访问 Hive；</li></ul><p>(2) 服务端组件</p><ul><li>Thrift Server：Thrift 是 facebook 开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，hive 集成了Thrift Server 服务，能让不同的编程语言调用 hive 的接口；</li><li>Metastore：元数据服务组件，这个组件用于存储 hive 的元数据，包括表名、表所属的数据库、表的拥有者、列/分区字段、表的类型、表的数据所在目录等内容。hive 的元数据存储在关系数据库里，支持 derby、mysql 两种关系型数据库；</li><li>Driver：包括 Complier、Optimizer 和 Executor，它们的作用是将我们写的 HiveQL 语句进行解析、编译、优化，生成执行计划，然后调用底层的 MapReduce 计算框架；</li></ul><h2 id="0x01-Hive-工作原理"><a href="#0x01-Hive-工作原理" class="headerlink" title="0x01 Hive 工作原理"></a>0x01 Hive 工作原理</h2><p><img src="https://i.loli.net/2019/11/02/GIvJ2RqS6d3Hf4i.png" alt="Hive 查询的工作流程"></p><ol><li>UI 提交查询（HiveQL）给 Driver；</li><li>Driver 为查询创建会话句柄，并将查询发给 Complier 生成执行计划；</li><li>Complier 从 Metastore 获取必要的元数据信息；</li><li>由 Complier 生成的执行计划是 作业的DAG，包括三种作业：MapReduce 作业、元数据操作、HDFS 操作；</li><li>Execution Engine 将不同作业提交给对应的组件；</li><li>Execution Engine 将匹配上的查询结果经过 Driver 发送给 UI。</li></ol><p>细心的读者会发现，上述执行过程还是 JobTracker 版本，从 Hadoop-2.x 开始升级为 YARN，MapReduce 作为一种应用程序运行在 YARN 框架下。</p><h2 id="0x02-Hive-特点"><a href="#0x02-Hive-特点" class="headerlink" title="0x02 Hive 特点"></a>0x02 Hive 特点</h2><p>(1) 优点</p><ul><li>简单容易上手：提供了类 SQL 查询语言 HiveQL</li><li>可扩展：为超大数据集设计了计算/扩展能力（MR 作为计算引擎，HDFS 作为存储系统）</li><li>提供统一的元数据管理</li><li>延展性：Hive 支持用户自定义函数，还允许 TRANSFORM 嵌入不同语言的 map-reduce 脚本</li><li>容错：良好的容错性，节点出现问题 SQL 仍可完成执行</li></ul><p>(2) 缺点</p><ul><li>HiveQL 表达能力有限（迭代式算法，数据挖掘）</li><li>hive 效率比较低（mapreduce，调优）</li><li>hive 可控性差</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/Design" target="_blank" rel="noopener">Hive Architecture Overview</a><br><a href="https://www.cnblogs.com/zimo-jing/p/9028949.html" target="_blank" rel="noopener">深入学习Hive应用场景及架构原理</a><br><a href="https://blog.csdn.net/m0_37914799/article/details/85109143" target="_blank" rel="noopener">Hive的架构及元数据三种存储模式</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 最初是由 FaceBook 公司开发的一个基于 Hadoop 框架并且开源的一个数据仓库工具，后贡献给了 Apache 基金会由 Apache 来进行维护和更新。Hive 可以将结构化的文件映射为一张数据表，但并不提供查询功能，而是将 HiveQL 转化为 MapReduce 任务进行运行。同时，Hive 本身不存储数据，只是存储数据的路径或者操作信息，真的数据是存储在可靠的文件系统当中（如HDFS、Derby等）。&lt;br&gt;
    
    </summary>
    
      <category term="Data Warehouse" scheme="https://jordenbruce.com/categories/Data-Warehouse/"/>
    
    
      <category term="hive" scheme="https://jordenbruce.com/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>HDFS架构原理</title>
    <link href="https://jordenbruce.com/2019/10/19/hadoop-hdfs/"/>
    <id>https://jordenbruce.com/2019/10/19/hadoop-hdfs/</id>
    <published>2019-10-19T09:29:24.000Z</published>
    <updated>2019-10-22T12:36:09.282Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS 被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统，是 Hadoop 应用程序使用的主要分布式存储。HDFS 与现有的分布式文件系统有许多相似之处；但是，与其他分布式文件系统的区别也是很明显的。HDFS 具有高度的容错能力，旨在部署在低成本硬件上；HDFS 提供对应用程序数据的高吞吐量访问，并且适用于具有大数据集的应用程序；HDFS 放宽了一些POSIX要求，以实现对文件系统数据的流式访问。<br><a id="more"></a></p><h2 id="0x00-HDFS-架构"><a href="#0x00-HDFS-架构" class="headerlink" title="0x00 HDFS 架构"></a>0x00 HDFS 架构</h2><p>HDFS (Hadoop Distributed File System) 采用的是 Master/Slave 架构，一个 HDFS 集群包含一个单独的 NameNode 和多个 DataNode 节点，如下图所示：</p><p><img src="https://i.loli.net/2019/10/21/4JgoDXhFeVaukj9.png" alt="HDFS 架构"></p><p>主要由以下几个组件构成：</p><ul><li>NameNode</li><li>Secondary NameNode</li><li>DataNode</li><li>Client</li></ul><p>需要说明的是：类似于磁盘 Block size 概念，Hadoop 集群中文件的存储都是以块的形式存储在 HDFS 中。通过设置 hdfs-site.xml 文件中 dfs.blocksize 参数来配置块大小，默认是128M。</p><h2 id="0x01-文件写入过程"><a href="#0x01-文件写入过程" class="headerlink" title="0x01 文件写入过程"></a>0x01 文件写入过程</h2><p><img src="https://i.loli.net/2019/10/21/5EOiCB2JVQYNswd.png" alt="HDFS 文件写入过程"></p><p>Client 向 HDFS 写入文件的具体过程如下：</p><ol><li><p>Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象；</p></li><li><p>通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；</p></li><li><p>通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；</p></li><li><p>DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；</p></li><li><p>DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；</p></li><li><p>完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，完成文件写入；</p></li><li><p>调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。</p></li></ol><h2 id="0x02-文件读取过程"><a href="#0x02-文件读取过程" class="headerlink" title="0x02 文件读取过程"></a>0x02 文件读取过程</h2><p><img src="https://i.loli.net/2019/10/21/oYipLz9MEq4TyS7.png" alt="HDFS 文件读取过程"></p><p>Client 向 HDFS 读取文件的具体过程如下：</p><ol><li><p>Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；</p></li><li><p>NameNode 返回存储的每个块的 DataNode 列表；</p></li><li><p>Client 将连接到列表中最近的 DataNode；</p></li><li><p>Client 开始从 DataNode 并行读取数据；</p></li><li><p>一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。</p></li></ol><p>在处理 Client 的读取请求时，HDFS 会利用机架感知选举最接近 Client 位置的副本，这将会减少读取延迟和带宽消耗。</p><h2 id="0x03-HDFS-遗留问题"><a href="#0x03-HDFS-遗留问题" class="headerlink" title="0x03 HDFS 遗留问题"></a>0x03 HDFS 遗留问题</h2><p>在前面的介绍中，关于 HDFS 1.0 架构的设计缺陷主要有以下两点：</p><ul><li><p>NameNode 的单点问题，如果 NameNode 挂掉了，数据读写都会受到影响，HDFS 整体将变得不可用，这在生产环境中是不可接受的；</p></li><li><p>水平扩展问题，随着集群规模的扩大，1.0 时集群规模达到3000时，会导致整个集群管理的文件数目达到上限（因为 NameNode 要管理整个集群 block 元信息、数据目录信息等）。</p></li></ul><p>为了解决上面的两个问题，Hadoop2.0 提供一套统一的解决方案：</p><ul><li><p>HA（High Availability 高可用方案）：这个是为了解决 NameNode 单点问题；</p></li><li><p>NameNode Federation：是用来解决 HDFS 集群的线性扩展能力。</p></li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html" target="_blank" rel="noopener">HDFS Users Guide</a><br><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="noopener">HDFS Architecture</a><br><a href="https://www.cnblogs.com/sheng-sjk/p/10624958.html" target="_blank" rel="noopener">Hdfs block数据块大小的设置规则</a><br><a href="https://www.jianshu.com/p/25bdb4b8051e" target="_blank" rel="noopener">HDFS基础架构与各个组件的功能</a><br><a href="https://matt33.com/2018/07/15/hdfs-architecture-learn/" target="_blank" rel="noopener">HDFS 架构学习总结</a><br><a href="https://www.open-open.com/lib/view/open1376228205209.html" target="_blank" rel="noopener">HDFS 原理、架构与特性介绍</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HDFS 被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统，是 Hadoop 应用程序使用的主要分布式存储。HDFS 与现有的分布式文件系统有许多相似之处；但是，与其他分布式文件系统的区别也是很明显的。HDFS 具有高度的容错能力，旨在部署在低成本硬件上；HDFS 提供对应用程序数据的高吞吐量访问，并且适用于具有大数据集的应用程序；HDFS 放宽了一些POSIX要求，以实现对文件系统数据的流式访问。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="hdfs" scheme="https://jordenbruce.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>YARN技术原理</title>
    <link href="https://jordenbruce.com/2019/10/16/hadoop-yarn/"/>
    <id>https://jordenbruce.com/2019/10/16/hadoop-yarn/</id>
    <published>2019-10-15T23:58:29.000Z</published>
    <updated>2019-10-22T12:36:15.572Z</updated>
    
    <content type="html"><![CDATA[<p>YARN (Yet Another Resource Negotiator) 是从 hadoop-0.23 开始引入的新架构，他的基本设计思想是将 JobTracker 的两个主要功能，资源管理和作业控制（包括调度/监视等），拆分为单独的守护进程，分别是一个全局的 ResourceManager（RM）和每个应用程序的 ApplicationMaster（AM）。应用程序可以是单个作业，也可以是作业的DAG。<br><a id="more"></a></p><h2 id="0x00-YARN-架构"><a href="#0x00-YARN-架构" class="headerlink" title="0x00 YARN 架构"></a>0x00 YARN 架构</h2><p>YARN 的全称是 Yet Another Resource Negotiator，YARN 整体上是 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave，如下图所示：</p><p><img src="https://i.loli.net/2019/02/20/5c6d021102002.png" alt="YARN架构"></p><p>主要由以下几个组件构成：</p><ul><li>ResourceManager (RM) : Scheduler, Applications Manager (ASM)</li><li>NodeManager (NM)</li><li>ApplicationMaster (AM)</li><li>Container</li></ul><p>需要说明的是：Scheduler 是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 FIFO Scheduler，Fair Scheduler 和 Capacity Schedule 等。</p><h2 id="0x01-YARN-工作流程"><a href="#0x01-YARN-工作流程" class="headerlink" title="0x01 YARN 工作流程"></a>0x01 YARN 工作流程</h2><p>如下图所示用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：</p><ul><li>启动AM ，如下步骤1~3；</li><li>由AM创建应用程序为它申请资源并监控它的整个运行过程，直到运行完成，如下步骤4~7。</li></ul><p><img src="https://i.loli.net/2019/02/20/5c6d03e204bfc.png" alt="YARN工作流程"></p><ol><li><p>用户向YARN中提交应用程序，其中包括AM程序、启动AM的命令、命令参数、用户程序等；事实上，需要准确描述运行ApplicationMaster的unix进程的所有信息。提交工作通常由YarnClient来完成。</p></li><li><p>RM为该应用程序分配第一个Container，并与对应的NM通信，要求它在这个Container中启动AM；</p></li><li><p>AM首先向RM注册，这样用户可以直接通过RM査看应用程序的运行状态，运行状态通过 AMRMClientAsync.CallbackHandler的getProgress() 方法来传递给RM。 然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4〜7；</p></li><li><p>AM采用轮询的方式通过RPC协议向RM申请和领取资源；资源的协调通过 AMRMClientAsync异步完成,相应的处理方法封装在AMRMClientAsync.CallbackHandler中。</p></li><li><p>—旦AM申请到资源后，便与对应的NM通信，要求它启动任务；通常需要指定一个ContainerLaunchContext，提供Container启动时需要的信息。</p></li><li><p>NM为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</p></li><li><p>各个任务通过某个RPC协议向AM汇报自己的状态和进度，以让AM随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；ApplicationMaster与NM的通信通过NMClientAsync object来完成，容器的所有事件通过NMClientAsync.CallbackHandler来处理。例如启动、状态更新、停止等。</p></li><li><p>应用程序运行完成后，AM向RM注销并关闭自己。</p></li></ol><h2 id="0x02-YARN-资源管理"><a href="#0x02-YARN-资源管理" class="headerlink" title="0x02 YARN 资源管理"></a>0x02 YARN 资源管理</h2><p>当前 YARN 支持内存和CPU两种资源类型的管理和分配。主要是 scheduler 和 NodeManager 的配置选项设置，参数如下：</p><table><thead><tr><th>参数名</th><th>默认值</th></tr></thead><tbody><tr><td>yarn.resourcemanager.scheduler.class</td><td>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>32</td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>8192</td></tr><tr><td>yarn.nodemanager.vmem-pmem-ratio</td><td>2.1</td></tr><tr><td>yarn.nodemanager.pmem-check-enabled</td><td>true</td></tr><tr><td>yarn.nodemanager.vmem-check-enabled</td><td>true</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>8</td></tr><tr><td>yarn.nodemanager.resource.percentage-physical-cpu-limit</td><td>100</td></tr></tbody></table><p>另外，YARN 使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">YARN Architecture</a><br><a href="https://matt33.com/2018/09/01/yarn-architecture-learn/" target="_blank" rel="noopener">YARN 架构学习总结</a><br><a href="https://cshihong.github.io/2018/05/11/Yarn技术原理/" target="_blank" rel="noopener">Yarn技术原理</a><br><a href="https://blog.csdn.net/bingduanlbd/article/details/51880019" target="_blank" rel="noopener">理解Hadoop YARN架构</a><br><a href="http://www.cnblogs.com/wcwen1990/p/6737985.html" target="_blank" rel="noopener">YARN架构设计详解</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YARN (Yet Another Resource Negotiator) 是从 hadoop-0.23 开始引入的新架构，他的基本设计思想是将 JobTracker 的两个主要功能，资源管理和作业控制（包括调度/监视等），拆分为单独的守护进程，分别是一个全局的 ResourceManager（RM）和每个应用程序的 ApplicationMaster（AM）。应用程序可以是单个作业，也可以是作业的DAG。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="yarn" scheme="https://jordenbruce.com/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce执行过程详解</title>
    <link href="https://jordenbruce.com/2019/10/13/hadoop-mapreduce/"/>
    <id>https://jordenbruce.com/2019/10/13/hadoop-mapreduce/</id>
    <published>2019-10-13T14:57:51.000Z</published>
    <updated>2019-10-13T12:34:06.589Z</updated>
    
    <content type="html"><![CDATA[<p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。<br><a id="more"></a></p><h2 id="0x00-分析MapReduce执行过程"><a href="#0x00-分析MapReduce执行过程" class="headerlink" title="0x00 分析MapReduce执行过程"></a>0x00 分析MapReduce执行过程</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。整个流程如图：</p><p><img src="https://i.loli.net/2019/10/12/jotATrBqyIzV5cY.png" alt="MapReduce执行过程"></p><h2 id="0x01-Mapper任务的执行过程详解"><a href="#0x01-Mapper任务的执行过程详解" class="headerlink" title="0x01 Mapper任务的执行过程详解"></a>0x01 Mapper任务的执行过程详解</h2><p><strong>每个Mapper任务是一个java进程</strong>，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下几个阶段，如图所示：</p><p><img src="https://i.loli.net/2019/10/12/6jbSxLJQNqls4FP.png" alt="Mapper执行过程"></p><p>在上图中，把Mapper任务的运行过程分为六个阶段。</p><ol><li><p>第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值64MB，输入文件有两个，一个是32MB，一个是72MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p></li><li><p>第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一行的起始位置(单位是字节)，“值”是本行的文本内容。</p></li><li><p>第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p></li><li><p>第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer任务运行的数量。默认只有一个Reducer任务。</p></li><li><p>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入第六阶段；如果没有，直接输出到本地的linux文件中。</p></li><li><p>第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。</p></li></ol><h2 id="0x02-Reducer任务的执行过程详解"><a href="#0x02-Reducer任务的执行过程详解" class="headerlink" title="0x02 Reducer任务的执行过程详解"></a>0x02 Reducer任务的执行过程详解</h2><p><strong>每个Reducer任务是一个java进程</strong>。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为如下图所示的几个阶段。</p><p><img src="https://i.loli.net/2019/10/12/HXhC2wvUQLKdV4e.png" alt="Reducer执行过程"></p><ol><li><p>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。</p></li><li><p>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。</p></li><li><p>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS文件中。</p></li></ol><p>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p><h2 id="0x03-键值对的编号"><a href="#0x03-键值对的编号" class="headerlink" title="0x03 键值对的编号"></a>0x03 键值对的编号</h2><p>在对Mapper任务、Reducer任务的分析过程中，会看到很多阶段都出现了键值对，读者容易混淆，所以这里对键值对进行编号，方便大家理解键值对的变化情况，如下图所示。</p><p><img src="https://i.loli.net/2019/10/12/q6Y9Sl5ofQCW7LJ.png" alt="键值对"></p><p>在上图中，对于Mapper任务输入的键值对，定义为key1和value1。在map方法中处理后，输出的键值对，定义为key2和value2。reduce方法接收key2和value2，处理后，输出key3和value3。在下文讨论键值对时，可能把key1和value1简写为&lt;k1,v1&gt;，key2和value2简写为&lt;k2,v2&gt;，key3和value3简写为&lt;k3,v3&gt;。</p><h2 id="转载说明"><a href="#转载说明" class="headerlink" title="转载说明"></a>转载说明</h2><p><a href="https://my.oschina.net/itblog/blog/275294" target="_blank" rel="noopener">Hadoop MapReduce执行过程详解（带hadoop例子）</a><br><a href="http://matt33.com/2016/03/02/hadoop-shuffle/" target="_blank" rel="noopener">MapReduce之Shuffle过程详述</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。&lt;br&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://jordenbruce.com/categories/Hadoop/"/>
    
    
      <category term="mapreduce" scheme="https://jordenbruce.com/tags/mapreduce/"/>
    
  </entry>
  
</feed>
