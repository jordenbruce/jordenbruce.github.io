<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="hadoop,hive,dw,database,sql,oozie,sqoop,architecture"><title>MapReduce之Shuffle过程详解 | JordenBruce</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">MapReduce之Shuffle过程详解</h1><a id="logo" href="/.">JordenBruce</a><p class="description">A thousand miles begins with a single step.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">MapReduce之Shuffle过程详解</h1><div class="post-meta">Feb 19, 2019<span> | </span><span class="category"><a href="/categories/hadoop/">hadoop</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.6k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 13</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>MapReduce计算模型一般包括两个重要的阶段：Map是映射，负责数据的过滤分发，将原始数据转化为键值对；Reduce是规约，将具有相同key值的value进行处理后再输出新的键值对作为最终结果。为了让Reduce可以并行处理Map的结果，必须对Map的输出进行一定的排序与分割，然后再交给对应的Reduce，而这个将Map输出进行进一步整理并交给Reduce的过程就是Shuffle。<br><a id="more"></a></p>
<h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p>Shuffle过程是MapReduce的核心，也被称为奇迹发生的地方。要想理解MapReduce，Shuffle是必须要了解的。这里先给出官网上关于这个过程的经典流程图：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/pNFrlaAqE9YU.png" alt="Shuffle过程"><br>上图是把MapReduce过程分为两个部分，而实际上从两边的Map和Reduce到中间的那一大块都属于Shuffle过程，也就是说，Shuffle过程有一部分是在Map端，有一部分是在Reduce端，下文也将会分两部分来介绍Shuffle过程。</p>
<p>对于Hadoop集群，当我们在运行作业时，大部分的情况下，map task与reduce task的执行是分布在不同的节点上的，因此，很多情况下，reduce执行时需要跨节点去拉取其他节点上的map task结果，这样造成了集群内部的网络资源消耗很严重，而且在节点的内部，相比于内存，磁盘IO对性能的影响是非常严重的。如果集群中运行的作业有很多，那么task的执行对于集群内部网络的资源消费非常大。因此，我们对于MapRedue作业Shuffle过程的期望是：</p>
<ul>
<li>完整地从map task端拉取数据到Reduce端；</li>
<li>在跨节点拉取数据时，尽可能地减少对带宽的不必要消耗；</li>
<li>减少磁盘IO对task执行的影响。</li>
</ul>
<h2 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h2><p>在进行海量数据处理时，外存文件数据I/O访问会成为一个制约系统性能的瓶颈，因此，Hadoop的Map过程实现的一个重要原则就是：计算靠近数据，这里主要指两个方面：</p>
<ol>
<li>代码靠近数据：<ul>
<li>原则：本地化数据处理（locality），即一个计算节点尽可能处理本地磁盘上所存储的数据；</li>
<li>尽量选择数据所在DataNode启动Map任务；</li>
<li>这样可以减少数据通信，提高计算效率；</li>
</ul>
</li>
<li>数据靠近代码：<ul>
<li>当本地没有数据处理时，尽可能从同一机架或最近其他节点传输数据进行处理（host选择算法）。</li>
</ul>
</li>
</ol>
<p>下面，我们分块去介绍Hadoop的Map过程，map的经典流程图如下：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/Hc0RIDhvW7CU.jpg" alt="map-shuffle"></p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ol>
<li>map task只读取split分片，split与block（hdfs的最小存储单位，默认为64MB）可能是一对一也能是一对多，但是对于一个split只会对应一个文件的一个block或多个block，不允许一个split对应多个文件的多个block；</li>
<li>这里切分和输入数据的时会涉及到InputFormat的文件切分算法和host选择算法。</li>
</ol>
<p>文件切分算法，主要用于确定InputSplit的个数以及每个InputSplit对应的数据段。FileInputFormat以文件为单位切分生成InputSplit，对于每个文件，由以下三个属性值决定其对应的InputSplit的个数：</p>
<ul>
<li>goalSize：它是根据用户期望的InputSplit数目计算出来的，即totalSize/numSplits。其中，totalSize为文件的总大小；numSplits为用户设定的Map Task个数，默认情况下是1；</li>
<li>minSize：InputSplit的最小值，由配置参数mapred.min.split.size确定，默认是1；</li>
<li>blockSize：文件在hdfs中存储的block大小，不同文件可能不同，默认是64MB。</li>
</ul>
<p>这三个参数共同决定InputSplit的最终大小，计算方法如下：<br><code>splitSize = max{minSize, min{gogalSize,blockSize}}</code></p>
<p>细节请参考《<a href="https://blog.csdn.net/xingliang_li/article/details/53285447" target="_blank" rel="noopener">FileInputFormat类中split切分算法和host选择算法介绍</a>》</p>
<h3 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h3><ul>
<li>作用：将map的结果发送到相应的reduce端，总的partition的数目等于reducer的数量。</li>
<li>实现功能：<ol>
<li>map输出的是key/value对，决定于当前的mapper的part交给哪个reduce的方法是：mapreduce提供的Partitioner接口，对key进行hash后，再以reducetask数量取模，然后到指定的job上（HashPartitioner，可以通过job.setPartitionerClass(MyPartition.class)自定义）。</li>
<li>然后将数据写入到内存缓冲区，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。key/value对以及Partition的结果都会被写入缓冲区。在写入之前，key与value值都会被序列化成字节数组。</li>
</ol>
</li>
<li>要求：负载均衡，效率；</li>
</ul>
<h3 id="spill（溢写）：sort-amp-combiner"><a href="#spill（溢写）：sort-amp-combiner" class="headerlink" title="spill（溢写）：sort &amp; combiner"></a>spill（溢写）：sort &amp; combiner</h3><ul>
<li>作用：把内存缓冲区中的数据写入到本地磁盘，在写入本地磁盘时先按照partition、再按照key进行排序（quick sort）；</li>
<li>注意：<ol>
<li>这个spill是由另外单独的线程来完成，不影响往缓冲区写map结果的线程；</li>
<li>内存缓冲区默认大小限制为100MB，它有个溢写比例（spill.percent），默认为0.8，当缓冲区的数据达到阈值时，溢写线程就会启动，先锁定这80MB的内存，执行溢写过程，maptask的输出结果还可以往剩下的20MB内存中写，互不影响。然后再重新利用这块缓冲区，因此Map的内存缓冲区又叫做环形缓冲区（两个指针的方向不会变，下面会详述）；</li>
<li>在将数据写入磁盘之前，先要对要写入磁盘的数据进行一次排序操作，先按&lt;key,value,partition&gt;中的partition分区号排序，然后再按key排序，这个就是sort操作，最后溢出的小文件是分区的，且同一个分区内是保证key有序的；</li>
</ol>
</li>
</ul>
<p>combine：执行combine操作要求开发者必须在程序中设置了combine（程序中通过job.setCombinerClass(myCombine.class)自定义combine操作）。</p>
<ul>
<li>程序中有两个阶段可能会执行combine操作：<ol>
<li>map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作（前提是作业中设置了这个操作）；</li>
<li>如果map输出比较大，溢出文件个数大于3（此值可以通过属性min.num.spills.for.combine配置）时，在merge的过程（多个spill文件合并为一个大文件）中还会执行combine操作；</li>
</ol>
</li>
<li>combine主要是把形如&lt;aa,1&gt;,&lt;aa,2&gt;这样的key值相同的数据进行计算，计算规则与reduce一致，比如：当前计算是求key对应的值求和，则combine操作后得到&lt;aa,3&gt;这样的结果。</li>
<li>注意事项：不是每种作业都可以做combine操作的，只有满足以下条件才可以：<ol>
<li>reduce的输入输出类型都一样，因为combine本质上就是reduce操作；</li>
<li>计算逻辑上，combine操作后不会影响计算结果，像求和就不会影响；</li>
</ol>
</li>
</ul>
<h3 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h3><ul>
<li>merge过程：当map很大时，每次溢写会产生一个spill_file，这样会有多个spill_file，而最终的一个map task输出只有一个文件，因此，最终的结果输出之前会对多个中间过程进行多次溢写文件（spill_file）的合并，此过程就是merge过程。也即是，待Map Task任务的所有数据都处理完后，会对任务产生的所有中间数据文件做一次合并操作，以确保一个Map Task最终只生成一个中间数据文件。</li>
<li>注意：<ol>
<li>如果生成的文件太多，可能会执行多次合并，每次最多能合并的文件数默认为10，可以通过属性min.num.spills.for.combine配置；</li>
<li>多个溢出文件合并时，会进行一次排序，排序算法是多路归并排序；</li>
<li>是否还需要做combine操作，一是看是否设置了combine，二是看溢出的文件数是否大于等于3；</li>
<li>最终生成的文件格式与单个溢出文件一致，也是按分区顺序存储，并且输出文件会有一个对应的索引文件，记录每个分区数据的起始位置，长度以及压缩长度，这个索引文件名叫做file.out.index。</li>
</ol>
</li>
</ul>
<h3 id="内存缓冲区"><a href="#内存缓冲区" class="headerlink" title="内存缓冲区"></a>内存缓冲区</h3><ol>
<li>在Map Task任务的业务处理方法map()中，最后一步通过OutputCollector.collect(key,value)或context.write(key,value)输出Map Task的中间处理结果，在相关的collect(key,value)方法中，会调用Partitioner.getPartition(K2 key, V2 value, int numPartitions)方法获得输出的key/value对应的分区号(分区号可以认为对应着一个要执行Reduce Task的节点)，然后将&lt;key,value,partition&gt;暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，该缓冲区的默认大小是100MB，可以通过参数io.sort.mb来调整其大小。</li>
<li>当缓冲区中的数据使用率达到一定阀值后，触发一次Spill操作，将环形缓冲区中的部分数据写到磁盘上，生成一个临时的Linux本地数据的spill文件；然后在缓冲区的使用率再次达到阀值后，再次生成一个spill文件。直到数据处理完毕，在磁盘上会生成很多的临时文件。</li>
<li>缓存有一个阀值比例配置，当达到整个缓存的这个比例时，会触发spill操作；触发时，map输出还会接着往剩下的空间写入，但是写满的空间会被锁定，数据溢出写入磁盘。当这部分溢出的数据写完后，空出的内存空间可以接着被使用，形成像环一样的被循环使用的效果，所以又叫做环形内存缓冲区；</li>
<li>MapOutputBuffe内部存数的数据采用了两个索引结构，涉及三个环形内存缓冲区。下来看一下两级索引结构：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/RSrtmYI28b5L.jpg" alt="MapOutputBuffer的两级索引结构"></li>
</ol>
<p>写入到缓冲区的数据采取了压缩算法 <a href="http://www.cnblogs.com/edisonchou/p/4298423.html" target="_blank" rel="noopener">http://www.cnblogs.com/edisonchou/p/4298423.html</a><br>这三个环形缓冲区的含义分别如下：</p>
<ol>
<li>kvoffsets缓冲区：也叫偏移量索引数组，用于保存key/value信息在位置索引 kvindices 中的偏移量。当 kvoffsets 的使用率超过 io.sort.spill.percent (默认为80%)后，便会触发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li>
<li>kvindices缓冲区：也叫位置索引数组，用于保存 key/value 在数据缓冲区 kvbuffer 中的起始位置。</li>
<li>kvbuffer即数据缓冲区：用于保存实际的 key/value 的值。默认情况下该缓冲区最多可以使用 io.sort.mb 的95%，当 kvbuffer 使用率超过 io.sort.spill.percent (默认为80%)后，便会出发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li>
</ol>
<p>写入到本地磁盘时，对数据进行排序，实际上是对kvoffsets这个偏移量索引数组进行排序。</p>
<h2 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h2><p>Reduce过程的经典流程图如下：<br><img src="http://pn4itjib1.bkt.clouddn.com/blog/20190219/aUlSEGON9iXl.png" alt="reduce-shuffle"></p>
<h3 id="copy过程"><a href="#copy过程" class="headerlink" title="copy过程"></a>copy过程</h3><ul>
<li>作用：拉取数据；</li>
<li>过程：Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为这时map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。</li>
<li>默认情况下，当整个MapReduce作业的所有已执行完成的Map Task任务数超过Map Task总数的5%后，JobTracker便会开始调度执行Reduce Task任务。然后Reduce Task任务默认启动mapred.reduce.parallel.copies(默认为5）个MapOutputCopier线程到已完成的Map Task任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，则写到磁盘上。</li>
</ul>
<h4 id="内存缓冲区-1"><a href="#内存缓冲区-1" class="headerlink" title="内存缓冲区"></a>内存缓冲区</h4><ul>
<li>这个内存缓冲区大小的控制就不像map那样可以通过io.sort.mb来设定了，而是通过另外一个参数来设置：mapred.job.shuffle.input.buffer.percent（default 0.7）， 这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。</li>
<li>如果该reduce task的最大heap使用量（通常通过mapred.child.java.opts来设置，比如设置为-Xmx1024m）的一定比例用来缓存数据。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。如果reduce的heap由于业务原因调整的比较大，相应的缓存大小也会变大，这也是为什么reduce用来做缓存的参数是一个百分比，而不是一个固定的值了。</li>
</ul>
<h3 id="merge过程"><a href="#merge过程" class="headerlink" title="merge过程"></a>merge过程</h3><ul>
<li>Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比 map 端的更为灵活，它基于 JVM 的heap size设置，因为 Shuffle 阶段 Reducer 不运行，所以应该把绝大部分的内存都给 Shuffle 用。</li>
<li>这里需要强调的是，merge 有三种形式：1)内存到内存 2)内存到磁盘 3)磁盘到磁盘。默认情况下第一种形式是不启用的。当内存中的数据量到达一定阈值，就启动内存到磁盘的 merge（图中的第一个merge，之所以进行merge是因为reduce端在从多个map端copy数据的时候，并没有进行sort，只是把它们加载到内存，当达到阈值写入磁盘时，需要进行merge） 。这和map端的很类似，这实际上就是溢写的过程，在这个过程中如果你设置有Combiner，它也是会启用的，然后在磁盘中生成了众多的溢写文件，这种merge方式一直在运行，直到没有 map 端的数据时才结束，然后才会启动第三种磁盘到磁盘的 merge （图中的第二个merge）方式生成最终的那个文件。</li>
<li>在远程copy数据的同时，Reduce Task在后台启动了两个后台线程对内存和磁盘上的数据文件做合并操作，以防止内存使用过多或磁盘生的文件过多。</li>
</ul>
<h3 id="reducer的输入文件"><a href="#reducer的输入文件" class="headerlink" title="reducer的输入文件"></a>reducer的输入文件</h3><ul>
<li>merge的最后会生成一个文件，大多数情况下存在于磁盘中，但是需要将其放入内存中。当reducer 输入文件已定，整个 Shuffle 阶段才算结束。然后就是 Reducer 执行，把结果放到 HDFS 上。</li>
</ul>
<p>以上内容来自：<a href="http://matt33.com/2016/03/02/hadoop-shuffle/" target="_blank" rel="noopener">MapReduce之Shuffle过程详述</a></p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>JordenBruce</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2019/02/19/hadoop-shuffle/">https://jordenbruce.com/2019/02/19/hadoop-shuffle/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jordenbruce.com/2019/02/19/hadoop-shuffle/" data-id="ck0kdjd9y000z9on3f3dyw7ug" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACKElEQVR42u3aQXICMQwEQP7/6c0DCDAjk1St3T5RCazde1DJkh6PeF1PK/9+8s32V8OFgYFxW8b1dr3/zvNRZrB231+eg4GBcQAjCYXJ4ZKg+fzf9gVhYGBg5Iz3mPyIGBgYGN9lrITLPAHFwMDAmF1ik3SwTSKTz1++i2NgYNyQkVfd///zn/Q3MDAwbsW4ypU/YdZyGJ4KAwNja8asAZnA8iQvuqAmDVEMDIytGe0Q2Pu/t2NkSftz6cqKgYFxc8ascNYGwW+1Kl+CMTAwjmQk6VceKNtrcB5qMTAwzmR8qwTWJpc54EPVEAMDYzvGbAisLZ/NEsfZLhgYGPsx2kJYHv7WW5t58MXAwNib8RcDE8k4RRsto2diYGAcxijCXHwpXb/ERgkiBgbGkYy8rN/+anbRfazEZgwMjC0Y+ZBEO5CRtEuTJ0SpIQYGxjGMvGGQB8phMF2ZSsPAwNiCsTLukITOtmDXXqExMDBOY8y2bF/N7BVEu2NgYGzKyI++kt6tN0c/nAQDA+MAxmz7WcshiZN12oqBgXEYY9a2XAm+bVnwZWMAAwNjO8ZVrtk27QhFnZ5iYGBszVhvc86KZe1QRd40xcDA2JUxK7etXGLblmfUyMTAwDiAkQe+fAAiaQa0qV5xF8fAwDiY0QbiPOAmzcsP0yIYGBgYC6F21i4tSBgYGAcwVsaz1gv6bUMCAwPjNEZ+dUyCcjsE1iaFS41MDAyM+zF+APz3apgOvpSyAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/MapReduce/">MapReduce</a></div><div class="post-nav"><a class="pre" href="/2019/02/21/hadoop-yarn/">YARN架构详解</a><a class="next" href="/2019/02/17/hadoop-mapreduce/">MapReduce执行过程详解</a></div><div id="container"></div><link rel="stylesheet" type="text/css" href="//unpkg.com/gitalk/dist/gitalk.css?v=0.0.0"><script type="text/javascript" src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js?v=0.0.0"></script><script type="text/javascript" src="//unpkg.com/gitalk/dist/gitalk.min.js?v=0.0.0"></script><script>var gitalk = new Gitalk({
  clientID: '24d554575e1f11313fa9',
  clientSecret: '8de4e32b66c3386a10942a3c5e29cf09b8954c26',
  repo: 'jordenbruce.github.io',
  owner: 'jordenbruce',
  admin: ['jordenbruce'],
  id: md5(location.pathname),
  distractionFreeMode: false
})
gitalk.render('container')
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"><input type="hidden" name="si" value="https://jordenbruce.com"><input name="tn" type="hidden" value="bds"><input name="cl" type="hidden" value="3"><input name="ct" type="hidden" value="2097152"><input name="s" type="hidden" value="on"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Warehouse/">Data Warehouse</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hive/">Hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/思维游戏/">思维游戏</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/MapReduce/" style="font-size: 15px;">MapReduce</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/hive/" style="font-size: 15px;">hive</a> <a href="/tags/sqoop/" style="font-size: 15px;">sqoop</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/UDF/" style="font-size: 15px;">UDF</a> <a href="/tags/Partition/" style="font-size: 15px;">Partition</a> <a href="/tags/策略思维/" style="font-size: 15px;">策略思维</a> <a href="/tags/YARN/" style="font-size: 15px;">YARN</a> <a href="/tags/OOM/" style="font-size: 15px;">OOM</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/09/15/hadoop-release/">hadoop发行版的选取</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/strategy001/">裁员还是减薪</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/14/hive-udf/">Hive自定义函数</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/04/hive-partition/">Hive分区表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/25/hive-oom/">HQL内存溢出的参数调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/21/hadoop-yarn/">YARN架构详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/19/hadoop-shuffle/">MapReduce之Shuffle过程详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/17/hadoop-mapreduce/">MapReduce执行过程详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/13/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://hao.199it.com" title="大数据导航" target="_blank">大数据导航</a><ul></ul><a href="https://www.infoq.cn" title="InfoQ" target="_blank">InfoQ</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">JordenBruce.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>